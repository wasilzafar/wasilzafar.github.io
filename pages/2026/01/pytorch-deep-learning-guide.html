<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Master PyTorch from scratch. Learn tensors, autograd, neural networks, CNNs, RNNs, transfer learning, and deployment. Complete beginner-friendly guide with executable examples." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="PyTorch, Deep Learning, Neural Networks, Python, Machine Learning, CNN, RNN, LSTM, Transfer Learning, GPU, Autograd, TorchScript" />
    <meta property="og:title" content="PyTorch Deep Learning: Complete Beginner's Guide to Building Neural Networks" />
    <meta property="og:description" content="Learn PyTorch fundamentals: tensors, autograd, building neural networks, CNNs, RNNs, transfer learning, and deployment. Beginner-friendly with hands-on examples." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-01" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>PyTorch Deep Learning: Complete Beginner's Guide to Building Neural Networks - Wasil Zafar</title>

    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Font Awesome Icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />

    <!-- Custom Styles -->
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />

    <!-- Prism.js Syntax Highlighting -->
    <!-- Multiple themes for dynamic switching -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />

    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <!-- Google Consent Mode v2 -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        
        gtag('consent', 'default', {
            'ad_storage': 'denied',
            'ad_user_data': 'denied',
            'ad_personalization': 'denied',
            'analytics_storage': 'denied',
            'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE']
        });
        
        gtag('consent', 'default', {
            'ad_storage': 'granted',
            'ad_user_data': 'granted',
            'ad_personalization': 'granted',
            'analytics_storage': 'granted'
        });
        
        gtag('set', 'url_passthrough', true);
    </script>

    <!-- Google Tag Manager -->
    <script>
        (function(w, d, s, l, i) {
            w[l] = w[l] || [];
            w[l].push({
                'gtm.start': new Date().getTime(),
                event: 'gtm.js'
            });
            var f = d.getElementsByTagName(s)[0],
                j = d.createElement(s),
                dl = l != 'dataLayer' ? '&l=' + l : '';
            j.async = true;
            j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    <style>
        /* Blog Post Specific Styles */
        .blog-hero {
            background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%);
            color: white;
            padding: 80px 0;
        }

        .blog-header {
            margin-bottom: 2rem;
        }

        .blog-meta {
            font-size: 0.95rem;
            color: var(--color-teal);
            margin-bottom: 1rem;
        }

        .blog-meta span {
            margin-right: 1.5rem;
        }

        .blog-content {
            max-width: 900px;
            margin: 0 auto;
            font-size: 1.05rem;
            line-height: 1.8;
            color: #333;
        }

        .blog-content h2 {
            font-size: 1.8rem;
            font-weight: 700;
            margin-top: 2.5rem;
            margin-bottom: 1.5rem;
            color: var(--color-navy);
            border-bottom: 3px solid var(--color-teal);
            padding-bottom: 0.5rem;
        }

        .blog-content h3 {
            font-size: 1.3rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: var(--color-blue);
        }

        .blog-content p {
            margin-bottom: 1.2rem;
            text-align: justify;
        }

        .blog-content strong {
            color: var(--color-crimson);
        }

        .highlight-box {
            background: rgba(59, 151, 151, 0.1);
            border-left: 4px solid var(--color-teal);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 4px;
        }

        .experiment-card {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: all 0.3s ease;
        }

        .experiment-card:hover {
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
            transform: translateY(-2px);
        }

        .experiment-card h4 {
            color: var(--color-crimson);
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        .experiment-card .card-meta {
            font-size: 0.9rem;
            color: var(--color-blue);
            margin-bottom: 1rem;
            font-style: italic;
        }

        .card-meta .badge {
            font-size: 0.85rem;
            font-weight: 600;
            padding: 0.5rem 1rem;
            margin-right: 0.5rem;
            letter-spacing: 0.3px;
        }

        .bg-teal {
            background-color: var(--color-teal) !important;
        }

        .bg-crimson {
            background-color: var(--color-crimson) !important;
        }

        .toc-box {
            background: #f8f9fa;
            border: 2px solid var(--color-teal);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 2rem 0;
        }

        .toc-box h3 {
            color: var(--color-navy);
            font-weight: 700;
            margin-bottom: 1rem;
            border: none;
            margin-top: 0;
        }

        .toc-box ol {
            margin-bottom: 0;
            padding-left: 1.5rem;
        }

        .toc-box li {
            margin-bottom: 0.5rem;
        }

        .toc-box a {
            color: var(--color-blue);
            text-decoration: none;
            transition: color 0.2s ease;
        }

        .toc-box a:hover {
            color: var(--color-crimson);
            text-decoration: underline;
        }

        .reading-time {
            display: inline-block;
            background: var(--color-crimson);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 4px;
            font-size: 0.9rem;
        }

        .back-link {
            display: inline-block;
            color: white;
            text-decoration: none;
            transition: all 0.3s ease;
            margin-bottom: 1rem;
            opacity: 0.9;
        }

        .back-link:hover {
            color: var(--color-teal);
            opacity: 1;
            transform: translateX(-5px);
        }

        .related-posts {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 2rem;
            margin-top: 3rem;
        }

        .related-posts h3 {
            color: var(--color-navy);
            margin-bottom: 1.5rem;
        }

        .related-post-item {
            padding: 1rem;
            border-left: 3px solid var(--color-teal);
            margin-bottom: 1rem;
            transition: all 0.3s ease;
        }

        .related-post-item:hover {
            background: white;
            border-left-color: var(--color-crimson);
        }

        .related-post-item a {
            color: var(--color-blue);
            text-decoration: none;
            font-weight: 600;
        }

        .related-post-item a:hover {
            color: var(--color-crimson);
        }

        /* Code Block Styles */
        pre[class*="language-"] {
            position: relative;
            margin: 1.5rem 0;
            padding-top: 3rem;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
        }

        code[class*="language-"] {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        /* Toolbar styling */
        div.code-toolbar > .toolbar {
            opacity: 1;
            display: flex;
            gap: 0.5rem;
        }

        div.code-toolbar > .toolbar > .toolbar-item > button {
            background: var(--color-teal);
            color: white;
            border: none;
            padding: 0.4rem 0.8rem;
            border-radius: 4px;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        div.code-toolbar > .toolbar > .toolbar-item > button:hover {
            background: var(--color-blue);
            transform: translateY(-1px);
        }

        div.code-toolbar > .toolbar > .toolbar-item > button:focus {
            outline: 2px solid var(--color-teal);
            outline-offset: 2px;
        }

        /* Theme switcher dropdown */
        div.code-toolbar > .toolbar > .toolbar-item > select {
            background: var(--color-navy);
            color: white;
            border: 1px solid var(--color-teal);
            padding: 0.4rem 0.8rem;
            border-radius: 4px;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.3s ease;
            outline: none;
        }

        div.code-toolbar > .toolbar > .toolbar-item > select:hover {
            background: var(--color-blue);
            border-color: var(--color-crimson);
        }

        div.code-toolbar > .toolbar > .toolbar-item > select:focus {
            outline: 2px solid var(--color-teal);
            outline-offset: 2px;
        }

        /* Style select options */
        div.code-toolbar > .toolbar > .toolbar-item > select option {
            background: var(--color-navy);
            color: white;
        }

        /* Scroll-to-Top Button */
        .scroll-to-top {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            background: var(--color-teal);
            color: white;
            border: none;
            border-radius: 50%;
            font-size: 1.2rem;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            box-shadow: 0 4px 12px rgba(59, 151, 151, 0.3);
            z-index: 999;
        }

        .scroll-to-top.show {
            opacity: 1;
            visibility: visible;
        }

        .scroll-to-top:hover {
            background: var(--color-crimson);
            transform: translateY(-3px);
            box-shadow: 0 6px 16px rgba(191, 9, 47, 0.4);
        }

        .scroll-to-top:active {
            transform: translateY(-1px);
        }

        @media (max-width: 768px) {
            .scroll-to-top {
                bottom: 1rem;
                right: 1rem;
                width: 45px;
                height: 45px;
                font-size: 1rem;
            }
        }

        .related-posts {
            margin-top: 3rem;
            padding-top: 2rem;
            border-top: 2px solid var(--color-teal);
        }

        .related-post-item {
            margin-bottom: 1rem;
        }

        .related-post-item a {
            color: var(--color-blue);
            text-decoration: none;
            font-weight: 600;
        }

        .related-post-item a:hover {
            color: var(--color-crimson);
        }
    </style>
</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript>
        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe>
    </noscript>

    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/">
                <span class="gradient-text">Wasil Zafar</span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="/">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#about">About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#skills">Skills</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#certifications">Certifications</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#interests">Interests</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link">
                    <i class="fas fa-arrow-left me-2"></i>Back to Technology
                </a>
                <h1 class="display-4 fw-bold mb-3">PyTorch Deep Learning: Complete Beginner's Guide to Building Neural Networks</h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 1, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-1"></i>40 min read</span>
                </div>
                <p class="lead">Master PyTorch from the ground up—learn tensors, automatic differentiation, neural network construction, training loops, CNNs, RNNs, transfer learning, and production deployment. A comprehensive hands-on guide with executable code examples.</p>
            </div>
        </div>
    </section>

    <!-- Main Content -->
    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    
                    <!-- Table of Contents -->
                    <div class="toc-box">
                        <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
                        <ol>
                            <li><a href="#introduction">What is PyTorch?</a></li>
                            <li><a href="#installation">Installation & Setup</a></li>
                            <li><a href="#tensors">Tensors: The Foundation</a></li>
                            <li><a href="#operations">Tensor Operations & Manipulations</a></li>
                            <li><a href="#autograd">Autograd: Automatic Differentiation</a></li>
                            <li><a href="#nn-module">Building Neural Networks with nn.Module</a></li>
                            <li><a href="#activations">Activation Functions & Initialization</a></li>
                            <li><a href="#optimizers">Optimizers & Learning Rate Scheduling</a></li>
                            <li><a href="#datasets">Datasets & DataLoaders</a></li>
                            <li><a href="#training-loop">The Training Loop</a></li>
                            <li><a href="#evaluation">Model Evaluation & Metrics</a></li>
                            <li><a href="#saving">Saving & Loading Models</a></li>
                            <li><a href="#gpu">GPU Acceleration & Device Management</a></li>
                            <li><a href="#mixed-precision">Mixed Precision Training</a></li>
                            <li><a href="#custom-dataset">Custom Datasets & Transforms</a></li>
                            <li><a href="#transfer-learning">Transfer Learning with Pretrained Models</a></li>
                            <li><a href="#cnn">Convolutional Neural Networks (CNNs)</a></li>
                            <li><a href="#rnn">Recurrent Neural Networks (RNNs & LSTMs)</a></li>
                            <li><a href="#embeddings">Embeddings for NLP</a></li>
                            <li><a href="#advanced">Advanced Training Techniques</a></li>
                            <li><a href="#custom-layers">Custom Layers & Loss Functions</a></li>
                            <li><a href="#transformers">Transformer Architectures (nn.Transformer)</a></li>
                            <li><a href="#vision-transformers">Vision Transformers (ViT)</a></li>
                            <li><a href="#attention-mechanisms">Advanced Attention Mechanisms</a></li>
                            <li><a href="#interpretability">Model Interpretability (Grad-CAM)</a></li>
                            <li><a href="#deployment">Deployment with TorchScript</a></li>
                            <li><a href="#best-practices">Best Practices & Common Pitfalls</a></li>
                        </ol>
                    </div>

                    <!-- Introduction -->
                    <div id="introduction" class="blog-content mt-5">
                        <h2><i class="fas fa-rocket me-2 text-teal"></i>What is PyTorch?</h2>
                        
                        <p>PyTorch is an open-source deep learning framework developed by Facebook's AI Research lab (FAIR). It has rapidly become one of the most popular frameworks in both research and production environments due to its <strong>flexibility, ease of use, and Pythonic design</strong>.</p>

                        <p>Unlike static graph frameworks, PyTorch uses <strong>dynamic computation graphs</strong> (define-by-run), which means the graph is built on-the-fly during forward passes. This makes debugging intuitive and enables complex architectures with varying control flow.</p>

                        <div class="highlight-box">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Why PyTorch?</strong> It combines the flexibility of NumPy with the power of GPU acceleration and automatic differentiation. You write standard Python code, and PyTorch handles gradient computation automatically—making it perfect for rapid prototyping and research.
                        </div>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">Key Concepts</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>Tensors:</strong> Multi-dimensional arrays (like NumPy) optimized for GPU computation</li>
                                    <li><strong>Autograd:</strong> Automatic differentiation engine for computing gradients</li>
                                    <li><strong>nn.Module:</strong> Base class for building neural network layers and models</li>
                                    <li><strong>Optimizers:</strong> Algorithms (SGD, Adam) that update model parameters</li>
                                    <li><strong>DataLoader:</strong> Efficient data loading with batching, shuffling, and parallel processing</li>
                                </ul>
                            </div>
                            <div class="card-tags">
                                <span class="bias-tag">Deep Learning</span>
                                <span class="bias-tag">Research-Friendly</span>
                                <span class="bias-tag">Production-Ready</span>
                            </div>
                        </div>

                        <p><strong>When to use PyTorch:</strong></p>
                        <ul>
                            <li>Building custom neural network architectures with complex control flow</li>
                            <li>Research projects requiring flexibility and rapid experimentation</li>
                            <li>Projects needing GPU acceleration for tensor operations</li>
                            <li>Production deployments with TorchScript for optimized inference</li>
                            <li>Computer vision, NLP, reinforcement learning, or any deep learning task</li>
                        </ul>
                    </div>

                    <!-- Installation -->
                    <div id="installation" class="blog-content mt-5">
                        <h2><i class="fas fa-download me-2 text-teal"></i>Installation & Setup</h2>
                        
                        <p>PyTorch installation varies based on your system configuration (CPU vs GPU, CUDA version). Visit <a href="https://pytorch.org/get-started/locally/" target="_blank">pytorch.org</a> for the latest installation commands tailored to your setup.</p>

                        <h3>CPU-Only Installation</h3>
                        <pre><code class="language-bash"># Install PyTorch (CPU version)
pip install torch torchvision torchaudio

# Verify installation
python -c "import torch; print('PyTorch version:', torch.__version__)"</code></pre>

                        <h3>GPU Installation (CUDA-enabled)</h3>
                        <pre><code class="language-bash"># Example: PyTorch with CUDA 12.1 support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Verify GPU availability
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"</code></pre>

                        <h3>Verify Installation</h3>
                        <pre><code class="language-python"># Import PyTorch and check configuration
import torch
import torchvision
import torchaudio
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# Display version and device information
print('PyTorch version:', torch.__version__)
print('CUDA available:', torch.cuda.is_available())
print('CUDA device count:', torch.cuda.device_count())
if torch.cuda.is_available():
    print('Current CUDA device:', torch.cuda.current_device())
    print('Device name:', torch.cuda.get_device_name(0))</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-info-circle"></i>
                            <strong>GPU Recommendation:</strong> If you have an NVIDIA GPU with CUDA support, installing the GPU version will significantly speed up training. Deep learning benefits massively from parallel GPU computation—training times can be 10-100x faster compared to CPU.
                        </div>
                    </div>

                    <!-- Tensors -->
                    <div id="tensors" class="blog-content mt-5">
                        <h2><i class="fas fa-cubes me-2 text-teal"></i>Tensors: The Foundation</h2>
                        
                        <p>Tensors are the fundamental data structure in PyTorch—multi-dimensional arrays similar to NumPy ndarrays but with GPU acceleration and automatic differentiation support. Everything in PyTorch operates on tensors. Think of a tensor as a generalization: scalars are 0D tensors, vectors are 1D tensors, matrices are 2D tensors, and arrays with more dimensions are higher-order tensors.</p>

                        <div class="highlight-box">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Tensor Basics:</strong> A tensor is a container for numerical data. The shape describes its dimensions: a tensor of shape [3, 4] has 3 rows and 4 columns. The dtype (data type) specifies what kind of numbers it stores (float32, int64, etc.). The device tells you where it lives (CPU or GPU).
                        </div>

                        <h3>Creating Tensors</h3>
                        <p>PyTorch provides multiple ways to create tensors. Each method has different use cases:</p>

                        <pre><code class="language-python">import torch
import numpy as np

# METHOD 1: From a Python list
# torch.tensor() converts Python lists to PyTorch tensors
# Parameters:
#   - data: the actual values (list or nested list)
#   - dtype: data type (torch.int32, torch.float32, torch.float64, etc.)
a = torch.tensor([1, 2, 3], dtype=torch.int32)
print('Tensor a:', a)
# Output: tensor([1, 2, 3], dtype=torch.int32)

# METHOD 2: Create with specific shape and random values
# torch.randn() fills tensor with random values from normal distribution (mean=0, std=1)
# Parameters:
#   - *size: dimensions of the tensor (variadic - can be any number of dimensions)
b = torch.randn(2, 3)  # Creates a 2x3 tensor (2 rows, 3 columns)
print('Tensor b shape:', b.shape)  # torch.Size([2, 3])
print('Tensor b dtype:', b.dtype)  # torch.float32 (default for randn)
print('Tensor b:\n', b)

# METHOD 3: Create with specific values
# torch.zeros() creates a tensor filled with zeros
# torch.ones() creates a tensor filled with ones
zeros = torch.zeros(2, 3)  # 2x3 tensor of all zeros
ones = torch.ones(2, 3)   # 2x3 tensor of all ones
print('Zeros:\n', zeros)
print('Ones:\n', ones)</code></pre>

                        <h3>NumPy Interoperability</h3>
                        <p>PyTorch and NumPy work seamlessly together. You can convert between them easily, which is useful when working with existing NumPy code or libraries.</p>

                        <pre><code class="language-python">import torch
import numpy as np

# Convert NumPy array to PyTorch tensor
# torch.from_numpy() is efficient—it shares memory, not copying data
np_array = np.array([[10, 20], [30, 40]], dtype=np.float32)
tensor_from_numpy = torch.from_numpy(np_array)
print('From NumPy:', tensor_from_numpy)
# Output: tensor([[10., 20.], [30., 40.]])

# Convert PyTorch tensor back to NumPy array
# WARNING: This shares memory with the original tensor!
# If you modify one, the other changes too
back_to_numpy = tensor_from_numpy.numpy()
print('Back to NumPy:', back_to_numpy)
print('Type:', type(back_to_numpy))  # <class 'numpy.ndarray'>

# Demonstrate shared memory (they point to same data)
tensor_from_numpy[0, 0] = 99  # Modify tensor
print('Modified NumPy array:', back_to_numpy)  # [[99., 20.], [30., 40.]] - changed!

# If you need a separate copy (not sharing memory), use .clone()
independent_copy = torch.from_numpy(np_array).clone()
independent_copy[0, 0] = 999  # Modifying copy doesn't affect original
print('Original unchanged:', np_array[0, 0])  # Still 10</code></pre>

                        <h3>Common Tensor Creation Functions</h3>
                        <p>PyTorch provides helper functions for quickly creating tensors with specific properties. These are essential for initialization and testing:</p>
                        
                        <pre><code class="language-python">import torch

# torch.zeros(shape) - all elements are 0
# Use for: initializing bias terms, creating masks
zeros = torch.zeros(3, 4)  # 3x4 matrix of zeros
print('Zeros shape:', zeros.shape)  # torch.Size([3, 4])
print('Zeros:\n', zeros)

# torch.ones(shape) - all elements are 1
# Use for: creating ones masks, initialization
ones = torch.ones(2, 3)
print('Ones:\n', ones)

# torch.empty(shape) - uninitialized values (faster but garbage values)
# Use for: performance-critical code when values will be filled immediately
# WARNING: values are undefined—whatever was in memory!
empty = torch.empty(2, 2)
print('Empty (uninitialized, unpredictable values):\n', empty)

# torch.arange(start, end, step) - like NumPy's range
# Creates tensor with evenly spaced values [start, end)
# Note: end is EXCLUSIVE (not included)
range_tensor = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]
print('Range (0 to 10, step 2):', range_tensor)

# torch.linspace(start, end, steps) - evenly spaced values BOTH endpoints included
# Creates exactly 'steps' number of values from start to end
linspace = torch.linspace(0, 1, 5)  # 5 values from 0 to 1
print('Linspace (5 values from 0 to 1):', linspace)  # [0.0, 0.25, 0.5, 0.75, 1.0]

# torch.eye(n) - identity matrix (diagonal ones, rest zeros)
# Use for: initializing attention weights, creating masks
identity = torch.eye(3)  # 3x3 identity matrix
print('Identity (3x3):\n', identity)

# torch.rand(shape) - random values from uniform distribution [0, 1)
# Use for: random initialization, Monte Carlo sampling
rand_uniform = torch.rand(2, 3)  # Random values in [0, 1)
print('Random uniform:\n', rand_uniform)

# torch.randn(shape) - random values from standard normal (Gaussian)
# Use for: neural network weight initialization, simulations
# Distribution: mean=0, std=1 (bell curve)
rand_normal = torch.randn(2, 3)
print('Random normal (mean=0, std=1):\n', rand_normal)

# torch.randint(low, high, shape) - random integers in [low, high)
# Parameters:
#   - low: minimum value (inclusive)
#   - high: maximum value (exclusive)
#   - shape: tuple of output shape
rand_int = torch.randint(0, 10, (3, 3))  # Random integers between 0 and 9
print('Random integers (0-9):\n', rand_int)</code></pre>

                        <h3>Tensor Attributes & Properties</h3>
                        <p>Every tensor has several attributes that describe its properties. Understanding these is crucial for debugging shape mismatches and device placement errors:</p>
                        
                        <pre><code class="language-python">import torch

# Create sample tensor to inspect
tensor = torch.randn(2, 3, 4)  # 3D tensor with dimensions 2, 3, 4

# tensor.shape - dimensions of the tensor as a torch.Size object
# torch.Size is like a tuple of integers
print('Shape:', tensor.shape)  # torch.Size([2, 3, 4])
print('Shape as list:', list(tensor.shape))  # [2, 3, 4]

# tensor.size() - same as .shape (alternative method)
print('Size:', tensor.size())  # torch.Size([2, 3, 4])

# tensor.ndim - number of dimensions (rank)
# 1D tensor: ndim=1 (vector)
# 2D tensor: ndim=2 (matrix)
# 3D tensor: ndim=3 (cube)
print('Number of dimensions:', tensor.ndim)  # 3

# tensor.numel() - total number of elements (product of all dimensions)
# Useful for calculating memory usage: memory = numel() * bytes_per_element
total_elements = tensor.numel()  # 2 * 3 * 4 = 24
print('Total elements:', total_elements)  # 24

# tensor.dtype - data type of elements
# Common types: torch.float32 (4 bytes), torch.float64 (8 bytes),
#              torch.int32 (4 bytes), torch.int64 (8 bytes)
print('Data type:', tensor.dtype)  # torch.float32

# tensor.device - where the tensor lives (CPU or GPU)
# device='cpu': tensor is in system RAM
# device='cuda:0': tensor is on first GPU's VRAM
print('Device:', tensor.device)  # cpu or cuda:0

# tensor.requires_grad - whether gradients will be tracked for this tensor
# True: gradients computed during backprop (for parameters)
# False: no gradients (for inputs or fixed tensors)
print('Requires grad:', tensor.requires_grad)  # False by default

# tensor.is_contiguous() - whether elements are laid out consecutively in memory
# Contiguous tensors are faster for operations
# Non-contiguous after transpose, slicing, etc.
print('Is contiguous:', tensor.is_contiguous())  # True for new tensors</code></pre>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">Tensor Concepts</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>Shape vs Size:</strong> Both give dimensions, but .shape returns torch.Size (like tuple), .size() returns torch.Size too—they're equivalent</li>
                                    <li><strong>dtype matters:</strong> float32 vs float64 affects memory and precision. Use float32 for speed, float64 for high precision</li>
                                    <li><strong>Device placement:</strong> Model and data MUST be on same device. Mismatch causes "expected CPU tensor" errors</li>
                                    <li><strong>requires_grad:</strong> Set True for learnable parameters, False for inputs. Saves memory and computation</li>
                                    <li><strong>Memory sharing:</strong> from_numpy() shares memory; .clone() makes independent copy</li>
                                </ul>
                            </div>
                        </div>
tensor_from_numpy = torch.from_numpy(np_array)
print('From NumPy:', tensor_from_numpy)
# Output: tensor([[10., 20.], [30., 40.]])

# Convert PyTorch tensor back to NumPy array
# WARNING: This shares memory with the original tensor!
back_to_numpy = tensor_from_numpy.numpy()
print('Back to NumPy:', back_to_numpy)
print('Type:', type(back_to_numpy))  # <class 'numpy.ndarray'>

# Modifying one affects the other (shared memory)
tensor_from_numpy[0, 0] = 99
print('Modified NumPy array:', back_to_numpy)  # [[99., 20.], [30., 40.]]</code></pre>

                        <h3>Common Tensor Creation Functions</h3>
                        <pre><code class="language-python">import torch

# Zeros tensor: all elements are 0
zeros = torch.zeros(3, 4)  # 3x4 matrix of zeros
print('Zeros shape:', zeros.shape)  # torch.Size([3, 4])

# Ones tensor: all elements are 1
ones = torch.ones(2, 3)
print('Ones:\n', ones)

# Empty tensor: uninitialized values (whatever was in memory)
# Faster than zeros/ones but values are garbage
empty = torch.empty(2, 2)
print('Empty (uninitialized):\n', empty)

# Range tensor: evenly spaced values (like NumPy arange)
# arange(start, end, step) - end is exclusive
range_tensor = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]
print('Range:', range_tensor)

# Linspace: evenly spaced values including both endpoints
linspace = torch.linspace(0, 1, 5)  # 5 values from 0 to 1
print('Linspace:', linspace)  # [0.0, 0.25, 0.5, 0.75, 1.0]

# Identity matrix: diagonal ones, rest zeros
identity = torch.eye(3)  # 3x3 identity matrix
print('Identity:\n', identity)

# Random tensors
rand_uniform = torch.rand(2, 3)  # Uniform distribution [0, 1)
rand_normal = torch.randn(2, 3)  # Standard normal distribution (mean=0, std=1)
rand_int = torch.randint(0, 10, (3, 3))  # Random integers in [0, 10)
print('Random uniform:\n', rand_uniform)
print('Random normal:\n', rand_normal)
print('Random integers:\n', rand_int)</code></pre>

                        <h3>Tensor Attributes</h3>
                        <pre><code class="language-python">import torch

# Create sample tensor
tensor = torch.randn(2, 3, 4)  # 3D tensor

# Shape: dimensions of the tensor
print('Shape:', tensor.shape)  # torch.Size([2, 3, 4])
print('Size:', tensor.size())  # Same as shape: torch.Size([2, 3, 4])

# Number of dimensions
print('Number of dimensions:', tensor.ndim)  # 3

# Total number of elements
print('Total elements:', tensor.numel())  # 2 * 3 * 4 = 24

# Data type
print('Data type:', tensor.dtype)  # torch.float32

# Device (CPU or GPU)
print('Device:', tensor.device)  # cpu or cuda:0

# Check if requires gradient tracking
print('Requires grad:', tensor.requires_grad)  # False by default

# Memory layout (row-major by default)
print('Is contiguous:', tensor.is_contiguous())  # True</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-exclamation-triangle"></i>
                            <strong>Memory Efficiency:</strong> Tensors created with <code>from_numpy()</code> share memory with the original NumPy array. Modifying one affects the other. Use <code>.clone()</code> if you need an independent copy: <code>independent = tensor_from_numpy.clone()</code>
                        </div>
                    </div>

                    <!-- Tensor Operations -->
                    <div id="operations" class="blog-content mt-5">
                        <h2><i class="fas fa-calculator me-2 text-teal"></i>Tensor Operations & Manipulations</h2>
                        
                        <p>PyTorch provides a rich set of operations for manipulating tensors—from basic arithmetic to advanced reshaping and slicing. Most operations have both functional (<code>torch.add()</code>) and method (<code>tensor.add()</code>) forms.</p>

                        <h3>Reshaping & Views</h3>
                        <pre><code class="language-python">import torch

# Create 1D tensor with 12 elements
x = torch.arange(12)  # [0, 1, 2, ..., 11]
print('Original:', x)

# Reshape to 2D (3 rows, 4 columns)
# view() returns a new tensor sharing the same data (no copy)
x_2d = x.view(3, 4)
print('Reshaped to 3x4:\n', x_2d)
# tensor([[ 0,  1,  2,  3],
#         [ 4,  5,  6,  7],
#         [ 8,  9, 10, 11]])

# Transpose: swap dimensions
# t() works only for 2D tensors
x_transposed = x_2d.t()
print('Transposed (4x3):\n', x_transposed)

# Automatic dimension inference with -1
# PyTorch calculates the missing dimension
auto_shape = x.view(4, -1)  # -1 means "figure this out" (becomes 3)
print('Auto-shaped (4, 3):', auto_shape.shape)  # torch.Size([4, 3])

# WARNING: view() requires contiguous memory
# If tensor is not contiguous, use reshape() instead
x_permuted = x_2d.permute(1, 0)  # Swap dimensions (now non-contiguous)
try:
    x_view = x_permuted.view(-1)  # This will fail!
except RuntimeError as e:
    print('Error:', str(e)[:50])  # "view size is not compatible..."

# reshape() handles non-contiguous tensors (may copy data)
x_reshaped = x_permuted.reshape(-1)
print('Reshaped from non-contiguous:', x_reshaped.shape)</code></pre>

                        <h3>Slicing & Indexing</h3>
                        <pre><code class="language-python">import torch

# Create 2D tensor
matrix = torch.tensor([[1, 2, 3],
                       [4, 5, 6],
                       [7, 8, 9]])

# Access single element (returns 0D tensor)
element = matrix[1, 2]  # Row 1, Column 2 (value: 6)
print('Element [1, 2]:', element)  # tensor(6)
print('As Python scalar:', element.item())  # 6

# Access entire row
row = matrix[0, :]  # Row 0, all columns
print('First row:', row)  # tensor([1, 2, 3])

# Access entire column
column = matrix[:, 1]  # All rows, column 1
print('Second column:', column)  # tensor([2, 5, 8])

# Slice submatrix
submatrix = matrix[0:2, 1:3]  # Rows 0-1, columns 1-2
print('Submatrix:\n', submatrix)
# tensor([[2, 3],
#         [5, 6]])

# Advanced indexing with lists
rows = matrix[[0, 2], :]  # Select rows 0 and 2
print('Rows 0 and 2:\n', rows)

# Boolean masking
mask = matrix > 5  # Create boolean mask
print('Mask (elements > 5):\n', mask)
filtered = matrix[mask]  # Extract elements where mask is True
print('Filtered values:', filtered)  # tensor([6, 7, 8, 9])</code></pre>

                        <h3>Arithmetic Operations (Element-wise)</h3>
                        <pre><code class="language-python">import torch

# Create sample tensors
a = torch.tensor([1, 2, 3, 4])
b = torch.tensor([10, 20, 30, 40])

# Element-wise addition
print('Addition:', a + b)  # tensor([11, 22, 33, 44])

# Element-wise multiplication
print('Multiplication:', a * b)  # tensor([10, 40, 90, 160])

# Element-wise division
print('Division:', b / a)  # tensor([10., 10., 10., 10.])

# Power
print('a squared:', a ** 2)  # tensor([1, 4, 9, 16])

# Universal functions (ufuncs)
print('Square root:', torch.sqrt(a.float()))  # tensor([1.0, 1.414, 1.732, 2.0])
print('Exponential:', torch.exp(a.float()))  # tensor([2.718, 7.389, 20.086, 54.598])
print('Sine:', torch.sin(a.float()))  # tensor([0.841, 0.909, 0.141, -0.757])
print('Natural log:', torch.log(a.float()))  # tensor([0.0, 0.693, 1.099, 1.386])

# In-place operations (modify tensor directly, denoted by underscore suffix)
a.add_(10)  # Add 10 to all elements in-place
print('After in-place add:', a)  # tensor([11, 12, 13, 14])</code></pre>

                        <h3>Matrix Operations</h3>
                        <pre><code class="language-python">import torch

# Matrix multiplication (dot product)
A = torch.tensor([[1, 2], [3, 4]])  # 2x2 matrix
B = torch.tensor([[5, 6], [7, 8]])  # 2x2 matrix

# @ operator for matrix multiplication (recommended)
C = A @ B
print('Matrix multiplication (A @ B):\n', C)
# tensor([[19, 22],
#         [43, 50]])

# Alternative: torch.matmul()
C_alt = torch.matmul(A, B)
print('Same result:', torch.equal(C, C_alt))  # True

# Batch matrix multiplication (3D tensors)
batch_a = torch.randn(10, 3, 4)  # 10 matrices of size 3x4
batch_b = torch.randn(10, 4, 5)  # 10 matrices of size 4x5
batch_c = batch_a @ batch_b  # Result: 10 matrices of size 3x5
print('Batch matmul shape:', batch_c.shape)  # torch.Size([10, 3, 5])

# Element-wise matrix multiplication (Hadamard product)
hadamard = A * B  # NOT matrix multiplication!
print('Element-wise multiplication:\n', hadamard)
# tensor([[ 5, 12],
#         [21, 32]])</code></pre>

                        <h3>Concatenation & Stacking</h3>
                        <pre><code class="language-python">import torch

# Create sample tensors
x = torch.tensor([[1, 2], [3, 4]])
y = torch.tensor([[5, 6], [7, 8]])

# Concatenate along rows (dimension 0)
# Stacks vertically - increases number of rows
concat_rows = torch.cat([x, y], dim=0)
print('Concatenate rows (dim=0):\n', concat_rows)
# tensor([[1, 2],
#         [3, 4],
#         [5, 6],
#         [7, 8]])
print('Shape:', concat_rows.shape)  # torch.Size([4, 2])

# Concatenate along columns (dimension 1)
# Stacks horizontally - increases number of columns
concat_cols = torch.cat([x, y], dim=1)
print('Concatenate columns (dim=1):\n', concat_cols)
# tensor([[1, 2, 5, 6],
#         [3, 4, 7, 8]])
print('Shape:', concat_cols.shape)  # torch.Size([2, 4])

# Stack: adds new dimension
# Creates 3D tensor by stacking 2D tensors
stacked = torch.stack([x, y], dim=0)
print('Stacked (dim=0) shape:', stacked.shape)  # torch.Size([2, 2, 2])
print('Stacked:\n', stacked)
# tensor([[[1, 2],
#          [3, 4]],
#         [[5, 6],
#          [7, 8]]])</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-bolt"></i>
                            <strong>Performance Tip:</strong> In-place operations (suffix <code>_</code>) modify tensors directly without allocating new memory. Use them carefully—they save memory but can interfere with autograd if the tensor requires gradients. Avoid in-place ops on tensors involved in gradient computation.
                        </div>
                    </div>

                    <!-- Autograd -->
                    <div id="autograd" class="blog-content mt-5">
                        <h2><i class="fas fa-project-diagram me-2 text-teal"></i>Autograd: Automatic Differentiation</h2>
                        
                        <p>Autograd is PyTorch's automatic differentiation engine—the magic behind neural network training. It automatically computes gradients (derivatives) of tensor operations, eliminating the need for manual backpropagation calculations. Without Autograd, you'd have to manually compute how changes in weights affect the loss function—a tedious and error-prone task.</p>

                        <div class="highlight-box">
                            <i class="fas fa-lightbulb"></i>
                            <strong>How Autograd Works:</strong> When you set <code>requires_grad=True</code> on a tensor, PyTorch records every operation. This creates a "computational graph"—a record of how the final output depends on the inputs. Calling <code>.backward()</code> traces backwards through this graph to compute how much each parameter should change to reduce the loss.
                        </div>

                        <h3>Basic Gradient Computation</h3>
                        <p>Gradients measure how a function changes with respect to its inputs. In neural networks, we use gradients to update weights in the direction that reduces loss:</p>

                        <pre><code class="language-python">import torch

# Step 1: Create tensor with gradient tracking enabled
# requires_grad=True tells PyTorch: "I want to compute gradients for this"
# Without this flag, no gradients are tracked (saves memory)
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
print('x:', x)
print('x.requires_grad:', x.requires_grad)  # True

# Step 2: Perform operations to build computation graph
# PyTorch records each operation in a graph structure
y = x ** 2  # y = x²  (element-wise squaring)
print('y:', y)  # tensor([1., 4., 9.], grad_fn=<PowBackward0>)
# Note: grad_fn shows HOW this tensor was created

z = y.sum()  # z = sum(y) = x₁² + x₂² + x₃² = 1 + 4 + 9 = 14
print('z:', z)  # tensor(14., grad_fn=<SumBackward0>)

# Step 3: Compute gradients via BACKPROPAGATION
# .backward() starts at z and traces backwards to x
# It computes dz/dx (how z changes with respect to x)
z.backward()

# Step 4: Access computed gradients
# x.grad contains dz/dx for each element of x
print('x.grad:', x.grad)  # tensor([2., 4., 6.])

# Explanation of results:
# If x = [1, 2, 3]:
# z = x₁² + x₂² + x₃² = 1 + 4 + 9 = 14
# dz/dx₁ = 2*x₁ = 2*1 = 2 ✓
# dz/dx₂ = 2*x₂ = 2*2 = 4 ✓
# dz/dx₃ = 2*x₃ = 2*3 = 6 ✓
print('Gradients match mathematical derivative: dz/dx = 2x')</code></pre>

                        <h3>Multiple Backward Passes (Gradient Accumulation)</h3>
                        <p>By default, gradients accumulate (add up) when you call .backward() multiple times. This is useful for training with gradient accumulation but requires careful management:</p>

                        <pre><code class="language-python">import torch

# Create a tensor with gradient tracking
# A single call to .backward() computes gradients
# But subsequent .backward() calls ADD to existing gradients
x = torch.tensor([1.0, 2.0], requires_grad=True)

# First computation and backward pass
y1 = (x ** 2).sum()  # y1 = 1² + 2² = 5
y1.backward()  # Compute dy1/dx
print('After first backward:', x.grad)  # tensor([2., 4.])
# Gradients: [2*1, 2*2] = [2, 4]

# Second computation WITHOUT zeroing gradients
# IMPORTANT: Gradients ACCUMULATE (add to existing values)
y2 = (x ** 3).sum()  # y2 = 1³ + 2³ = 9
y2.backward()  # Compute dy2/dx AND ADD to existing gradients
print('After second backward (accumulated):', x.grad)  # tensor([5., 16.])
# Why? [2 + 3, 4 + 12] = [2+3*1², 4+3*2²] = [5, 16]
# This is because gradients ADD up: d(y1+y2)/dx = dy1/dx + dy2/dx

# CRITICAL LESSON: In training loops, you MUST ZERO gradients
# before each new loss computation to prevent accumulation!
x.grad.zero_()  # Reset all gradients to zero
print('After zeroing:', x.grad)  # tensor([0., 0.])

# Now compute fresh gradients for a new batch
y3 = (x ** 2).sum()
y3.backward()
print('Fresh gradients (no accumulation):', x.grad)  # tensor([2., 4.]) - clean slate</code></pre>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">Autograd Concepts</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>requires_grad=True:</strong> Enable gradient tracking for a tensor. Only needed for parameters, not inputs</li>
                                    <li><strong>Computational Graph:</strong> Internal record of operations. Enables .backward() to trace back and compute derivatives</li>
                                    <li><strong>.backward():</strong> Computes gradients by traversing the graph. Must be called on a scalar (single number)</li>
                                    <li><strong>Gradient Accumulation:</strong> .grad += new_gradients (adds up). Always call .zero_grad() before new forward pass</li>
                                    <li><strong>.detach():</strong> Remove tensor from graph; use when you want values without gradient computation</li>
                                </ul>
                            </div>
                        </div>

# Second computation WITHOUT zeroing gradients
# Gradients will ACCUMULATE (add to existing gradients)
y2 = (x ** 3).sum()
y2.backward()
print('After second backward (accumulated):', x.grad)  # tensor([5., 16.])
# Expected: [2 + 3*1², 4 + 3*2²] = [2+3, 4+12] = [5, 16]

# CRITICAL: Always zero gradients before new backward pass in training!
x.grad.zero_()  # Reset gradients to zero
y3 = (x ** 2).sum()
y3.backward()
print('After zeroing and new backward:', x.grad)  # tensor([2., 4.])</code></pre>

                        <h3>Detaching from Computation Graph</h3>
                        <pre><code class="language-python">import torch

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = x ** 2

# Detach: create a tensor without gradient tracking
# Useful when you want to use values without affecting gradients
y_detached = y.detach()
print('y_detached.requires_grad:', y_detached.requires_grad)  # False

# Operations on detached tensor won't be tracked
z = y_detached * 2
# z.backward() would fail because z has no grad_fn

# Alternative: torch.no_grad() context manager
# Temporarily disables gradient tracking for all operations
with torch.no_grad():
    y_no_grad = x ** 3
    z_no_grad = y_no_grad.sum()
    print('z_no_grad.requires_grad:', z_no_grad.requires_grad)  # False

# Use no_grad during inference to save memory and speed up computation</code></pre>

                        <h3>Gradient Flow Example</h3>
                        <pre><code class="language-python">import torch

# Simulate a simple neural network computation
# Input → Weight multiplication → Activation → Loss

# Create learnable weights (parameters)
w = torch.randn(3, 3, requires_grad=True)
b = torch.randn(3, requires_grad=True)

# Input data (no gradient needed for inputs)
x = torch.randn(5, 3)  # 5 samples, 3 features

# Forward pass: compute predictions
# Linear transformation: y = xW + b
out = x @ w + b  # @ is matrix multiplication
print('Output shape:', out.shape)  # torch.Size([5, 3])

# Compute loss (mean squared error)
# In real training, you'd compare with actual labels
loss = out.pow(2).mean()  # L = mean((out)²)
print('Loss:', loss.item())  # Scalar value

# Backward pass: compute gradients
loss.backward()

# Check gradients
print('w.grad shape:', w.grad.shape)  # torch.Size([3, 3])
print('b.grad shape:', b.grad.shape)  # torch.Size([3])
print('Weight gradient (first 3 values):', w.grad.flatten()[:3])

# These gradients tell us how to adjust w and b to reduce loss
# Optimizers use these gradients to update parameters</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-brain"></i>
                            <strong>How Autograd Works:</strong> PyTorch builds a <strong>Dynamic Computation Graph</strong> during the forward pass. Each operation creates a node storing the operation and its inputs. When you call <code>.backward()</code>, PyTorch traverses this graph in reverse (backpropagation), applying the chain rule to compute gradients for all tensors with <code>requires_grad=True</code>.
                        </div>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">Common Autograd Patterns</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>Training:</strong> Enable gradients with <code>requires_grad=True</code> for parameters</li>
                                    <li><strong>Inference:</strong> Disable gradients with <code>torch.no_grad()</code> to save memory</li>
                                    <li><strong>Zero Gradients:</strong> Call <code>optimizer.zero_grad()</code> before each backward pass</li>
                                    <li><strong>Gradient Clipping:</strong> Prevent exploding gradients with <code>torch.nn.utils.clip_grad_norm_()</code></li>
                                    <li><strong>Detach:</strong> Use <code>.detach()</code> to break gradient flow when needed</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- nn.Module -->
                    <div id="nn-module" class="blog-content mt-5">
                        <h2><i class="fas fa-network-wired me-2 text-teal"></i>Building Neural Networks with nn.Module</h2>
                        
                        <p>The <code>nn.Module</code> class is the foundation for building neural networks in PyTorch. All models inherit from this base class, which provides essential functionality for parameter management, device transfer, and training/evaluation modes.</p>

                        <h3>Basic Neural Network Structure</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Define a simple feedforward neural network
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNet, self).__init__()  # Initialize parent class
        
        # Define layers as instance attributes
        # PyTorch automatically tracks these as parameters
        self.fc1 = nn.Linear(input_size, hidden_size)  # Input → Hidden
        self.fc2 = nn.Linear(hidden_size, output_size)  # Hidden → Output
        self.relu = nn.ReLU()  # Activation function
    
    def forward(self, x):
        # Define forward pass: how data flows through the network
        x = self.fc1(x)      # Apply first linear transformation
        x = self.relu(x)     # Apply activation
        x = self.fc2(x)      # Apply second linear transformation
        return x

# Create model instance
model = SimpleNet(input_size=10, hidden_size=20, output_size=5)
print(model)
# Output shows model architecture:
# SimpleNet(
#   (fc1): Linear(in_features=10, out_features=20, bias=True)
#   (fc2): Linear(in_features=20, out_features=5, bias=True)
#   (relu): ReLU()
# )

# Count total parameters
total_params = sum(p.numel() for p in model.parameters())
print(f'Total parameters: {total_params}')
# fc1: 10*20 + 20 = 220, fc2: 20*5 + 5 = 105, Total: 325</code></pre>

                        <h3>Forward Pass Example</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Create model
class SimpleNet(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNet, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

model = SimpleNet(input_size=10, hidden_size=20, output_size=5)

# Create batch of input data (32 samples, 10 features each)
batch = torch.randn(32, 10)

# Forward pass: model(batch) calls forward() automatically
output = model(batch)
print('Output shape:', output.shape)  # torch.Size([32, 5])
print('Output (first sample):', output[0])  # 5 logits for this sample</code></pre>

                        <h3>nn.Sequential: Quick Model Building</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Sequential allows building models without defining forward()
# Layers execute in order automatically
model = nn.Sequential(
    nn.Linear(10, 20),      # Layer 1
    nn.ReLU(),              # Activation 1
    nn.Linear(20, 20),      # Layer 2
    nn.ReLU(),              # Activation 2
    nn.Linear(20, 5)        # Output layer
)

print(model)

# Forward pass
x = torch.randn(32, 10)
output = model(x)
print('Output shape:', output.shape)  # torch.Size([32, 5])

# Access individual layers
print('First layer:', model[0])  # Linear(in_features=10, out_features=20)
print('First layer weights shape:', model[0].weight.shape)  # torch.Size([20, 10])</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-code"></i>
                            <strong>When to use Sequential vs Custom nn.Module:</strong> Use <code>nn.Sequential</code> for simple feed-forward architectures with linear data flow. Use custom <code>nn.Module</code> classes when you need complex control flow, skip connections (ResNet), multiple inputs/outputs, or custom forward logic.
                        </div>
                    </div>

                    <!-- Activations -->
                    <div id="activations" class="blog-content mt-5">
                        <h2><i class="fas fa-wave-square me-2 text-teal"></i>Activation Functions & Initialization</h2>
                        
                        <p>Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns. Weight initialization determines starting values for parameters and significantly impacts training convergence.</p>

                        <h3>Common Activation Functions</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Create sample input
x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])

# ReLU: max(0, x) - most common, fast, avoids vanishing gradients
relu = nn.ReLU()
print('ReLU:', relu(x))  # tensor([0., 0., 0., 1., 2.])

# Leaky ReLU: allows small negative values (0.01*x when x < 0)
leaky_relu = nn.LeakyReLU(negative_slope=0.01)
print('Leaky ReLU:', leaky_relu(x))  # tensor([-0.02, -0.01, 0., 1., 2.])

# Sigmoid: squashes values to (0, 1) - used in binary classification
sigmoid = nn.Sigmoid()
print('Sigmoid:', sigmoid(x))  # tensor([0.119, 0.269, 0.5, 0.731, 0.881])

# Tanh: squashes values to (-1, 1) - zero-centered
tanh = nn.Tanh()
print('Tanh:', tanh(x))  # tensor([-0.964, -0.762, 0., 0.762, 0.964])

# Softmax: converts logits to probabilities (sum to 1)
# Used in multi-class classification output layer
softmax = nn.Softmax(dim=0)
print('Softmax:', softmax(x))  # tensor([0.012, 0.032, 0.087, 0.236, 0.643])
print('Sum:', softmax(x).sum())  # tensor(1.0)</code></pre>

                        <h3>Weight Initialization</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Create a linear layer
layer = nn.Linear(10, 20)

# Default initialization (varies by layer type)
print('Default weights (first 5):', layer.weight.data.flatten()[:5])
print('Default bias (first 5):', layer.bias.data[:5])

# Xavier/Glorot initialization (good for tanh/sigmoid)
nn.init.xavier_uniform_(layer.weight)
print('Xavier weights (first 5):', layer.weight.data.flatten()[:5])

# He initialization (good for ReLU)
nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
print('He weights (first 5):', layer.weight.data.flatten()[:5])

# Constant initialization
nn.init.constant_(layer.bias, 0.0)  # Initialize all biases to 0
print('Constant bias:', layer.bias.data[:5])

# Custom initialization in model
class CustomInitNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 20)
        self.fc2 = nn.Linear(20, 10)
        
        # Apply custom initialization
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.kaiming_normal_(self.fc2.weight)
        nn.init.zeros_(self.fc1.bias)
        nn.init.zeros_(self.fc2.bias)
    
    def forward(self, x):
        return self.fc2(torch.relu(self.fc1(x)))

model = CustomInitNet()
print('Model initialized with custom weights')</code></pre>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">Activation Function Cheat Sheet</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>ReLU:</strong> Default choice for hidden layers. Fast, avoids vanishing gradients. Can suffer from "dying ReLU" problem.</li>
                                    <li><strong>Leaky ReLU:</strong> Fixes dying ReLU by allowing small negative values. Good alternative to ReLU.</li>
                                    <li><strong>Sigmoid:</strong> Output layer for binary classification. Suffers from vanishing gradients in deep networks.</li>
                                    <li><strong>Tanh:</strong> Zero-centered sigmoid. Better than sigmoid for hidden layers but still has vanishing gradient issues.</li>
                                    <li><strong>Softmax:</strong> Output layer for multi-class classification. Converts logits to probabilities.</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Optimizers -->
                    <div id="optimizers" class="blog-content mt-5">
                        <h2><i class="fas fa-cogs me-2 text-teal"></i>Optimizers & Learning Rate Scheduling</h2>
                        
                        <p>Optimizers update model parameters based on computed gradients. PyTorch provides many optimization algorithms, each with different convergence properties and hyperparameters.</p>

                        <h3>Common Optimizers</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim

# Create a simple model
model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 5)
)

# SGD (Stochastic Gradient Descent)
# Simple, requires careful learning rate tuning
optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
print('SGD optimizer:', optimizer_sgd)

# Adam (Adaptive Moment Estimation)
# Most popular, adapts learning rate per parameter
# Good default choice for most tasks
optimizer_adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
print('Adam optimizer:', optimizer_adam)

# AdamW (Adam with weight decay fix)
# Better generalization than Adam
optimizer_adamw = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
print('AdamW optimizer:', optimizer_adamw)

# RMSprop (Root Mean Square Propagation)
# Good for RNNs, adapts learning rates
optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001)
print('RMSprop optimizer:', optimizer_rmsprop)</code></pre>

                        <h3>Optimizer Usage Pattern: The Complete Training Loop</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim

# Create model and optimizer
model = nn.Sequential(nn.Linear(10, 5))
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()  # Loss function

# TRAINING LOOP: Standard pattern in PyTorch
for epoch in range(10):
    # STEP 1: ZERO GRADIENTS
    # Gradients accumulate by default (important for multi-task learning)
    # But usually we want fresh gradients each iteration
    # Call zero_grad() to reset all gradients to 0
    optimizer.zero_grad()
    
    # STEP 2: FORWARD PASS
    # Create sample data (batch_size=32, features=10)
    inputs = torch.randn(32, 10)
    targets = torch.randn(32, 5)  # True values
    
    # Feed data through model
    outputs = model(inputs)  # Shape: [32, 5]
    
    # STEP 3: COMPUTE LOSS
    # Measure how far predictions are from targets
    loss = criterion(outputs, targets)  # Returns a scalar
    
    # STEP 4: BACKWARD PASS (Backpropagation)
    # Automatically computes ∂loss/∂parameter for all parameters
    # This is the "A" in autograd—automatic differentiation
    loss.backward()
    
    # At this point, every parameter has a .grad attribute
    # containing ∂loss/∂parameter
    # Example: model[0].weight.grad has shape [5, 10]
    
    # STEP 5: UPDATE PARAMETERS (Gradient Descent)
    # Apply optimizer: param = param - learning_rate * gradient
    # For Adam: more complex, uses momentum and adaptive rates
    optimizer.step()
    
    # Print progress
    if epoch % 2 == 0:
        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')

# Final weights are now learned from data!
</code></pre>

                        <h4>Why optimizer.zero_grad() is Critical</h4>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Create simple layer and parameter
layer = nn.Linear(2, 2)
x = torch.tensor([[1.0, 2.0]])
target = torch.tensor([[0.0, 0.0]])

# First backward pass
output1 = layer(x)
loss1 = (output1 ** 2).sum()
loss1.backward()
print('Gradient after first backward:', layer.weight.grad)

# WITHOUT zero_grad(): Gradients accumulate!
output2 = layer(x)
loss2 = (output2 ** 2).sum()
loss2.backward()
print('Gradient after 2nd backward (accumulated):', layer.weight.grad)
# Notice: grad values are LARGER, not fresh

# WITH zero_grad(): Correct pattern
layer.zero_grad()  # Reset gradients to 0
output3 = layer(x)
loss3 = (output3 ** 2).sum()
loss3.backward()
print('Gradient after zero_grad() + backward:', layer.weight.grad)
# Now gradient is computed fresh for this iteration only

# REMEMBER:
# optimizer.zero_grad()  → Reset gradients to 0
# loss.backward()        → Compute new gradients
# optimizer.step()       → Update parameters using gradients
</code></pre>

                        <h3>Learning Rate Scheduling</h3>
                        <h3>Learning Rate Scheduling: Decay Over Time</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR

# Create model and optimizer
model = nn.Sequential(nn.Linear(10, 5))
optimizer = optim.Adam(model.parameters(), lr=0.1)

# ===== STEPLR: Reduce LR by factor every N epochs =====
# Parameters:
#   - step_size: reduce LR every N epochs
#   - gamma: multiply LR by this factor (e.g., 0.1 = divide by 10)
# Schedule: lr(t) = lr_0 * gamma^floor(t / step_size)
scheduler_step = StepLR(optimizer, step_size=30, gamma=0.1)
# This means: Every 30 epochs, multiply LR by 0.1
# Epoch 0-29: lr = 0.1
# Epoch 30-59: lr = 0.01
# Epoch 60+: lr = 0.001

print('Initial LR:', optimizer.param_groups[0]['lr'])  # 0.1

for epoch in range(5):
    # Training code here...
    pass
    
    # Step scheduler at end of epoch
    # Call AFTER training, updates optimizer's learning rate
    scheduler_step.step()
    print(f'Epoch {epoch+1}, LR: {optimizer.param_groups[0]["lr"]:.6f}')

# ===== REDUCELRONPLATEAU: Reduce when metric stops improving =====
# Monitors validation metric, reduces LR when it plateaus
# Parameters:
#   - mode: 'min' for loss (lower is better), 'max' for accuracy
#   - factor: multiply LR by this (0.5 = halve learning rate)
#   - patience: wait this many epochs of no improvement before reducing
#   - min_lr: don't go below this learning rate
optimizer2 = optim.Adam(model.parameters(), lr=0.1)
scheduler_plateau = ReduceLROnPlateau(
    optimizer2, 
    mode='min',           # Monitor loss (lower is better)
    factor=0.5,           # Halve learning rate when triggered
    patience=5,           # Wait 5 epochs of no improvement
    min_lr=0.00001        # Don't reduce below 1e-5
)

# During training, call with validation loss
for epoch in range(10):
    val_loss = 0.5 - epoch * 0.03  # Simulated decreasing loss
    # step() takes the metric value (validation loss)
    # Automatically reduces LR if no improvement for 'patience' epochs
    scheduler_plateau.step(val_loss)
    print(f'Epoch {epoch+1}, Val Loss: {val_loss:.4f}, LR: {optimizer2.param_groups[0]["lr"]:.6f}')

# ===== COSINEANNEALINGLR: Smooth cosine decay =====
# Gradually decreases LR using cosine function
# Nice smooth schedule, often works better than step decay
# Parameters:
#   - T_max: number of epochs for one cosine cycle
#   - eta_min: minimum learning rate (end value)
# Formula: lr(t) = eta_min + (lr_0 - eta_min) * (1 + cos(πt/T_max)) / 2
optimizer3 = optim.Adam(model.parameters(), lr=0.1)
scheduler_cosine = CosineAnnealingLR(
    optimizer3, 
    T_max=10,           # Complete one cosine cycle in 10 epochs
    eta_min=0.001       # Minimum LR at end
)

for epoch in range(10):
    scheduler_cosine.step()
    print(f'Epoch {epoch+1}, LR: {optimizer3.param_groups[0]["lr"]:.6f}')

# LEARNING RATE SCHEDULING GUIDE:
print('\nWhen to use each schedule:')
print('StepLR: Simple, predictable schedule. Use when you know training length.')
print('ReduceLROnPlateau: Adaptive, responds to actual training progress.')
print('CosineAnnealingLR: Smooth, empirically works well for many tasks.')
</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Optimizer Choice Guide:</strong> Start with <strong>Adam</strong> or <strong>AdamW</strong> (lr=0.001) as default. Use <strong>SGD with momentum</strong> (lr=0.01-0.1, momentum=0.9) for computer vision when you need best final performance and have time to tune. Use <strong>RMSprop</strong> for RNNs. Always enable weight decay for regularization.
                        </div>
                    </div>

                    <!-- Datasets & DataLoaders -->
                    <div id="datasets" class="blog-content mt-5">
                        <h2><i class="fas fa-database me-2 text-teal"></i>Datasets & DataLoaders</h2>
                        
                        <p>PyTorch's <code>Dataset</code> and <code>DataLoader</code> classes provide efficient data loading with automatic batching, shuffling, and parallel loading. This is essential for training on large datasets.</p>

                        <h3>Creating Custom Dataset</h3>
                        <pre><code class="language-python">import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np

# Custom Dataset: must implement __len__ and __getitem__
class CustomDataset(Dataset):
    def __init__(self, num_samples=1000, num_features=10):
        # Initialize data (in practice, load from files here)
        self.data = torch.randn(num_samples, num_features)
        self.labels = torch.randint(0, 2, (num_samples,))  # Binary labels
    
    def __len__(self):
        # Return total number of samples
        return len(self.data)
    
    def __getitem__(self, idx):
        # Return one sample (data, label) at index idx
        return self.data[idx], self.labels[idx]

# Create dataset instance
dataset = CustomDataset(num_samples=1000, num_features=10)
print(f'Dataset size: {len(dataset)}')

# Access individual samples
sample_data, sample_label = dataset[0]
print(f'Sample data shape: {sample_data.shape}')  # torch.Size([10])
print(f'Sample label: {sample_label.item()}')  # 0 or 1</code></pre>

                        <h3>DataLoader: Batching & Shuffling</h3>
                        <pre><code class="language-python">import torch
from torch.utils.data import Dataset, DataLoader

# Custom dataset (same as above)
class CustomDataset(Dataset):
    def __init__(self, num_samples=1000, num_features=10):
        self.data = torch.randn(num_samples, num_features)
        self.labels = torch.randint(0, 2, (num_samples,))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

dataset = CustomDataset(num_samples=1000, num_features=10)

# Create DataLoader
dataloader = DataLoader(
    dataset,
    batch_size=32,        # Load 32 samples per batch
    shuffle=True,         # Shuffle data every epoch
    num_workers=0,        # Parallel data loading (0 = single process)
    pin_memory=True       # Faster GPU transfer (use with CUDA)
)

print(f'Number of batches: {len(dataloader)}')  # 1000 / 32 = 32 batches

# Iterate through batches
for batch_idx, (data, labels) in enumerate(dataloader):
    print(f'Batch {batch_idx}: data shape {data.shape}, labels shape {labels.shape}')
    # data shape: torch.Size([32, 10])
    # labels shape: torch.Size([32])
    
    if batch_idx == 2:  # Show only first 3 batches
        break</code></pre>

                        <h3>Using Built-in Datasets (TorchVision)</h3>
                        <pre><code class="language-python">import torch
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms

# Define data transformations
transform = transforms.Compose([
    transforms.ToTensor(),  # Convert PIL Image to Tensor
    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]
])

# Load MNIST dataset (downloads automatically on first run)
train_dataset = torchvision.datasets.MNIST(
    root='./data',           # Download location
    train=True,              # Training set
    download=True,           # Download if not present
    transform=transform      # Apply transformations
)

test_dataset = torchvision.datasets.MNIST(
    root='./data',
    train=False,             # Test set
    download=True,
    transform=transform
)

print(f'Training samples: {len(train_dataset)}')  # 60,000
print(f'Test samples: {len(test_dataset)}')      # 10,000

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Examine one batch
images, labels = next(iter(train_loader))
print(f'Image batch shape: {images.shape}')  # torch.Size([64, 1, 28, 28])
print(f'Label batch shape: {labels.shape}')  # torch.Size([64])</code></pre>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">DataLoader Best Practices</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>Batch Size:</strong> Start with 32 or 64. Larger batches (256+) need higher learning rates. GPU memory limits max batch size.</li>
                                    <li><strong>Shuffle:</strong> Always shuffle training data. Don't shuffle validation/test data (reproducible evaluation).</li>
                                    <li><strong>num_workers:</strong> Use 2-4 workers for parallel loading on multi-core CPUs. Set to 0 on Windows to avoid issues.</li>
                                    <li><strong>pin_memory:</strong> Set True when using GPU—faster data transfer to CUDA.</li>
                                    <li><strong>drop_last:</strong> Set True to drop incomplete final batch (useful for batch normalization).</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Training Loop -->
                    <div id="training-loop" class="blog-content mt-5">
                        <h2><i class="fas fa-sync-alt me-2 text-teal"></i>The Training Loop</h2>
                        
                        <p>The training loop is where all components come together: model, optimizer, loss function, and data. Understanding this pattern is crucial for successfully training neural networks.</p>

                        <h3>Complete Training Loop Example</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# 1. Define Dataset
class SimpleDataset(Dataset):
    def __init__(self, num_samples=1000):
        self.data = torch.randn(num_samples, 10)
        self.labels = torch.randint(0, 2, (num_samples,))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# 2. Create DataLoader
train_dataset = SimpleDataset(num_samples=1000)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# 3. Define Model
model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 2)  # Binary classification (2 classes)
)

# 4. Define Loss and Optimizer
criterion = nn.CrossEntropyLoss()  # For classification
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 5. Training Loop
num_epochs = 10

for epoch in range(num_epochs):
    model.train()  # Set model to training mode (enables dropout, batch norm)
    running_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (data, labels) in enumerate(train_loader):
        # Zero gradients
        optimizer.zero_grad()
        
        # Forward pass
        outputs = model(data)
        loss = criterion(outputs, labels)
        
        # Backward pass
        loss.backward()
        
        # Update weights
        optimizer.step()
        
        # Track metrics
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    
    # Epoch statistics
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100 * correct / total
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')</code></pre>

                        <h3>Training with Validation</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# Dataset (same as above)
class SimpleDataset(Dataset):
    def __init__(self, num_samples=1000):
        self.data = torch.randn(num_samples, 10)
        self.labels = torch.randint(0, 2, (num_samples,))
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# Create train and validation sets
train_loader = DataLoader(SimpleDataset(800), batch_size=32, shuffle=True)
val_loader = DataLoader(SimpleDataset(200), batch_size=32, shuffle=False)

model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 2))
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop with validation
for epoch in range(10):
    # TRAINING PHASE
    model.train()
    train_loss = 0.0
    
    for data, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    
    train_loss /= len(train_loader)
    
    # VALIDATION PHASE
    model.eval()  # Set to evaluation mode (disables dropout, batch norm training)
    val_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():  # Disable gradient computation (saves memory)
        for data, labels in val_loader:
            outputs = model(data)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    val_loss /= len(val_loader)
    val_acc = 100 * correct / total
    
    print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-exclamation-circle"></i>
                            <strong>Critical Training Steps:</strong> Always call <code>optimizer.zero_grad()</code> before backward pass (gradients accumulate by default). Use <code>model.train()</code> for training and <code>model.eval()</code> + <code>torch.no_grad()</code> for validation/testing. This ensures correct behavior of dropout and batch normalization layers.
                        </div>
                    </div>

                    <!-- Evaluation, Saving, GPU, Transfer Learning, CNNs, Best Practices -->
                    
                    <div id="evaluation" class="blog-content mt-5">
                        <h2><i class="fas fa-chart-line me-2 text-teal"></i>Model Evaluation & Metrics</h2>
                        
                        <p>Proper evaluation requires setting the model to evaluation mode and computing appropriate metrics. Always evaluate on held-out test data to assess generalization.</p>

                        <h3>Evaluation Pattern</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

# Simple dataset and model
class TestDataset(Dataset):
    def __init__(self):
        self.data = torch.randn(100, 10)
        self.labels = torch.randint(0, 3, (100,))
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

test_loader = DataLoader(TestDataset(), batch_size=32)
model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 3))

# Evaluation function
def evaluate_model(model, dataloader):
    model.eval()  # Set to evaluation mode
    correct = 0
    total = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():  # Disable gradient tracking
        for data, labels in dataloader:
            outputs = model(data)
            _, predicted = torch.max(outputs, 1)
            
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    accuracy = 100 * correct / total
    return accuracy, all_preds, all_labels

acc, preds, labels = evaluate_model(model, test_loader)
print(f'Test Accuracy: {acc:.2f}%')</code></pre>
                    </div>

                    <div id="saving" class="blog-content mt-5">
                        <h2><i class="fas fa-save me-2 text-teal"></i>Saving & Loading Models</h2>
                        
                        <p>PyTorch provides two approaches: save the entire model or save only the state dictionary (recommended). The state dict contains all learnable parameters.</p>

                        <h3>Save and Load State Dict (Recommended)</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Define and train a model
model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 5))

# Save only the state dict (parameters)
torch.save(model.state_dict(), 'model_weights.pth')
print('Model state dict saved')

# Load state dict into a NEW model instance
# IMPORTANT: Model architecture must match exactly
model_loaded = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 5))
model_loaded.load_state_dict(torch.load('model_weights.pth'))
model_loaded.eval()
print('Model state dict loaded')

# Verify weights match
print('Weights match:', torch.equal(model.state_dict()['0.weight'], 
                                     model_loaded.state_dict()['0.weight']))</code></pre>

                        <h3>Save Entire Model (Alternative)</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

model = nn.Sequential(nn.Linear(10, 5))

# Save entire model (architecture + weights)
torch.save(model, 'entire_model.pth')

# Load entire model
model_loaded = torch.load('entire_model.pth')
model_loaded.eval()
print('Entire model loaded')</code></pre>

                        <h3>Save Training Checkpoint</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim

model = nn.Sequential(nn.Linear(10, 5))
optimizer = optim.Adam(model.parameters(), lr=0.001)
epoch = 10
loss = 0.5

# Save complete training state
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss
}
torch.save(checkpoint, 'checkpoint.pth')
print('Checkpoint saved')

# Load checkpoint and resume training
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
start_epoch = checkpoint['epoch']
last_loss = checkpoint['loss']
print(f'Resumed from epoch {start_epoch}, loss {last_loss:.4f}')</code></pre>
                    </div>

                    <div id="gpu" class="blog-content mt-5">
                        <h2><i class="fas fa-microchip me-2 text-teal"></i>GPU Acceleration & Device Management</h2>
                        
                        <p>Training on GPU can be 10-100x faster than CPU. PyTorch makes GPU usage simple with the <code>.to(device)</code> method.</p>

                        <h3>Basic GPU Usage</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Check GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

if torch.cuda.is_available():
    print(f'GPU: {torch.cuda.get_device_name(0)}')
    print(f'Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB')

# Move model to GPU
model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 5))
model = model.to(device)
print('Model moved to', device)

# Move data to GPU (must match model device!)
data = torch.randn(32, 10).to(device)
labels = torch.randint(0, 5, (32,)).to(device)

# Forward pass on GPU
outputs = model(data)
print('Output device:', outputs.device)  # cuda:0</code></pre>

                        <h3>Training Loop with GPU</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Setup device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Create data
X_train = torch.randn(1000, 10)
y_train = torch.randint(0, 2, (1000,))
train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32)

# Model, loss, optimizer on GPU
model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 2)).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(5):
    for data, labels in train_loader:
        # Move batch to GPU
        data, labels = data.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
    
    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-bolt"></i>
                            <strong>GPU Best Practices:</strong> Always move both model AND data to the same device. Use <code>torch.cuda.empty_cache()</code> to free unused GPU memory. Monitor memory with <code>torch.cuda.memory_allocated()</code>. Use mixed precision training (next section) to reduce memory usage and speed up training.
                        </div>
                    </div>

                    <div id="mixed-precision" class="blog-content mt-5">
                        <h2><i class="fas fa-tachometer-alt me-2 text-teal"></i>Mixed Precision Training</h2>
                        
                        <p>Mixed precision uses FP16 (16-bit) instead of FP32 (32-bit) for faster training and lower memory usage. PyTorch's automatic mixed precision (AMP) handles this automatically.</p>

                        <h3>Using torch.cuda.amp</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
from torch.utils.data import DataLoader, TensorDataset

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Model and data
model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 2)).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

X_train = torch.randn(1000, 10)
y_train = torch.randint(0, 2, (1000,))
train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32)

# Create gradient scaler for mixed precision
scaler = GradScaler()

# Training loop with AMP
for epoch in range(5):
    for data, labels in train_loader:
        data, labels = data.to(device), labels.to(device)
        
        optimizer.zero_grad()
        
        # Wrap forward pass in autocast
        with autocast():
            outputs = model(data)
            loss = criterion(outputs, labels)
        
        # Scale loss and backward pass
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
    
    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')

print('Mixed precision training complete - typically 2-3x faster!')</code></pre>
                    </div>

                    <div id="transfer-learning" class="blog-content mt-5">
                        <h2><i class="fas fa-exchange-alt me-2 text-teal"></i>Transfer Learning with Pretrained Models</h2>
                        
                        <p>Transfer learning uses models pretrained on large datasets (ImageNet) as feature extractors. This dramatically reduces training time and data requirements.</p>

                        <h3>Using Pretrained ResNet</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torchvision.models as models

# Load pretrained ResNet-18 (trained on ImageNet)
model = models.resnet18(pretrained=True)
print('Pretrained ResNet-18 loaded')

# Freeze all layers (don't update pretrained weights)
for param in model.parameters():
    param.requires_grad = False

# Replace final layer for new task (e.g., 10 classes instead of 1000)
num_features = model.fc.in_features  # 512 for ResNet-18
model.fc = nn.Linear(num_features, 10)  # Only this layer will be trained

print('Modified for 10-class classification')
print(f'Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')

# Now train only the final layer on your dataset</code></pre>

                        <h3>Fine-Tuning Pretrained Model</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models

# Load pretrained model
model = models.resnet18(pretrained=True)

# Replace final layer
model.fc = nn.Linear(model.fc.in_features, 10)

# Two-stage training:
# Stage 1: Train only final layer (frozen backbone)
for param in model.parameters():
    param.requires_grad = False
model.fc.requires_grad_(True)  # Unfreeze only final layer

optimizer = optim.Adam(model.fc.parameters(), lr=0.001)
# Train for a few epochs...

# Stage 2: Fine-tune entire model with lower learning rate
for param in model.parameters():
    param.requires_grad = True

optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Lower LR!
# Continue training...</code></pre>
                    </div>

                    <div id="cnn" class="blog-content mt-5">
                        <h2><i class="fas fa-images me-2 text-teal"></i>Convolutional Neural Networks (CNNs)</h2>
                        
                        <p>CNNs are the architecture of choice for computer vision. They use convolutional layers to automatically learn spatial hierarchies of features.</p>

                        <h3>Simple CNN for Image Classification</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # 1 input channel (grayscale)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)  # 2x2 max pooling
        
        # Fully connected layers
        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # After 2 pooling: 28->14->7
        self.fc2 = nn.Linear(128, 10)  # 10 classes (MNIST digits)
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        # Conv block 1: Conv -> ReLU -> Pool
        x = self.pool(F.relu(self.conv1(x)))  # (28, 28) -> (14, 14)
        
        # Conv block 2: Conv -> ReLU -> Pool
        x = self.pool(F.relu(self.conv2(x)))  # (14, 14) -> (7, 7)
        
        # Flatten for fully connected layers
        x = x.view(-1, 64 * 7 * 7)  # Batch x Features
        
        # Fully connected layers
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Create model
model = SimpleCNN()
print(model)

# Test forward pass
x = torch.randn(4, 1, 28, 28)  # Batch of 4 grayscale 28x28 images
output = model(x)
print(f'Output shape: {output.shape}')  # torch.Size([4, 10])</code></pre>
                    </div>

                    <div id="rnn" class="blog-content mt-5">
                        <h2><i class="fas fa-stream me-2 text-teal"></i>Recurrent Neural Networks (RNNs & LSTMs)</h2>
                        
                        <p>RNNs process sequential data by maintaining hidden states across time steps. LSTMs (Long Short-Term Memory) solve the vanishing gradient problem in standard RNNs.</p>

                        <h3>Simple RNN for Sequence Classification</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

class RNNClassifier(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNNClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # RNN layer
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        
        # Fully connected output layer
        self.fc = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        # Initialize hidden state
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # Forward propagate RNN
        out, _ = self.rnn(x, h0)  # out: (batch, seq_len, hidden_size)
        
        # Get output from last time step
        out = self.fc(out[:, -1, :])  # (batch, num_classes)
        return out

# Create model
model = RNNClassifier(input_size=10, hidden_size=128, num_layers=2, num_classes=5)
print(model)

# Test forward pass
x = torch.randn(32, 20, 10)  # (batch, seq_len, input_size)
output = model(x)
print(f'Output shape: {output.shape}')  # torch.Size([32, 5])</code></pre>

                        <h3>LSTM for Text Classification</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):
        super(LSTMClassifier, self).__init__()
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # LSTM layer (bidirectional for better context)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)
        
        # Fully connected layer (2x hidden_dim because bidirectional)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        # x: (batch, seq_len) - integer token IDs
        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)
        
        # LSTM forward pass
        lstm_out, (hidden, cell) = self.lstm(embedded)
        
        # Concatenate final forward and backward hidden states
        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        
        # Apply dropout and output layer
        out = self.dropout(hidden)
        out = self.fc(out)
        return out

# Create model
model = LSTMClassifier(vocab_size=10000, embed_dim=100, hidden_dim=256, num_classes=2)
print(model)

# Test forward pass
x = torch.randint(0, 10000, (32, 50))  # (batch, seq_len) - token IDs
output = model(x)
print(f'Output shape: {output.shape}')  # torch.Size([32, 2])</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-lightbulb"></i>
                            <strong>LSTM vs RNN:</strong> Use LSTMs for longer sequences (>20 steps) where long-term dependencies matter. For very long sequences (>500 tokens), consider Transformers instead. Bidirectional LSTMs see both past and future context but can't be used for real-time prediction.
                        </div>
                    </div>

                    <div id="embeddings" class="blog-content mt-5">
                        <h2><i class="fas fa-language me-2 text-teal"></i>Embeddings for NLP</h2>
                        
                        <p>Embeddings convert discrete tokens (words, characters) into continuous vector representations that capture semantic meaning.</p>

                        <h3>Creating Word Embeddings</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Create embedding layer
vocab_size = 10000  # Size of vocabulary
embedding_dim = 300  # Dimension of embedding vectors

embedding = nn.Embedding(vocab_size, embedding_dim)
print(f'Embedding matrix shape: {embedding.weight.shape}')  # torch.Size([10000, 300])

# Convert word IDs to embeddings
word_ids = torch.LongTensor([5, 42, 123, 7])  # Word IDs from vocabulary
word_vectors = embedding(word_ids)
print(f'Word vectors shape: {word_vectors.shape}')  # torch.Size([4, 300])

# Batch of sentences
sentences = torch.LongTensor([[5, 42, 123, 7, 0],    # Sentence 1 (0 = padding)
                              [15, 88, 3, 0, 0]])      # Sentence 2
sentence_vectors = embedding(sentences)
print(f'Sentence embeddings shape: {sentence_vectors.shape}')  # torch.Size([2, 5, 300])</code></pre>

                        <h3>Using Pretrained Embeddings (GloVe)</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Load pretrained GloVe embeddings (example - you'd load from file)
vocab_size = 10000
embedding_dim = 300

# Create embedding layer
embedding = nn.Embedding(vocab_size, embedding_dim)

# Load pretrained weights (replace with actual GloVe weights)
# pretrained_weights = torch.load('glove.6B.300d.pt')
# embedding.weight.data.copy_(pretrained_weights)

# Freeze embeddings (don't update during training)
embedding.weight.requires_grad = False
print('Pretrained embeddings loaded and frozen')

# For fine-tuning, keep trainable:
# embedding.weight.requires_grad = True</code></pre>

                        <h3>Embedding with Padding</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Create embedding with padding_idx
embedding = nn.Embedding(vocab_size=10000, embedding_dim=300, padding_idx=0)

# Padding token (ID=0) will always have zero vector
print(f'Padding embedding: {embedding(torch.LongTensor([0]))}')  # All zeros

# Regular tokens have learned embeddings
print(f'Word embedding shape: {embedding(torch.LongTensor([42])).shape}')  # (1, 300)</code></pre>
                    </div>

                    <div id="advanced" class="blog-content mt-5">
                        <h2><i class="fas fa-cogs me-2 text-teal"></i>Advanced Training Techniques</h2>
                        
                        <h3>Gradient Clipping</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim

model = nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 5))
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Training loop with gradient clipping
data = torch.randn(32, 10)
labels = torch.randint(0, 5, (32,))

optimizer.zero_grad()
outputs = model(data)
loss = criterion(outputs, labels)
loss.backward()

# Clip gradients to prevent exploding gradients
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

optimizer.step()
print('Gradients clipped to max norm 1.0')</code></pre>

                        <h3>Learning Rate Scheduling</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau

model = nn.Linear(10, 5)
optimizer = optim.Adam(model.parameters(), lr=0.01)

# StepLR: Decay LR by gamma every step_size epochs
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

# Training loop
for epoch in range(30):
    # ... training code ...
    
    # Update learning rate
    scheduler.step()
    print(f'Epoch {epoch}, LR: {optimizer.param_groups[0]["lr"]:.6f}')

# ReduceLROnPlateau: Reduce LR when metric plateaus
scheduler_plateau = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

for epoch in range(30):
    # ... training code ...
    val_loss = 0.5  # Example validation loss
    
    # Reduce LR if validation loss doesn't improve
    scheduler_plateau.step(val_loss)</code></pre>

                        <h3>Early Stopping</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim

class EarlyStopping:
    def __init__(self, patience=7, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# Usage
model = nn.Linear(10, 5)
optimizer = optim.Adam(model.parameters())
early_stopping = EarlyStopping(patience=10)

for epoch in range(100):
    # ... training code ...
    val_loss = 0.5  # Example validation loss
    
    early_stopping(val_loss)
    if early_stopping.early_stop:
        print(f'Early stopping at epoch {epoch}')
        break</code></pre>
                    </div>

                    <div id="custom-layers" class="blog-content mt-5">
                        <h2><i class="fas fa-layer-group me-2 text-teal"></i>Custom Layers & Loss Functions</h2>
                        
                        <h3>Creating Custom Layers</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

class CustomLinear(nn.Module):
    def __init__(self, in_features, out_features, use_bias=True):
        super(CustomLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # Initialize weights and bias
        self.weight = nn.Parameter(torch.randn(out_features, in_features))
        if use_bias:
            self.bias = nn.Parameter(torch.randn(out_features))
        else:
            self.register_parameter('bias', None)
    
    def forward(self, x):
        # Custom linear transformation: y = xW^T + b
        output = torch.matmul(x, self.weight.t())
        if self.bias is not None:
            output += self.bias
        return output

# Use custom layer
layer = CustomLinear(10, 5)
x = torch.randn(32, 10)
output = layer(x)
print(f'Output shape: {output.shape}')  # torch.Size([32, 5])</code></pre>

                        <h3>Custom Activation Function</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

class Swish(nn.Module):
    """Swish activation: x * sigmoid(x)"""
    def forward(self, x):
        return x * torch.sigmoid(x)

# Use custom activation
model = nn.Sequential(
    nn.Linear(10, 20),
    Swish(),  # Custom activation
    nn.Linear(20, 5)
)

x = torch.randn(32, 10)
output = model(x)
print(f'Output shape: {output.shape}')  # torch.Size([32, 5])</code></pre>

                        <h3>Custom Loss Function</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

class FocalLoss(nn.Module):
    """Focal Loss for handling class imbalance"""
    def __init__(self, alpha=1, gamma=2):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.ce_loss = nn.CrossEntropyLoss(reduction='none')
    
    def forward(self, inputs, targets):
        ce_loss = self.ce_loss(inputs, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()

# Use custom loss
model = nn.Linear(10, 5)
criterion = FocalLoss(alpha=1, gamma=2)

outputs = model(torch.randn(32, 10))
targets = torch.randint(0, 5, (32,))
loss = criterion(outputs, targets)
print(f'Focal loss: {loss.item():.4f}')</code></pre>
                    </div>

                    <div id="interpretability" class="blog-content mt-5">
                        <h2><i class="fas fa-eye me-2 text-teal"></i>Model Interpretability (Grad-CAM)</h2>
                        
                        <p>Grad-CAM (Gradient-weighted Class Activation Mapping) visualizes which parts of an image a CNN focuses on for predictions.</p>

                        <h3>Implementing Grad-CAM</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        
        # Register hooks
        target_layer.register_forward_hook(self.save_activation)
        target_layer.register_backward_hook(self.save_gradient)
    
    def save_activation(self, module, input, output):
        self.activations = output
    
    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0]
    
    def generate_cam(self, input_image, target_class):
        # Forward pass
        output = self.model(input_image)
        
        # Backward pass for target class
        self.model.zero_grad()
        class_loss = output[0, target_class]
        class_loss.backward()
        
        # Get gradients and activations
        gradients = self.gradients.detach()
        activations = self.activations.detach()
        
        # Global average pooling of gradients
        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)
        
        # Weighted combination of activation maps
        cam = torch.sum(weights * activations, dim=1, keepdim=True)
        cam = F.relu(cam)  # Only positive influence
        
        # Normalize
        cam = cam - cam.min()
        cam = cam / cam.max()
        
        return cam

# Example usage with ResNet
import torchvision.models as models

model = models.resnet18(pretrained=True)
model.eval()

# Get last convolutional layer
target_layer = model.layer4[-1].conv2

# Create Grad-CAM
grad_cam = GradCAM(model, target_layer)

# Generate CAM for an image
image = torch.randn(1, 3, 224, 224)  # Example image
cam = grad_cam.generate_cam(image, target_class=243)  # 243 = "bull mastiff"
print(f'CAM shape: {cam.shape}')  # torch.Size([1, 1, 7, 7])</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-info-circle"></i>
                            <strong>Interpretability Tools:</strong> For production use, consider libraries like <code>captum</code> (PyTorch's official interpretability library) which provides Grad-CAM, Integrated Gradients, and many other attribution methods out of the box.
                        </div>
                    </div>

                    <!-- Transformer Architectures -->
                    <div id="transformers" class="blog-content mt-5">
                        <h2><i class="fas fa-brain me-2 text-teal"></i>Transformer Architectures (nn.Transformer)</h2>
                        
                        <p>Transformers have revolutionized deep learning, particularly in NLP and increasingly in computer vision. PyTorch's <code>nn.Transformer</code> module provides an efficient, ready-to-use implementation of the complete Transformer architecture with multi-head self-attention, feed-forward networks, and layer normalization.</p>

                        <div class="highlight-box">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Transformer Power:</strong> Transformers use self-attention to process sequences in parallel (unlike RNNs which process sequentially), enabling better training efficiency and longer-range dependency capture. Each token can directly attend to every other token in the sequence.
                        </div>

                        <p><strong>Key Components:</strong> The Transformer consists of an encoder and decoder stack, each with multi-head attention, feed-forward networks, and residual connections.</p>

                        <pre><code class="language-python">import torch
import torch.nn as nn

# Create a Transformer model
batch_size = 32
seq_length = 10
d_model = 512
num_heads = 8
num_layers = 6

transformer = nn.Transformer(
    d_model=d_model,
    nhead=num_heads,
    num_encoder_layers=num_layers,
    num_decoder_layers=num_layers,
    dim_feedforward=2048,
    dropout=0.1,
    batch_first=True
)

# Input: (batch_size, seq_length, d_model)
src = torch.randn(batch_size, seq_length, d_model)
tgt = torch.randn(batch_size, seq_length, d_model)

# Forward pass
output = transformer(src, tgt)
print(f"Output shape: {output.shape}")  # [32, 10, 512]</code></pre>

                        <p><strong>Creating a Custom Transformer-based Model:</strong></p>

                        <pre><code class="language-python">import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_seq_length=100, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        
        # Create position encodings
        pe = torch.zeros(max_seq_length, d_model)
        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                             (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
    
    def forward(self, x):
        # x shape: (batch, seq_length, d_model)
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class TransformerSequenceModel(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_heads=8, 
                 num_layers=6, dim_feedforward=2048, max_seq_length=100):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_seq_length)
        self.transformer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=dim_feedforward,
            dropout=0.1,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(self.transformer, num_layers=num_layers)
        self.output_layer = nn.Linear(d_model, vocab_size)
    
    def forward(self, x):
        # x shape: (batch, seq_length)
        embedded = self.embedding(x)  # (batch, seq_length, d_model)
        pos_encoded = self.pos_encoding(embedded)  # Add positional info
        encoded = self.encoder(pos_encoded)  # (batch, seq_length, d_model)
        output = self.output_layer(encoded)  # (batch, seq_length, vocab_size)
        return output

# Create and use the model
vocab_size = 10000
model = TransformerSequenceModel(vocab_size, d_model=512, num_heads=8, num_layers=6)

# Forward pass
batch_size = 16
seq_length = 20
input_ids = torch.randint(0, vocab_size, (batch_size, seq_length))
output = model(input_ids)
print(f"Output shape: {output.shape}")  # [16, 20, 10000]</code></pre>

                        <p><strong>Masking for Autoregressive Generation:</strong> For sequence-to-sequence tasks, we need to prevent the model from attending to future tokens during training.</p>

                        <pre><code class="language-python">import torch
import torch.nn as nn

def create_causal_mask(seq_length, device):
    """Create a mask preventing attention to future tokens"""
    mask = torch.triu(torch.ones(seq_length, seq_length, device=device) * float('-inf'), diagonal=1)
    return mask

# Use mask in Transformer
transformer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)
src = torch.randn(32, 20, 512)  # (batch, seq_length, d_model)

# Create causal mask
mask = create_causal_mask(20, src.device)

# Forward with mask
output = transformer(src, src_mask=mask)
print(f"Output shape: {output.shape}")  # [32, 20, 512]</code></pre>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">Transformer Components</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>Multi-Head Attention:</strong> Multiple representation subspaces allow the model to attend to information from different representation subspaces at different positions</li>
                                    <li><strong>Feed-Forward Networks:</strong> Two linear layers with ReLU activation applied to each position separately and identically</li>
                                    <li><strong>Positional Encoding:</strong> Sine and cosine functions at different frequencies encode token positions since attention is position-agnostic</li>
                                    <li><strong>Layer Normalization:</strong> Applied before (pre-norm) or after (post-norm) sublayers to stabilize training</li>
                                    <li><strong>Residual Connections:</strong> Skip connections around each sublayer enable training of very deep networks</li>
                                </ul>
                            </div>
                        </div>

                        <p><strong>Transformer Decoder for Sequence Generation:</strong></p>

                        <pre><code class="language-python">import torch
import torch.nn as nn

class TransformerDecoder(nn.Module):
    def __init__(self, vocab_size, d_model=512, num_heads=8, 
                 num_layers=6, dim_feedforward=2048, max_seq_length=100):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_seq_length)
        
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=dim_feedforward,
            dropout=0.1,
            batch_first=True
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
        self.output_layer = nn.Linear(d_model, vocab_size)
    
    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):
        # tgt shape: (batch, tgt_seq_length, d_model)
        # memory shape: (batch, src_seq_length, d_model)
        
        tgt_embedded = self.embedding(tgt)
        tgt_encoded = self.pos_encoding(tgt_embedded)
        
        decoded = self.decoder(
            tgt_encoded, 
            memory,
            tgt_mask=tgt_mask,
            memory_mask=memory_mask
        )
        output = self.output_layer(decoded)
        return output

# Create encoder-decoder architecture
encoder = nn.TransformerEncoder(
    nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True),
    num_layers=6
)
decoder = TransformerDecoder(vocab_size=10000, d_model=512, num_heads=8, num_layers=6)

# Encode source sequence
src = torch.randn(16, 30, 512)  # (batch, src_len, d_model)
memory = encoder(src)

# Decode with target and encoded source
tgt = torch.randint(0, 10000, (16, 20))  # (batch, tgt_len)
output = decoder(tgt, memory)
print(f"Decoder output shape: {output.shape}")  # [16, 20, 10000]</code></pre>
                    </div>

                    <!-- Vision Transformers -->
                    <div id="vision-transformers" class="blog-content mt-5">
                        <h2><i class="fas fa-image me-2 text-teal"></i>Vision Transformers (ViT)</h2>
                        
                        <p>Vision Transformers apply the Transformer architecture directly to image patches, treating them like sequence tokens. This approach has achieved state-of-the-art results on image classification while being more efficient to train on large datasets compared to CNNs.</p>

                        <div class="highlight-box">
                            <i class="fas fa-lightbulb"></i>
                            <strong>ViT Innovation:</strong> Instead of using convolutional filters, ViT divides an image into non-overlapping patches, embeds them linearly, and processes them with standard Transformers. This enables the model to learn global dependencies from the start, rather than building them up through multiple convolutional layers.
                        </div>

                        <p><strong>Basic Vision Transformer Implementation:</strong></p>

                        <pre><code class="language-python">import torch
import torch.nn as nn
import math

class VisionTransformer(nn.Module):
    def __init__(self, image_size=224, patch_size=16, num_classes=1000, 
                 d_model=768, num_heads=12, num_layers=12, dim_feedforward=3072):
        super().__init__()
        
        # Patch embedding
        num_patches = (image_size // patch_size) ** 2
        patch_dim = 3 * patch_size * patch_size  # 3 channels * patch_size * patch_size
        
        self.patch_embedding = nn.Linear(patch_dim, d_model)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))
        
        # Positional embeddings
        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, d_model))
        
        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=num_heads,
            dim_feedforward=dim_feedforward,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # Classification head
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Linear(d_model, num_classes)
        
        self.patch_size = patch_size
    
    def forward(self, x):
        # x shape: (batch, 3, 224, 224)
        batch_size = x.shape[0]
        
        # Split into patches and embed
        # Reshape to (batch, num_patches, patch_dim)
        x = x.reshape(batch_size, 3, -1, self.patch_size, self.patch_size)
        x = x.permute(0, 2, 3, 4, 1).reshape(batch_size, -1, 3 * self.patch_size * self.patch_size)
        
        x = self.patch_embedding(x)  # (batch, num_patches, d_model)
        
        # Add class token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # (batch, num_patches+1, d_model)
        
        # Add positional embeddings
        x = x + self.pos_embedding
        
        # Transformer encoder
        x = self.transformer(x)  # (batch, num_patches+1, d_model)
        
        # Classification from [CLS] token
        x = self.norm(x[:, 0])  # Take [CLS] token
        x = self.head(x)  # (batch, num_classes)
        
        return x

# Create and use ViT
vit = VisionTransformer(image_size=224, patch_size=16, num_classes=1000, d_model=768, num_heads=12, num_layers=12)

# Forward pass
images = torch.randn(8, 3, 224, 224)  # (batch, channels, height, width)
logits = vit(images)
print(f"Output shape: {logits.shape}")  # [8, 1000]
print(f"Total parameters: {sum(p.numel() for p in vit.parameters()) / 1e6:.1f}M")</code></pre>

                        <p><strong>Using Pretrained Vision Transformers from torchvision:</strong></p>

                        <pre><code class="language-python">import torch
import torchvision.models as models
from torchvision.transforms import Compose, Resize, Normalize, ToTensor

# Load pretrained ViT-Base model
vit = models.vit_b_16(pretrained=True, progress=True)

# Freeze feature extraction layers
for param in vit.features.parameters():
    param.requires_grad = False

# Replace classification head for custom task
num_classes = 10
vit.heads.head = torch.nn.Linear(768, num_classes)

# Prepare input
transform = Compose([
    Resize((224, 224)),
    ToTensor(),
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Forward pass
image = torch.randn(1, 3, 224, 224)
output = vit(image)
print(f"Output shape: {output.shape}")  # [1, 10]</code></pre>

                        <p><strong>Patch-level Feature Extraction:</strong> Vision Transformers learn rich patch-level representations useful for detection and segmentation tasks.</p>

                        <pre><code class="language-python">import torch
import torch.nn as nn

class ViTPatchFeatures(nn.Module):
    def __init__(self, vit_model):
        super().__init__()
        self.vit = vit_model
    
    def forward(self, x):
        # Remove classification head
        batch_size = x.shape[0]
        
        # Embed patches
        x = self.vit._process_input(x)
        n, _, c = x.shape
        
        # Expand the class token to the full batch
        batch_class_token = self.vit.class_token.expand(batch_size, -1, -1)
        x = torch.cat([batch_class_token, x], dim=1)
        
        # Apply transformer
        x = self.vit.encoder(x)
        
        # Return patch features (excluding class token)
        return x[:, 1:, :]  # (batch, num_patches, d_model)

# Extract patch features for custom downstream tasks
vit = models.vit_b_16(pretrained=True)
patch_extractor = ViTPatchFeatures(vit)

# Get patch-level features
images = torch.randn(4, 3, 224, 224)
patch_features = patch_extractor(images)
print(f"Patch features shape: {patch_features.shape}")  # [4, 196, 768]

# Use for object detection or segmentation
# Each patch can be processed independently with a detection head
detection_head = nn.Sequential(
    nn.Linear(768, 256),
    nn.ReLU(),
    nn.Linear(256, 4)  # x, y, width, height
)

detections = detection_head(patch_features)
print(f"Detections shape: {detections.shape}")  # [4, 196, 4]</code></pre>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">ViT Advantages</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>Global Receptive Field:</strong> Each patch can attend to all other patches from the first layer, unlike CNNs which need multiple layers</li>
                                    <li><strong>Parallelizable:</strong> No sequential convolutions; all patches processed in parallel</li>
                                    <li><strong>Scalable:</strong> Scales better to very large models and datasets compared to CNNs</li>
                                    <li><strong>Transfer Learning:</strong> Pretrained on ImageNet-21k, achieves excellent results with minimal fine-tuning</li>
                                    <li><strong>Interpretability:</strong> Attention weights show which patches the model focuses on for predictions</li>
                                </ul>
                            </div>
                        </div>

                        <p><strong>Fine-tuning Vision Transformers for Custom Tasks:</strong></p>

                        <pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Setup
vit = models.vit_b_16(pretrained=True)

# Replace head for binary classification
vit.heads.head = nn.Linear(768, 2)

# Freeze encoder, train only head
for param in vit.features.parameters():
    param.requires_grad = False
for param in vit.encoder.parameters():
    param.requires_grad = False

# Training setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
vit = vit.to(device)

optimizer = optim.AdamW(vit.heads.head.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

# Create dummy data
X = torch.randn(100, 3, 224, 224)
y = torch.randint(0, 2, (100,))
dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=8)

# Training loop
for epoch in range(3):
    total_loss = 0
    for images, labels in dataloader:
        images, labels = images.to(device), labels.to(device)
        
        # Forward
        outputs = vit(images)
        loss = criterion(outputs, labels)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    print(f"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}")</code></pre>
                    </div>

                    <!-- Advanced Attention Mechanisms -->
                    <div id="attention-mechanisms" class="blog-content mt-5">
                        <h2><i class="fas fa-network-wired me-2 text-teal"></i>Advanced Attention Mechanisms</h2>
                        
                        <p>PyTorch's <code>nn.MultiheadAttention</code> module is highly flexible and can be customized for various attention patterns. Understanding how to use and modify attention mechanisms is crucial for building state-of-the-art models.</p>

                        <div class="highlight-box">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Attention is All You Need:</strong> The attention mechanism computes a weighted sum of values based on the similarity between queries and keys. Multi-head attention applies this operation multiple times in parallel, capturing different types of relationships.
                        </div>

                        <p><strong>Standard Multi-Head Attention:</strong></p>

                        <pre><code class="language-python">import torch
import torch.nn as nn

# Create multi-head attention
d_model = 512
num_heads = 8

mha = nn.MultiheadAttention(
    embed_dim=d_model,
    num_heads=num_heads,
    dropout=0.1,
    batch_first=True  # Expect (batch, seq_len, d_model) instead of (seq_len, batch, d_model)
)

# Prepare inputs
batch_size = 32
seq_length = 20

query = torch.randn(batch_size, seq_length, d_model)
key = torch.randn(batch_size, seq_length, d_model)
value = torch.randn(batch_size, seq_length, d_model)

# Forward pass
attn_output, attn_weights = mha(query, key, value)

print(f"Attention output shape: {attn_output.shape}")  # [32, 20, 512]
print(f"Attention weights shape: {attn_weights.shape}")  # [32, 20, 20]</code></pre>

                        <p><strong>Cross-Attention (Query from one sequence, Key/Value from another):</strong></p>

                        <pre><code class="language-python">import torch
import torch.nn as nn

# Setup
mha = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)

# Encoder output (context for cross-attention)
encoder_output = torch.randn(16, 30, 512)  # (batch, src_len, d_model)

# Decoder query
decoder_query = torch.randn(16, 20, 512)  # (batch, tgt_len, d_model)

# Cross-attention: Query from decoder, Key/Value from encoder
cross_attn_output, cross_attn_weights = mha(
    query=decoder_query,
    key=encoder_output,
    value=encoder_output
)

print(f"Cross-attention output shape: {cross_attn_output.shape}")  # [16, 20, 512]
print(f"Cross-attention weights shape: {cross_attn_weights.shape}")  # [16, 20, 30]</code></pre>

                        <p><strong>Attention with Masking (Causal and Padding Masks):</strong></p>

                        <pre><code class="language-python">import torch
import torch.nn as nn

def create_causal_mask(seq_length, device):
    """Prevent attending to future positions"""
    mask = torch.triu(torch.ones(seq_length, seq_length, device=device) * float('-inf'), diagonal=1)
    return mask

def create_padding_mask(seq_lengths, max_length, device):
    """Mask padded positions"""
    mask = torch.arange(max_length, device=device).unsqueeze(0) < seq_lengths.unsqueeze(1)
    return mask

# Setup
mha = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)

# Data
batch_size = 16
seq_length = 20

query = torch.randn(batch_size, seq_length, 512)
key = torch.randn(batch_size, seq_length, 512)
value = torch.randn(batch_size, seq_length, 512)

# Create causal mask (for autoregressive generation)
causal_mask = create_causal_mask(seq_length, query.device)

# Forward with causal mask
attn_output, attn_weights = mha(query, key, value, attn_mask=causal_mask)

print(f"Output shape: {attn_output.shape}")  # [16, 20, 512]

# For padding mask, typical usage with variable length sequences
seq_lengths = torch.tensor([20, 18, 15, 20, 19, 17, 20, 16, 19, 20, 18, 17, 20, 19, 15, 20])
padding_mask = create_padding_mask(seq_lengths, seq_length, query.device)

# Padding mask needs to be inverted (False for valid, True for padding)
key_padding_mask = ~padding_mask

attn_output, _ = mha(query, key, value, key_padding_mask=key_padding_mask)
print(f"Output with padding mask: {attn_output.shape}")  # [16, 20, 512]</code></pre>

                        <p><strong>Custom Attention Layer with Scaled Dot-Product:</strong></p>

                        <pre><code class="language-python">import torch
import torch.nn as nn
import math

class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super().__init__()
        self.d_k = d_k
    
    def forward(self, query, key, value, mask=None):
        # Scaled dot product
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # Apply mask if provided
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        # Apply softmax
        attention_weights = torch.softmax(scores, dim=-1)
        
        # Apply dropout for regularization
        attention_weights = torch.nn.functional.dropout(attention_weights, p=0.1, training=self.training)
        
        # Weighted sum of values
        output = torch.matmul(attention_weights, value)
        
        return output, attention_weights

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.linear_q = nn.Linear(d_model, d_model)
        self.linear_k = nn.Linear(d_model, d_model)
        self.linear_v = nn.Linear(d_model, d_model)
        self.linear_out = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(self.d_k)
    
    def forward(self, query, key, value, mask=None):
        batch_size = query.shape[0]
        
        # Linear transformations and reshape for multiple heads
        Q = self.linear_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.linear_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.linear_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # Apply attention
        attn_output, attn_weights = self.attention(Q, K, V, mask)
        
        # Concatenate heads
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        # Final linear transformation
        output = self.linear_out(attn_output)
        
        return output, attn_weights

# Use custom attention
custom_mha = MultiHeadAttention(d_model=512, num_heads=8)

query = torch.randn(32, 20, 512)
key = torch.randn(32, 20, 512)
value = torch.randn(32, 20, 512)

output, weights = custom_mha(query, key, value)
print(f"Custom MHA output shape: {output.shape}")  # [32, 20, 512]</code></pre>

                        <p><strong>Sparse Attention (Linear Attention):</strong> For long sequences, standard attention becomes prohibitively expensive (O(n²) complexity). Sparse attention patterns reduce this cost.</p>

                        <pre><code class="language-python">import torch
import torch.nn as nn

class SparseAttention(nn.Module):
    def __init__(self, d_model, num_heads, window_size=64):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.window_size = window_size
        self.d_k = d_model // num_heads
        
        self.linear_q = nn.Linear(d_model, d_model)
        self.linear_k = nn.Linear(d_model, d_model)
        self.linear_v = nn.Linear(d_model, d_model)
        self.linear_out = nn.Linear(d_model, d_model)
    
    def forward(self, query, key, value):
        batch_size = query.shape[0]
        seq_length = query.shape[1]
        
        Q = self.linear_q(query).view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)
        K = self.linear_k(key).view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)
        V = self.linear_v(value).view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)
        
        # Local attention window
        window_size = self.window_size
        scores = torch.zeros(batch_size, self.num_heads, seq_length, seq_length, device=query.device)
        
        for i in range(seq_length):
            start = max(0, i - window_size // 2)
            end = min(seq_length, i + window_size // 2 + 1)
            
            # Attention only within window
            local_scores = torch.matmul(Q[:, :, i:i+1, :], K[:, :, start:end, :].transpose(-2, -1)) / (self.d_k ** 0.5)
            scores[:, :, i, start:end] = torch.softmax(local_scores.squeeze(2), dim=-1)
        
        # Apply sparse attention
        attn_output = torch.matmul(scores, V)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)
        output = self.linear_out(attn_output)
        
        return output

# Use sparse attention
sparse_attn = SparseAttention(d_model=512, num_heads=8, window_size=64)

# Long sequence
long_query = torch.randn(4, 2048, 512)  # 2048 tokens
output = sparse_attn(long_query, long_query, long_query)
print(f"Sparse attention output shape: {output.shape}")  # [4, 2048, 512]
print(f"Computational complexity: O(n * window_size) instead of O(n²)")</code></pre>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">Attention Variants</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>Self-Attention:</strong> Query, Key, Value from same source; captures internal dependencies</li>
                                    <li><strong>Cross-Attention:</strong> Query from different source than Key/Value; for encoder-decoder fusion</li>
                                    <li><strong>Causal Attention:</strong> Masked to prevent attending to future tokens; for autoregressive generation</li>
                                    <li><strong>Sparse Attention:</strong> Local window attention; reduces O(n²) to O(n*w) for long sequences</li>
                                    <li><strong>Linear Attention:</strong> Kernel-based approximation; O(n) complexity but lower quality</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <div id="deployment" class="blog-content mt-5">
                        <h2><i class="fas fa-rocket me-2 text-teal"></i>Deployment with TorchScript</h2>
                        
                        <p>TorchScript converts PyTorch models into an intermediate representation that can run independently of Python, enabling production deployment in C++.</p>

                        <h3>Tracing a Model</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.linear = nn.Linear(10, 5)
    
    def forward(self, x):
        return torch.relu(self.linear(x))

model = SimpleModel()
model.eval()

# Create example input
example_input = torch.randn(1, 10)

# Trace the model
traced_model = torch.jit.trace(model, example_input)

# Save traced model
traced_model.save('model_traced.pt')
print('Model traced and saved')

# Load and use traced model
loaded_model = torch.jit.load('model_traced.pt')
output = loaded_model(example_input)
print(f'Output: {output}')</code></pre>

                        <h3>Scripting a Model (for Control Flow)</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn

class ModelWithControlFlow(nn.Module):
    def __init__(self):
        super(ModelWithControlFlow, self).__init__()
        self.linear = nn.Linear(10, 5)
    
    def forward(self, x):
        # Control flow - use scripting instead of tracing
        if x.sum() > 0:
            return torch.relu(self.linear(x))
        else:
            return torch.sigmoid(self.linear(x))

model = ModelWithControlFlow()
model.eval()

# Script the model (handles control flow)
scripted_model = torch.jit.script(model)

# Save scripted model
scripted_model.save('model_scripted.pt')
print('Model scripted and saved')

# Load and use
loaded_model = torch.jit.load('model_scripted.pt')
output = loaded_model(torch.randn(1, 10))
print(f'Output: {output}')</code></pre>

                        <h3>Optimizing for Mobile Deployment</h3>
                        <pre><code class="language-python">import torch
import torch.nn as nn
from torch.utils.mobile_optimizer import optimize_for_mobile

# Create and trace model
model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 5)
)
model.eval()

example = torch.randn(1, 10)
traced_model = torch.jit.trace(model, example)

# Optimize for mobile
mobile_model = optimize_for_mobile(traced_model)

# Save optimized model
mobile_model._save_for_lite_interpreter('model_mobile.ptl')
print('Model optimized and saved for mobile deployment')

# Model can now be loaded in iOS/Android apps</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-server"></i>
                            <strong>Production Deployment Options:</strong> For web services, use TorchServe (official PyTorch serving framework) or ONNX Runtime. For edge devices, use TorchScript with mobile optimization. For maximum performance, convert to ONNX and use TensorRT (NVIDIA GPUs).
                        </div>
                    </div>

                    <div id="best-practices" class="blog-content mt-5">
                        <h2><i class="fas fa-check-circle me-2 text-teal"></i>Best Practices & Common Pitfalls</h2>
                        
                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">Essential Best Practices</span>
                            </div>
                            <div class="card-content">
                                <h4>Model Development</h4>
                                <ul>
                                    <li><strong>Start simple:</strong> Begin with a small model, ensure it overfits on a tiny dataset (proves implementation works), then scale up.</li>
                                    <li><strong>Normalize inputs:</strong> Always normalize/standardize input data (mean=0, std=1) for faster convergence.</li>
                                    <li><strong>Batch normalization:</strong> Add batch norm layers after linear/conv layers for more stable training.</li>
                                    <li><strong>Dropout:</strong> Use dropout (0.2-0.5) to prevent overfitting, especially in fully connected layers.</li>
                                </ul>

                                <h4>Training</h4>
                                <ul>
                                    <li><strong>Learning rate:</strong> Most important hyperparameter. Start with 0.001 for Adam, 0.01-0.1 for SGD. Use learning rate scheduling.</li>
                                    <li><strong>Gradient clipping:</strong> Prevent exploding gradients with <code>torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)</code></li>
                                    <li><strong>Early stopping:</strong> Stop training when validation loss stops improving (patience ~10 epochs).</li>
                                    <li><strong>Checkpointing:</strong> Save model checkpoints every N epochs to avoid losing progress.</li>
                                </ul>

                                <h4>Common Pitfalls</h4>
                                <ul>
                                    <li><strong>Forgetting .eval():</strong> Always set model to eval mode during validation/testing to disable dropout and batch norm training behavior.</li>
                                    <li><strong>Not zeroing gradients:</strong> Always call <code>optimizer.zero_grad()</code> before backward pass—gradients accumulate by default!</li>
                                    <li><strong>Device mismatch:</strong> Ensure model and data are on the same device (both CPU or both GPU).</li>
                                    <li><strong>Wrong loss function:</strong> Use CrossEntropyLoss for classification (includes softmax), MSELoss for regression.</li>
                                    <li><strong>Data leakage:</strong> Never include test data in training. Don't normalize train and test together—fit on train, transform test.</li>
                                </ul>

                                <h4>Debugging</h4>
                                <ul>
                                    <li><strong>Check shapes:</strong> Print tensor shapes frequently. Most bugs are shape mismatches.</li>
                                    <li><strong>Overfit small batch:</strong> Ensure model can overfit 1-2 batches (loss → 0). If not, there's a bug in implementation.</li>
                                    <li><strong>Gradient checks:</strong> Verify gradients are flowing: <code>print([p.grad.abs().mean() for p in model.parameters()])</code></li>
                                    <li><strong>Learning rate finder:</strong> Plot loss vs learning rate to find optimal LR range.</li>
                                </ul>

                                <h4>Performance Optimization</h4>
                                <ul>
                                    <li><strong>GPU utilization:</strong> Use larger batch sizes to maximize GPU usage. Monitor with <code>nvidia-smi</code>.</li>
                                    <li><strong>Mixed precision:</strong> Enable AMP for 2-3x speedup on modern GPUs (Volta, Turing, Ampere).</li>
                                    <li><strong>DataLoader workers:</strong> Use num_workers=4-8 for parallel data loading on multi-core CPUs.</li>
                                    <li><strong>Pin memory:</strong> Enable pin_memory=True in DataLoader when using GPU for faster transfers.</li>
                                </ul>
                            </div>
                            <div class="card-tags">
                                <span class="bias-tag">Best Practices</span>
                                <span class="bias-tag">Production Ready</span>
                                <span class="bias-tag">Debugging</span>
                            </div>
                        </div>

                        <div class="highlight-box mt-4">
                            <i class="fas fa-graduation-cap"></i>
                            <strong>Next Steps:</strong> You now have a solid foundation in PyTorch! Practice by implementing classic papers (ResNet, LSTM), participate in Kaggle competitions, or build your own projects. The PyTorch documentation and community forums are excellent resources for continued learning. Remember: the best way to learn deep learning is by doing—start building!
                        </div>
                    </div>
                    
                    <!-- Related Posts -->
                    <div class="related-posts">
                        <h3><i class="fas fa-book me-2"></i>Related Articles in This Series</h3>
                        <div class="related-post-item">
                            <h5 class="mb-2">TensorFlow Deep Learning: Complete Beginner's Guide to Building Neural Networks</h5>
                            <p class="text-muted small mb-2">Master TensorFlow 2 and Keras for deep learning. Learn neural network architectures, training workflows, and deployment techniques.</p>
                            <a href="tensorflow-deep-learning-guide.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                        </div>
                        <div class="related-post-item">
                            <h5 class="mb-2">Part 4: Machine Learning with Scikit-learn</h5>
                            <p class="text-muted small mb-2">Build and evaluate machine learning models using Scikit-learn. Learn classification, regression, clustering, and best practices for real-world projects.</p>
                            <a href="../12/python-data-science-machine-learning.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                        </div>
                        <div class="related-post-item">
                            <h5 class="mb-2">Part 1: NumPy Foundations for Data Science</h5>
                            <p class="text-muted small mb-2">Master NumPy arrays, vectorization, broadcasting, and linear algebra operations—the foundation of Python data science.</p>
                            <a href="../12/python-data-science-numpy-foundations.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                        </div>
                    </div>

            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">
                        I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.
                    </p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook">
                            <i class="fab fa-facebook-f"></i>
                        </a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter">
                            <i class="fab fa-twitter"></i>
                        </a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn">
                            <i class="fab fa-linkedin-in"></i>
                        </a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube">
                            <i class="fab fa-youtube"></i>
                        </a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram">
                            <i class="fab fa-instagram"></i>
                        </a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest">
                            <i class="fab fa-pinterest-p"></i>
                        </a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email">
                            <i class="fas fa-envelope"></i>
                        </a>
                    </div>
                </div>
            </div>

            <hr class="bg-secondary">

            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small mb-2">
                        <i class="fas fa-camera me-2"></i>Background photo by Max Andrey from <a href="https://www.pexels.com/" target="_blank" class="text-light">Pexels</a>
                    </p>
                    <p class="small">
                        <i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a>
                    </p>
                    <p class="small mt-3">
                        <a href="/" class="text-light text-decoration-none">Home</a> | 
                        <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | 
                        <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a>
                    </p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">
                        Updated by <strong>Wasil Zafar</strong> | <time>January 1, 2026</time>
                    </p>
                </div>
            </div>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    
    <!-- Scroll-to-Top Button -->
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top">
        <i class="fas fa-arrow-up"></i>
    </button>
    
    <!-- Cookie Consent JS -->
    <script src="../../../js/cookie-consent.js"></script>
    
    <!-- Main JS -->
    <script src="../../../js/main.js"></script>
    
    <!-- Prism.js for Syntax Highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <!-- Scroll-to-Top Script -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const scrollToTopBtn = document.getElementById('scrollToTop');
            
            // Show/hide button on scroll
            window.addEventListener('scroll', function() {
                if (window.scrollY > 300) {
                    scrollToTopBtn.classList.add('show');
                } else {
                    scrollToTopBtn.classList.remove('show');
                }
            });
            
            // Smooth scroll to top on click
            scrollToTopBtn.addEventListener('click', function() {
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        });
    </script>

    <!-- Prism Theme Switcher -->
    <script>
        // Available themes with display names
        const themes = {
            'prism-theme': 'Tomorrow Night',
            'prism-default': 'Default',
            'prism-dark': 'Dark',
            'prism-twilight': 'Twilight',
            'prism-okaidia': 'Okaidia',
            'prism-solarizedlight': 'Solarized Light'
        };

        // Load saved theme from localStorage or use default
        const savedTheme = localStorage.getItem('prism-theme') || 'prism-theme';

        // Function to switch theme
        function switchTheme(themeId) {
            // Disable all themes
            Object.keys(themes).forEach(id => {
                const link = document.getElementById(id);
                if (link) {
                    link.disabled = true;
                }
            });
            
            // Enable selected theme
            const selectedLink = document.getElementById(themeId);
            if (selectedLink) {
                selectedLink.disabled = false;
                localStorage.setItem('prism-theme', themeId);
            }

            // Update all dropdowns on the page to match selected theme
            document.querySelectorAll('div.code-toolbar select').forEach(dropdown => {
                dropdown.value = themeId;
            });

            // Re-apply syntax highlighting with new theme
            setTimeout(() => {
                Prism.highlightAll();
            }, 10);
        }

        // Apply saved theme on page load
        document.addEventListener('DOMContentLoaded', function() {
            switchTheme(savedTheme);
        });

        // Add theme switcher to Prism toolbar
        Prism.plugins.toolbar.registerButton('theme-switcher', function(env) {
            const select = document.createElement('select');
            select.setAttribute('aria-label', 'Select code theme');
            select.className = 'prism-theme-selector';
            
            // Populate dropdown with themes
            Object.keys(themes).forEach(themeId => {
                const option = document.createElement('option');
                option.value = themeId;
                option.textContent = themes[themeId];
                if (themeId === savedTheme) {
                    option.selected = true;
                }
                select.appendChild(option);
            });
            
            // Handle theme change
            select.addEventListener('change', function(e) {
                switchTheme(e.target.value);
            });
            
            return select;
        });
    </script>
</body>
</html>