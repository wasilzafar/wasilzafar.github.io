<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Master computer vision fundamentals: object detection (YOLO, Faster R-CNN), segmentation (U-Net, Mask R-CNN), and generative models (GANs, Diffusion). Complete beginner's guide." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="Computer Vision, Object Detection, YOLO, Faster R-CNN, U-Net, Mask R-CNN, Segmentation, GANs, Diffusion Models, OpenCV, PyTorch, Deep Learning, Image Processing" />
    <meta property="og:title" content="Computer Vision Fundamentals: A Complete Beginner's Guide to Teaching Machines to See" />
    <meta property="og:description" content="Learn computer vision from scratch: object detection, segmentation, and generative models. Understand YOLO, R-CNN, U-Net, GANs, and diffusion models with practical examples." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-02" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>Computer Vision Fundamentals: A Complete Beginner's Guide - Wasil Zafar</title>

    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />

    <!-- Custom Styles -->
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />

    <!-- Prism.js Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />

    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <!-- Google Consent Mode v2 -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        
        gtag('consent', 'default', {
            'ad_storage': 'denied',
            'ad_user_data': 'denied',
            'ad_personalization': 'denied',
            'analytics_storage': 'denied',
            'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE']
        });
        
        gtag('consent', 'default', {
            'ad_storage': 'granted',
            'ad_user_data': 'granted',
            'ad_personalization': 'granted',
            'analytics_storage': 'granted'
        });
        
        gtag('set', 'url_passthrough', true);
    </script>

    <!-- Google Tag Manager -->
    <script>
        (function(w, d, s, l, i) {
            w[l] = w[l] || [];
            w[l].push({
                'gtm.start': new Date().getTime(),
                event: 'gtm.js'
            });
            var f = d.getElementsByTagName(s)[0],
                j = d.createElement(s),
                dl = l != 'dataLayer' ? '&l=' + l : '';
            j.async = true;
            j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    <style>
        /* Blog Post Specific Styles */
        .blog-hero {
            background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%);
            color: white;
            padding: 80px 0;
        }

        .blog-header {
            margin-bottom: 2rem;
        }

        .blog-meta {
            font-size: 0.95rem;
            color: var(--color-teal);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }

        .blog-meta span {
            margin-right: 0.5rem;
        }

        .print-btn {
            background: var(--color-teal);
            color: white;
            border: none;
            padding: 0.4rem 1rem;
            border-radius: 4px;
            font-size: 0.9rem;
            cursor: pointer;
            transition: all 0.3s ease;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
        }

        .print-btn:hover {
            background: var(--color-crimson);
            transform: translateY(-1px);
        }

        @media print {
            /* Hide print button and navigation */
            .print-btn,
            nav,
            .navbar,
            footer,
            .back-link,
            .related-posts,
            .scroll-to-top,
            .toc-toggle-btn,
            .sidenav-toc,
            .sidenav-overlay { 
                display: none !important; 
            }
            
            /* Force color printing */
            * {
                -webkit-print-color-adjust: exact !important;
                print-color-adjust: exact !important;
                color-adjust: exact !important;
            }
            
            /* Preserve header colors */
            .blog-content h2 {
                color: var(--color-navy) !important;
                border-bottom: 3px solid var(--color-teal) !important;
                page-break-after: avoid;
            }
            
            .blog-content h3 {
                color: var(--color-blue) !important;
                page-break-after: avoid;
            }
            
            .blog-content h4 {
                color: var(--color-crimson) !important;
                page-break-after: avoid;
            }
            
            /* Preserve strong text color */
            .blog-content strong {
                color: var(--color-crimson) !important;
            }
            
            /* Preserve highlight boxes */
            .highlight-box {
                background: rgba(59, 151, 151, 0.1) !important;
                border-left: 4px solid var(--color-teal) !important;
                page-break-inside: avoid;
            }
            
            /* Preserve experiment cards */
            .experiment-card {
                border: 1px solid #ddd !important;
                page-break-inside: avoid;
            }
            
            .experiment-card h4 {
                color: var(--color-crimson) !important;
            }
            
            /* Preserve badges */
            .badge {
                -webkit-print-color-adjust: exact !important;
                print-color-adjust: exact !important;
            }
            
            .bg-teal {
                background-color: var(--color-teal) !important;
                color: white !important;
            }
            
            .bg-crimson {
                background-color: var(--color-crimson) !important;
                color: white !important;
            }
            
            /* Preserve TOC box */
            .toc-box {
                border: 2px solid var(--color-teal) !important;
                page-break-inside: avoid;
            }
            
            .toc-box h3 {
                color: var(--color-navy) !important;
            }
            
            .toc-box a {
                color: var(--color-blue) !important;
            }
            
            /* Code blocks */
            pre[class*="language-"] {
                page-break-inside: avoid;
                border: 1px solid #ddd !important;
            }
            
            /* Reading time badge */
            .reading-time {
                background: var(--color-crimson) !important;
                color: white !important;
            }
            
            /* Page breaks */
            .blog-content h2 {
                page-break-before: auto;
            }
            
            /* Ensure good spacing */
            body {
                font-size: 12pt;
                line-height: 1.6;
            }
        }

        .blog-content {
            font-size: 1.05rem;
            line-height: 1.8;
            color: #333;
            text-align: justify;
        }

        .blog-content h2 {
            font-size: 1.8rem;
            font-weight: 700;
            margin-top: 2.5rem;
            margin-bottom: 1.5rem;
            color: var(--color-navy);
            border-bottom: 3px solid var(--color-teal);
            padding-bottom: 0.5rem;
        }

        .blog-content h3 {
            font-size: 1.3rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: var(--color-blue);
        }

        .blog-content p {
            margin-bottom: 1.2rem;
        }

        .blog-content strong {
            color: var(--color-crimson);
        }

        .highlight-box {
            background: rgba(59, 151, 151, 0.1);
            border-left: 4px solid var(--color-teal);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 4px;
        }

        .experiment-card {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: all 0.3s ease;
        }

        .experiment-card:hover {
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
            transform: translateY(-2px);
        }

        .experiment-card h4 {
            color: var(--color-crimson);
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        .card-meta {
            font-size: 0.9rem;
            color: var(--color-blue);
            margin-bottom: 1rem;
            font-style: italic;
        }

        .card-content {
            color: #333;
        }

        .card-tags {
            margin-top: 1rem;
        }

        .bias-tag {
            display: inline-block;
            background: var(--color-teal);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 20px;
            font-size: 0.85rem;
            margin-right: 0.5rem;
            margin-bottom: 0.5rem;
        }

        /* Side Navigation Table of Contents (Modern Overlay Style) */
        /* Toggle Button */
        .toc-toggle-btn {
            position: fixed;
            bottom: 2rem;
            left: 2rem;
            width: 60px;
            height: 60px;
            background: var(--color-teal);
            color: white;
            border: none;
            border-radius: 50%;
            font-size: 1.5rem;
            cursor: pointer;
            box-shadow: 0 4px 12px rgba(59, 151, 151, 0.4);
            transition: all 0.3s ease;
            z-index: 1049;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .toc-toggle-btn:hover {
            background: var(--color-crimson);
            transform: scale(1.1);
            box-shadow: 0 6px 16px rgba(191, 9, 47, 0.5);
        }

        .toc-toggle-btn:active {
            transform: scale(0.95);
        }

        /* Side Navigation Overlay */
        .sidenav-toc {
            height: calc(100% - 64px);
            width: 0;
            position: fixed;
            z-index: 1050;
            top: 64px;
            left: 0;
            background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%);
            overflow-x: hidden;
            overflow-y: auto;
            transition: width 0.4s ease;
            padding-top: 30px;
            box-shadow: 4px 0 15px rgba(0, 0, 0, 0.3);
        }

        .sidenav-toc.open {
            width: 350px;
        }

        /* Header row with close button and title */
        .sidenav-toc .toc-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 20px 30px;
            margin-bottom: 20px;
            border-bottom: 2px solid var(--color-teal);
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
        }

        .sidenav-toc.open .toc-header {
            opacity: 1;
            visibility: visible;
        }

        .sidenav-toc .closebtn {
            font-size: 32px;
            color: white;
            background: transparent;
            border: none;
            cursor: pointer;
            transition: all 0.3s ease;
            line-height: 1;
            padding: 0;
            margin: 0;
        }

        .sidenav-toc .closebtn:hover {
            color: var(--color-crimson);
            transform: rotate(90deg);
        }

        .sidenav-toc h3 {
            color: white;
            margin: 0;
            padding: 0;
            font-weight: 700;
            font-size: 1.3rem;
            flex-grow: 1;
        }

        .sidenav-toc ol {
            list-style: decimal;
            padding: 0;
            padding-left: 30px;
            margin: 0;
            color: rgba(255, 255, 255, 0.9);
        }

        .sidenav-toc ol li {
            margin: 0;
            margin-bottom: 8px;
        }

        .sidenav-toc ul {
            list-style-type: lower-alpha;
            padding-left: 30px;
            margin-top: 8px;
            margin-bottom: 8px;
        }

        .sidenav-toc ul li {
            margin-bottom: 6px;
        }

        .sidenav-toc a {
            padding: 12px 30px;
            text-decoration: none;
            font-size: 0.95rem;
            color: rgba(255, 255, 255, 0.85);
            display: block;
            transition: all 0.3s ease;
            border-left: 4px solid transparent;
            position: relative;
        }

        .sidenav-toc a:hover {
            color: white;
            background: rgba(59, 151, 151, 0.2);
            border-left-color: var(--color-teal);
            padding-left: 35px;
        }

        .sidenav-toc a.active {
            color: white;
            background: rgba(191, 9, 47, 0.3);
            border-left-color: var(--color-crimson);
            font-weight: 600;
        }

        .sidenav-toc a.active::before {
            content: '▶';
            position: absolute;
            left: 15px;
            font-size: 0.7rem;
            color: var(--color-crimson);
        }

        /* Overlay backdrop */
        .sidenav-overlay {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: 1049;
            transition: opacity 0.4s ease;
        }

        .sidenav-overlay.show {
            display: block;
        }

        /* Responsive adjustments */
        @media (max-width: 768px) {
            .sidenav-toc.open {
                width: 280px;
            }
            
            .toc-toggle-btn {
                width: 50px;
                height: 50px;
                font-size: 1.2rem;
                left: 15px;
            }
        }

        /* Smooth scroll behavior */
        html {
            scroll-behavior: smooth;
        }

        .reading-time {
            display: inline-block;
            background: var(--color-crimson);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 4px;
            font-size: 0.9rem;
            margin-left: 0.5rem;
        }

        .back-link {
            display: inline-block;
            color: white;
            text-decoration: none;
            transition: all 0.3s ease;
            margin-bottom: 1rem;
            opacity: 0.9;
        }

        .back-link:hover {
            color: var(--color-teal);
            opacity: 1;
            transform: translateX(-5px);
        }

        .related-posts {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 2rem;
            margin-top: 3rem;
        }

        .related-posts h3 {
            color: var(--color-navy);
            margin-bottom: 1.5rem;
        }

        .related-post-item {
            padding: 1rem;
            border-left: 3px solid var(--color-teal);
            margin-bottom: 1rem;
            transition: all 0.3s ease;
            background: white;
        }

        .related-post-item:hover {
            border-left-color: var(--color-crimson);
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .related-post-item h5 {
            color: var(--color-navy);
            margin-bottom: 0.5rem;
        }

        .related-post-item a {
            color: var(--color-blue);
            text-decoration: none;
            font-weight: 600;
        }

        .related-post-item a:hover {
            color: var(--color-crimson);
        }

        /* Code Block Styles */
        pre[class*="language-"] {
            position: relative;
            margin: 1.5rem 0;
            padding-top: 3rem;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
        }

        code[class*="language-"] {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        /* Toolbar styling */
        div.code-toolbar > .toolbar {
            opacity: 1;
            display: flex;
            gap: 0.5rem;
        }

        div.code-toolbar > .toolbar > .toolbar-item > button {
            background: var(--color-teal);
            color: white;
            border: none;
            padding: 0.4rem 0.8rem;
            border-radius: 4px;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        div.code-toolbar > .toolbar > .toolbar-item > button:hover {
            background: var(--color-blue);
            transform: translateY(-1px);
        }

        div.code-toolbar > .toolbar > .toolbar-item > button:focus {
            outline: 2px solid var(--color-teal);
            outline-offset: 2px;
        }

        /* Theme switcher dropdown */
        div.code-toolbar > .toolbar > .toolbar-item > select {
            background: var(--color-navy);
            color: white;
            border: 1px solid var(--color-teal);
            padding: 0.4rem 0.8rem;
            border-radius: 4px;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.3s ease;
            outline: none;
        }

        div.code-toolbar > .toolbar > .toolbar-item > select:hover {
            background: var(--color-blue);
            border-color: var(--color-crimson);
        }

        div.code-toolbar > .toolbar > .toolbar-item > select:focus {
            outline: 2px solid var(--color-teal);
            outline-offset: 2px;
        }

        /* Style select options */
        div.code-toolbar > .toolbar > .toolbar-item > select option {
            background: var(--color-navy);
            color: white;
        }

        /* Scroll-to-Top Button */
        .scroll-to-top {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            background: var(--color-teal);
            color: white;
            border: none;
            border-radius: 50%;
            font-size: 1.2rem;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            box-shadow: 0 4px 12px rgba(59, 151, 151, 0.3);
            z-index: 999;
        }

        .scroll-to-top.show {
            opacity: 1;
            visibility: visible;
        }

        .scroll-to-top:hover {
            background: var(--color-crimson);
            transform: translateY(-3px);
            box-shadow: 0 6px 16px rgba(191, 9, 47, 0.4);
        }

        .scroll-to-top:active {
            transform: translateY(-1px);
        }

        @media (max-width: 768px) {
            .scroll-to-top {
                bottom: 1rem;
                right: 1rem;
                width: 45px;
                height: 45px;
                font-size: 1rem;
            }
        }
    </style>
</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript>
        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe>
    </noscript>

    <!-- GDPR Cookie Consent Banner -->
    <div id="cookieBanner" class="light display-bottom" style="display: none;">
        <div id="closeIcon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
                <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"></path>
            </svg>
        </div>
        
        <div class="content-wrap">
            <div class="msg-wrap">
                <div class="title-wrap">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="20" height="20">
                        <path fill="#3B9797" d="M510.52 255.82c-69.97-.85-126.47-57.69-126.47-127.86-70.17 0-127-56.49-127.86-126.45-27.26-4.14-55.13.3-79.72 12.82l-69.13 35.22a132.221 132.221 0 0 0-57.79 57.81l-35.1 68.88a132.645 132.645 0 0 0-12.82 80.95l12.08 76.27a132.521 132.521 0 0 0 37.16 70.37l54.64 54.64a132.036 132.036 0 0 0 70.37 37.16l76.27 12.15c27.51 4.36 55.7-.11 80.95-12.8l68.88-35.08a132.166 132.166 0 0 0 57.79-57.81l35.1-68.88c12.56-24.64 17.01-52.58 12.91-79.91zM176 368c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32zm32-160c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32zm160 128c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32z"></path>
                    </svg>
                    <h4 style="margin: 0; font-size: 18px; color: var(--color-navy); font-weight: 700;">Cookie Consent</h4>
                </div>
                <p style="font-size: 14px; line-height: 1.6; color: var(--color-navy); margin-bottom: 15px;">
                    We use cookies to enhance your browsing experience, serve personalized content, and analyze our traffic. 
                    By clicking "Accept All", you consent to our use of cookies. See our 
                    <a href="/privacy-policy.html" style="color: var(--color-teal); border-bottom: 1px dotted var(--color-teal);">Privacy Policy</a> 
                    for more information.
                </p>
                
                <div id="cookieSettings" style="display: none;">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="14" height="14">
                        <path fill="currentColor" d="M487.4 315.7l-42.6-24.6c4.3-23.2 4.3-47 0-70.2l42.6-24.6c4.9-2.8 7.1-8.6 5.5-14-11.1-35.6-30-67.8-54.7-94.6-3.8-4.1-10-5.1-14.8-2.3L380.8 110c-17.9-15.4-38.5-27.3-60.8-35.1V25.8c0-5.6-3.9-10.5-9.4-11.7-36.7-8.2-74.3-7.8-109.2 0-5.5 1.2-9.4 6.1-9.4 11.7V75c-22.2 7.9-42.8 19.8-60.8 35.1L88.7 85.5c-4.9-2.8-11-1.9-14.8 2.3-24.7 26.7-43.6 58.9-54.7 94.6-1.7 5.4.6 11.2 5.5 14L67.3 221c-4.3 23.2-4.3 47 0 70.2l-42.6 24.6c-4.9 2.8-7.1 8.6-5.5 14 11.1 35.6 30 67.8 54.7 94.6 3.8 4.1 10 5.1 14.8 2.3l42.6-24.6c17.9 15.4 38.5 27.3 60.8 35.1v49.2c0 5.6 3.9 10.5 9.4 11.7 36.7 8.2 74.3 7.8 109.2 0 5.5-1.2 9.4-6.1 9.4-11.7v-49.2c22.2-7.9 42.8-19.8 60.8-35.1l42.6 24.6c4.9 2.8 11 1.9 14.8-2.3 24.7-26.7 43.6-58.9 54.7-94.6 1.5-5.5-.7-11.3-5.6-14.1zM256 336c-44.1 0-80-35.9-80-80s35.9-80 80-80 80 35.9 80 80-35.9 80-80 80z"></path>
                    </svg>
                    <span style="margin-left: 5px; font-size: 12px; font-weight: 600; color: var(--color-navy);">Customize Settings</span>
                </div>
                
                <div id="cookieTypes" style="display: none; margin-top: 15px; padding-top: 15px; border-top: 1px solid rgba(59, 151, 151, 0.2);">
                    <h5 style="font-size: 12px; font-weight: 700; color: var(--color-navy); margin-bottom: 10px; text-transform: uppercase;">Cookie Preferences</h5>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" checked disabled style="margin-top: 2px; margin-right: 8px; cursor: not-allowed;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Essential Cookies (Required)</strong>
                                <span style="font-size: 12px; color: #666;">Necessary for the website to function properly.</span>
                            </div>
                        </label>
                    </div>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" id="analyticsCookies" checked style="margin-top: 2px; margin-right: 8px;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Analytics Cookies</strong>
                                <span style="font-size: 12px; color: #666;">Help us understand how you interact with the website.</span>
                            </div>
                        </label>
                    </div>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" id="marketingCookies" style="margin-top: 2px; margin-right: 8px;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Marketing Cookies</strong>
                                <span style="font-size: 12px; color: #666;">Used to deliver relevant advertisements.</span>
                            </div>
                        </label>
                    </div>
                </div>
            </div>
            
            <div class="btn-wrap">
                <button id="cookieAccept" style="background: var(--color-teal); color: white; font-weight: 600;">Accept All</button>
                <button id="cookieReject" style="background: transparent; color: var(--color-navy); border: 2px solid var(--color-teal); font-weight: 600;">Reject All</button>
                <button id="cookieSave" style="background: var(--color-blue); color: white; font-weight: 600; display: none;">Save Preferences</button>
            </div>
        </div>
    </div>

    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/">
                <span class="gradient-text">Wasil Zafar</span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="/">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#about">About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#skills">Skills</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#certifications">Certifications</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#interests">Interests</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Blog Hero Section -->
    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link">
                    <i class="fas fa-arrow-left me-2"></i>Back to Technology
                </a>
                <h1 class="display-4 fw-bold mb-3">
                    Computer Vision Fundamentals: A Complete Beginner's Guide to Teaching Machines to See
                </h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 2, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-2"></i>28 min read</span>
                    <button onclick="window.print()" class="print-btn" title="Print this article">
                        <i class="fas fa-print"></i> Print
                    </button>
                </div>
                <p class="lead">Master the fundamentals of computer vision: from object detection with YOLO and Faster R-CNN, to pixel-perfect segmentation with U-Net and Mask R-CNN, to cutting-edge generative models like GANs and Diffusion Models.</p>
            </div>
        </div>
    </section>

    <!-- Main Content -->
    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="blog-content">
                        
                        <!-- Table of Contents Toggle Button -->
                        <button class="toc-toggle-btn" onclick="openNav()" title="Table of Contents" aria-label="Open Table of Contents">
                            <i class="fas fa-list"></i>
                        </button>

                        <!-- Side Navigation Overlay -->
                        <div id="tocSidenav" class="sidenav-toc">
                            <div class="toc-header">
                                <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
                                <button class="closebtn" onclick="closeNav()" aria-label="Close Table of Contents">&times;</button>
                            </div>
                            <ol>
                                <li><a href="#what-is-cv" onclick="closeNav()">What is Computer Vision?</a></li>
                                <li><a href="#object-detection" onclick="closeNav()">Object Detection: Finding Things in Images</a>
                                    <ul>
                                        <li><a href="#yolo" onclick="closeNav()">YOLO (You Only Look Once)</a></li>
                                        <li><a href="#faster-rcnn" onclick="closeNav()">Faster R-CNN</a></li>
                                        <li><a href="#yolo-vs-rcnn" onclick="closeNav()">YOLO vs Faster R-CNN Comparison</a></li>
                                    </ul>
                                </li>
                                <li><a href="#segmentation" onclick="closeNav()">Image Segmentation: Pixel-Perfect Understanding</a>
                                    <ul>
                                        <li><a href="#unet" onclick="closeNav()">Semantic Segmentation with U-Net</a></li>
                                        <li><a href="#mask-rcnn" onclick="closeNav()">Instance Segmentation with Mask R-CNN</a></li>
                                        <li><a href="#semantic-vs-instance" onclick="closeNav()">Semantic vs Instance Segmentation</a></li>
                                    </ul>
                                </li>
                                <li><a href="#generative" onclick="closeNav()">Generative Models: Creating and Transforming Images</a>
                                    <ul>
                                        <li><a href="#gans" onclick="closeNav()">GANs (Generative Adversarial Networks)</a></li>
                                        <li><a href="#diffusion" onclick="closeNav()">Diffusion Models (State of the Art)</a></li>
                                    </ul>
                                </li>
                                <li><a href="#frameworks" onclick="closeNav()">Frameworks & Tools: Your CV Toolkit</a></li>
                                <li><a href="#choosing" onclick="closeNav()">Choosing the Right Approach</a></li>
                                <li><a href="#learning-path" onclick="closeNav()">Learning Path & Next Steps</a></li>
                                <li><a href="#best-practices" onclick="closeNav()">Best Practices & Common Pitfalls</a></li>
                            </ol>
                        </div>

                        <!-- Overlay Backdrop -->
                        <div id="tocOverlay" class="sidenav-overlay" onclick="closeNav()"></div>

                        <!-- Section 1: What is Computer Vision? -->
                        <h2 id="what-is-cv"><i class="fas fa-eye me-2"></i>What is Computer Vision?</h2>
                        
                        <p>Computer Vision (CV) is a field of artificial intelligence that enables machines to <strong>see, interpret, and reason</strong> about images and videos—similar to how humans use their eyes and brain to understand the visual world. But instead of biological neurons, computer vision uses mathematical algorithms and neural networks to extract meaningful information from visual data.</p>

                        <p>Think of it this way: when you look at a photograph, your brain instantly recognizes faces, objects, text, emotions, and spatial relationships. Computer vision aims to give machines this same remarkable ability—and in many cases, surpass human capabilities in speed, consistency, and scale.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>Key Insight</h4>
                            <p class="mb-0">Computer vision is the "eyes" of AI. While Natural Language Processing (NLP) teaches machines to understand text, and speech recognition handles audio, computer vision is specifically designed to extract knowledge from pixels—whether in photographs, videos, medical scans, satellite imagery, or real-time camera feeds.</p>
                        </div>

                        <h3>The Core Tasks of Computer Vision</h3>

                        <p>Computer vision encompasses a wide range of tasks, each answering a different question about visual data:</p>

                        <div class="row">
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4><i class="fas fa-image me-2"></i>Image Classification</h4>
                                    <div class="card-meta">Question: "What is in this image?"</div>
                                    <div class="card-content">
                                        <p>The model assigns a single label to an entire image. For example, classifying an image as "cat," "dog," or "car."</p>
                                        <p class="mb-0"><strong>Example:</strong> Is this chest X-ray normal or does it show pneumonia?</p>
                                    </div>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4><i class="fas fa-vector-square me-2"></i>Object Detection</h4>
                                    <div class="card-meta">Question: "Where are objects and what are they?"</div>
                                    <div class="card-content">
                                        <p>The model identifies multiple objects in an image and draws bounding boxes around each one with class labels.</p>
                                        <p class="mb-0"><strong>Example:</strong> Finding all pedestrians and vehicles in a traffic camera feed.</p>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <div class="row">
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4><i class="fas fa-th me-2"></i>Image Segmentation</h4>
                                    <div class="card-meta">Question: "Which exact pixels belong to each object?"</div>
                                    <div class="card-content">
                                        <p>The model classifies every single pixel in the image, creating precise boundaries around objects.</p>
                                        <p class="mb-0"><strong>Example:</strong> Identifying exactly which pixels are tumor tissue in an MRI scan.</p>
                                    </div>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4><i class="fas fa-magic me-2"></i>Generative Vision</h4>
                                    <div class="card-meta">Question: "Can we create or modify images?"</div>
                                    <div class="card-content">
                                        <p>The model generates new images from scratch or transforms existing ones based on learned patterns.</p>
                                        <p class="mb-0"><strong>Example:</strong> Creating photorealistic images from text descriptions (like DALL·E or Stable Diffusion).</p>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <h3>Real-World Applications</h3>

                        <p>Computer vision has become ubiquitous in our daily lives, often working invisibly behind the scenes:</p>

                        <ul>
                            <li><strong>Autonomous Vehicles:</strong> Tesla, Waymo, and other self-driving systems use CV to understand road conditions, detect obstacles, and navigate safely.</li>
                            <li><strong>Healthcare:</strong> AI systems detect diseases in medical images—from diabetic retinopathy in eye scans to cancer in mammograms—often matching or exceeding expert radiologists.</li>
                            <li><strong>Retail:</strong> Amazon Go stores use CV to track what customers pick up, enabling checkout-free shopping.</li>
                            <li><strong>Security:</strong> Facial recognition for device unlocking (Face ID) and surveillance systems for public safety.</li>
                            <li><strong>Agriculture:</strong> Drones with CV identify crop diseases, monitor growth, and optimize irrigation.</li>
                            <li><strong>Manufacturing:</strong> Quality control systems inspect products for defects at speeds impossible for human workers.</li>
                        </ul>

                        <div class="highlight-box">
                            <h4><i class="fas fa-chart-line me-2"></i>The Growth of Computer Vision</h4>
                            <p class="mb-0">The computer vision market is projected to exceed $50 billion by 2030. The field has accelerated dramatically since 2012 when deep learning revolutionized image recognition. Today, models can identify thousands of object categories with superhuman accuracy, and generative AI can create images indistinguishable from photographs.</p>
                        </div>

                        <!-- Section 2: Object Detection -->
                        <h2 id="object-detection"><i class="fas fa-search-location me-2"></i>Object Detection: Finding Things in Images</h2>

                        <p>Object detection is one of the most practical and widely-deployed computer vision tasks. Unlike simple image classification (which answers "what is this image?"), object detection answers <strong>two questions simultaneously: "What objects are in this image?" and "Where exactly are they?"</strong></p>

                        <p>The output of an object detection model includes:</p>
                        <ul>
                            <li><strong>Bounding boxes:</strong> Rectangular coordinates (x, y, width, height) around each detected object</li>
                            <li><strong>Class labels:</strong> What type of object each bounding box contains (person, car, dog, etc.)</li>
                            <li><strong>Confidence scores:</strong> How certain the model is about each detection (0-100%)</li>
                        </ul>

                        <h3 id="yolo">YOLO (You Only Look Once)</h3>

                        <p>YOLO revolutionized object detection when it was introduced in 2016 by Joseph Redmon. The name says it all: unlike previous approaches that looked at an image multiple times to find objects, <strong>YOLO processes the entire image in a single forward pass</strong> through the neural network.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-cogs me-2"></i>How YOLO Works</h4>
                            <div class="card-meta">Single-Stage Detection Architecture</div>
                            <div class="card-content">
                                <p>YOLO divides the input image into a grid (e.g., 13×13 cells). Each grid cell is responsible for detecting objects whose center falls within that cell. For each cell, YOLO predicts:</p>
                                <ol>
                                    <li><strong>Bounding box coordinates</strong> (x, y, width, height) relative to the cell</li>
                                    <li><strong>Objectness score</strong> — confidence that a box contains any object</li>
                                    <li><strong>Class probabilities</strong> — likelihood of each object category</li>
                                </ol>
                                <p class="mb-0">All predictions happen simultaneously in one network pass, making YOLO extremely fast—capable of processing 45-155 frames per second depending on the version.</p>
                            </div>
                        </div>

                        <h4>YOLO Versions Evolution</h4>

                        <p>YOLO has evolved significantly since its introduction:</p>

                        <div class="table-responsive">
                            <table class="table table-bordered">
                                <thead class="table-dark">
                                    <tr>
                                        <th>Version</th>
                                        <th>Year</th>
                                        <th>Key Improvements</th>
                                        <th>Best For</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>YOLOv3</strong></td>
                                        <td>2018</td>
                                        <td>Multi-scale detection, better small object handling</td>
                                        <td>General purpose</td>
                                    </tr>
                                    <tr>
                                        <td><strong>YOLOv5</strong></td>
                                        <td>2020</td>
                                        <td>PyTorch-native, easier training, excellent docs</td>
                                        <td>Production deployment</td>
                                    </tr>
                                    <tr>
                                        <td><strong>YOLOv7</strong></td>
                                        <td>2022</td>
                                        <td>State-of-the-art speed-accuracy tradeoff</td>
                                        <td>High-performance needs</td>
                                    </tr>
                                    <tr>
                                        <td><strong>YOLOv8</strong></td>
                                        <td>2023</td>
                                        <td>Unified framework (detect, segment, classify, pose)</td>
                                        <td>Modern projects (recommended)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>YOLO-NAS</strong></td>
                                        <td>2023</td>
                                        <td>Neural Architecture Search optimized</td>
                                        <td>Edge deployment</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <h4>YOLO Strengths and Weaknesses</h4>

                        <div class="row">
                            <div class="col-md-6">
                                <div class="highlight-box" style="border-left-color: var(--color-teal);">
                                    <h5><i class="fas fa-check-circle me-2 text-success"></i>Strengths</h5>
                                    <ul class="mb-0">
                                        <li>Real-time detection (30-150+ FPS)</li>
                                        <li>Simple deployment pipeline</li>
                                        <li>Edge and mobile-friendly</li>
                                        <li>Excellent community support</li>
                                        <li>Unified framework for multiple tasks</li>
                                    </ul>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="highlight-box" style="border-left-color: var(--color-crimson);">
                                    <h5><i class="fas fa-times-circle me-2 text-danger"></i>Weaknesses</h5>
                                    <ul class="mb-0">
                                        <li>Slightly less accurate than two-stage detectors</li>
                                        <li>Struggles with very small objects</li>
                                        <li>Difficulty with overlapping objects</li>
                                        <li>Fixed grid can miss dense object clusters</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <h4>Common Use Cases for YOLO</h4>

                        <div class="card-tags mb-4">
                            <span class="bias-tag"><i class="fas fa-car me-2"></i>Traffic Monitoring</span>
                            <span class="bias-tag"><i class="fas fa-video me-2"></i>Surveillance</span>
                            <span class="bias-tag"><i class="fas fa-road me-2"></i>Autonomous Driving</span>
                            <span class="bias-tag"><i class="fas fa-store me-2"></i>Retail Analytics</span>
                            <span class="bias-tag"><i class="fas fa-warehouse me-2"></i>Warehouse Automation</span>
                            <span class="bias-tag"><i class="fas fa-futbol me-2"></i>Sports Analytics</span>
                        </div>

                        <h3 id="faster-rcnn">Faster R-CNN</h3>

                        <p>Faster R-CNN represents a different philosophy: <strong>prioritize accuracy over speed</strong>. It's a two-stage detector, meaning it looks at the image twice—first to propose regions that might contain objects, then to classify and refine those regions.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-project-diagram me-2"></i>How Faster R-CNN Works</h4>
                            <div class="card-meta">Two-Stage Detection Architecture</div>
                            <div class="card-content">
                                <p><strong>Stage 1 — Region Proposal Network (RPN):</strong></p>
                                <p>A small neural network slides over the image feature map and proposes ~2000 rectangular regions that likely contain objects. These are called "region proposals" or "anchors."</p>
                                
                                <p><strong>Stage 2 — Classification & Refinement:</strong></p>
                                <p>Each proposed region is:</p>
                                <ol>
                                    <li>Cropped and resized to a fixed size (ROI Pooling)</li>
                                    <li>Passed through fully connected layers</li>
                                    <li>Classified into object categories (or background)</li>
                                    <li>Refined with more precise bounding box coordinates</li>
                                </ol>
                                <p class="mb-0">This two-stage approach allows Faster R-CNN to carefully examine each candidate region, resulting in higher accuracy—especially for small or occluded objects.</p>
                            </div>
                        </div>

                        <h4>Faster R-CNN Strengths and Weaknesses</h4>

                        <div class="row">
                            <div class="col-md-6">
                                <div class="highlight-box" style="border-left-color: var(--color-teal);">
                                    <h5><i class="fas fa-check-circle me-2 text-success"></i>Strengths</h5>
                                    <ul class="mb-0">
                                        <li>High detection accuracy</li>
                                        <li>Excellent for small objects</li>
                                        <li>Better handling of occluded objects</li>
                                        <li>More precise bounding boxes</li>
                                        <li>Foundation for Mask R-CNN</li>
                                    </ul>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="highlight-box" style="border-left-color: var(--color-crimson);">
                                    <h5><i class="fas fa-times-circle me-2 text-danger"></i>Weaknesses</h5>
                                    <ul class="mb-0">
                                        <li>Slower than single-stage detectors (5-7 FPS)</li>
                                        <li>Higher computational cost</li>
                                        <li>More complex training pipeline</li>
                                        <li>Not suitable for real-time applications</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <h4>Common Use Cases for Faster R-CNN</h4>

                        <div class="card-tags mb-4">
                            <span class="bias-tag"><i class="fas fa-x-ray me-2"></i>Medical Imaging</span>
                            <span class="bias-tag"><i class="fas fa-satellite me-2"></i>Satellite Imagery</span>
                            <span class="bias-tag"><i class="fas fa-microscope me-2"></i>Scientific Research</span>
                            <span class="bias-tag"><i class="fas fa-file-alt me-2"></i>Document Analysis</span>
                            <span class="bias-tag"><i class="fas fa-chart-bar me-2"></i>High-Precision Analytics</span>
                        </div>

                        <h3 id="yolo-vs-rcnn">YOLO vs Faster R-CNN: Head-to-Head Comparison</h3>

                        <p>Choosing between YOLO and Faster R-CNN depends on your specific requirements. Here's a comprehensive comparison:</p>

                        <div class="table-responsive">
                            <table class="table table-bordered table-striped">
                                <thead class="table-dark">
                                    <tr>
                                        <th>Feature</th>
                                        <th>YOLO</th>
                                        <th>Faster R-CNN</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Speed</strong></td>
                                        <td>⭐⭐⭐⭐⭐ (30-150+ FPS)</td>
                                        <td>⭐⭐ (5-7 FPS)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Accuracy</strong></td>
                                        <td>⭐⭐⭐⭐ (Good)</td>
                                        <td>⭐⭐⭐⭐⭐ (Excellent)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Real-time Capable</strong></td>
                                        <td>✅ Yes</td>
                                        <td>❌ No</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Small Objects</strong></td>
                                        <td>⭐⭐⭐ (Moderate)</td>
                                        <td>⭐⭐⭐⭐⭐ (Excellent)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Implementation Complexity</strong></td>
                                        <td>Low</td>
                                        <td>High</td>
                                    </tr>
                                    <tr>
                                        <td><strong>GPU Memory</strong></td>
                                        <td>Lower</td>
                                        <td>Higher</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Edge Deployment</strong></td>
                                        <td>Excellent</td>
                                        <td>Challenging</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Best Framework</strong></td>
                                        <td>Ultralytics, PyTorch</td>
                                        <td>Detectron2, MMDetection</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="highlight-box">
                            <h4><i class="fas fa-balance-scale me-2"></i>When to Choose Which?</h4>
                            <p><strong>Choose YOLO when:</strong> You need real-time detection, are deploying on edge devices, have limited computational resources, or speed matters more than perfect accuracy.</p>
                            <p class="mb-0"><strong>Choose Faster R-CNN when:</strong> Accuracy is paramount, you're working with small objects, processing time isn't critical (batch processing), or you need a foundation for instance segmentation (Mask R-CNN).</p>
                        </div>

                        <!-- Section 3: Image Segmentation -->
                        <h2 id="segmentation"><i class="fas fa-th me-2"></i>Image Segmentation: Pixel-Perfect Understanding</h2>

                        <p>While object detection draws bounding boxes around objects, <strong>image segmentation goes deeper</strong>—it classifies every single pixel in an image. This provides precise boundaries and enables applications where exact shape matters, like medical imaging or autonomous driving.</p>

                        <p>There are two main types of segmentation:</p>

                        <div class="row mb-4">
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4><i class="fas fa-layer-group me-2"></i>Semantic Segmentation</h4>
                                    <div class="card-content">
                                        <p>Labels every pixel with a class, but doesn't distinguish between individual instances.</p>
                                        <p class="mb-0"><strong>Example:</strong> All cars are labeled "car" (same color), regardless of how many cars there are.</p>
                                    </div>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4><i class="fas fa-object-ungroup me-2"></i>Instance Segmentation</h4>
                                    <div class="card-content">
                                        <p>Labels every pixel AND distinguishes between individual objects of the same class.</p>
                                        <p class="mb-0"><strong>Example:</strong> Car #1 (red), Car #2 (blue), Car #3 (green)—each instance gets a unique mask.</p>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <h3 id="unet">Semantic Segmentation with U-Net</h3>

                        <p>U-Net is one of the most influential architectures in computer vision, originally designed for biomedical image segmentation. Its elegant design has made it the <strong>go-to architecture for semantic segmentation</strong>, especially when training data is limited.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-sitemap me-2"></i>U-Net Architecture: The Encoder-Decoder Design</h4>
                            <div class="card-meta">Named for its U-shaped structure</div>
                            <div class="card-content">
                                <p>U-Net consists of two symmetric paths:</p>
                                
                                <p><strong>Encoder (Contracting Path) — Left side of the U:</strong></p>
                                <ul>
                                    <li>Series of convolutional layers + max pooling</li>
                                    <li>Progressively reduces spatial dimensions</li>
                                    <li>Captures "what" is in the image (features)</li>
                                </ul>
                                
                                <p><strong>Decoder (Expanding Path) — Right side of the U:</strong></p>
                                <ul>
                                    <li>Series of up-convolutions (transposed convolutions)</li>
                                    <li>Progressively restores spatial dimensions</li>
                                    <li>Produces pixel-wise predictions</li>
                                </ul>
                                
                                <p><strong>Skip Connections — The bridges across the U:</strong></p>
                                <p class="mb-0">Features from the encoder are concatenated with features in the decoder at corresponding resolutions. This preserves fine spatial details that would otherwise be lost during downsampling.</p>
                            </div>
                        </div>

                        <h4>Why U-Net Works So Well</h4>

                        <div class="highlight-box">
                            <h4><i class="fas fa-star me-2"></i>The Skip Connection Secret</h4>
                            <p>The key innovation of U-Net is its skip connections. When an image is downsampled, we lose fine details (edges, small structures). Skip connections allow the decoder to access these details directly from the encoder, combining:</p>
                            <ul class="mb-0">
                                <li><strong>High-level features</strong> (from the bottleneck): What objects are present</li>
                                <li><strong>Low-level features</strong> (from skip connections): Precise boundaries and details</li>
                            </ul>
                        </div>

                        <h4>U-Net Strengths and Use Cases</h4>

                        <div class="row">
                            <div class="col-md-6">
                                <div class="highlight-box" style="border-left-color: var(--color-teal);">
                                    <h5><i class="fas fa-check-circle me-2 text-success"></i>Strengths</h5>
                                    <ul class="mb-0">
                                        <li>Works well with limited training data</li>
                                        <li>Precise boundary delineation</li>
                                        <li>Fast inference</li>
                                        <li>Easy to implement and modify</li>
                                        <li>Many pretrained variants available</li>
                                    </ul>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="card-tags">
                                    <p><strong>Common Applications:</strong></p>
                                    <span class="bias-tag"><i class="fas fa-heartbeat me-2"></i>Tumor Detection</span>
                                    <span class="bias-tag"><i class="fas fa-lungs me-2"></i>Organ Segmentation</span>
                                    <span class="bias-tag"><i class="fas fa-satellite me-2"></i>Land Cover Mapping</span>
                                    <span class="bias-tag"><i class="fas fa-file-image me-2"></i>Document Layout</span>
                                    <span class="bias-tag"><i class="fas fa-dna me-2"></i>Cell Segmentation</span>
                                </div>
                            </div>
                        </div>

                        <h3 id="mask-rcnn">Instance Segmentation with Mask R-CNN</h3>

                        <p>Mask R-CNN extends Faster R-CNN by adding a <strong>segmentation branch</strong> that predicts a pixel-wise mask for each detected object. It answers three questions at once: "What objects are here?", "Where are they?", and "What is their exact shape?"</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-masks-theater me-2"></i>How Mask R-CNN Works</h4>
                            <div class="card-meta">Faster R-CNN + Mask Branch</div>
                            <div class="card-content">
                                <p>Mask R-CNN builds on Faster R-CNN's two-stage approach:</p>
                                
                                <p><strong>Stage 1 — Region Proposal Network:</strong> Same as Faster R-CNN, proposes candidate object regions.</p>
                                
                                <p><strong>Stage 2 — Three parallel branches:</strong></p>
                                <ol>
                                    <li><strong>Classification branch:</strong> What class is this object?</li>
                                    <li><strong>Bounding box branch:</strong> Refine the box coordinates</li>
                                    <li><strong>Mask branch:</strong> Predict a binary mask for the object (new!)</li>
                                </ol>
                                
                                <p><strong>Key Innovation — RoIAlign:</strong></p>
                                <p class="mb-0">Mask R-CNN introduces RoIAlign, which uses bilinear interpolation instead of quantization when extracting features. This small change significantly improves mask accuracy by preserving spatial precision.</p>
                            </div>
                        </div>

                        <h4>Mask R-CNN Output</h4>

                        <p>For each detected object, Mask R-CNN provides:</p>
                        <ul>
                            <li><strong>Bounding box:</strong> Rectangle around the object</li>
                            <li><strong>Class label:</strong> What type of object (person, car, etc.)</li>
                            <li><strong>Confidence score:</strong> Detection certainty</li>
                            <li><strong>Instance mask:</strong> Pixel-perfect silhouette of the object</li>
                        </ul>

                        <h4>Mask R-CNN Strengths and Weaknesses</h4>

                        <div class="row">
                            <div class="col-md-6">
                                <div class="highlight-box" style="border-left-color: var(--color-teal);">
                                    <h5><i class="fas fa-check-circle me-2 text-success"></i>Strengths</h5>
                                    <ul class="mb-0">
                                        <li>Fine-grained object understanding</li>
                                        <li>Handles overlapping objects</li>
                                        <li>Works well in cluttered scenes</li>
                                        <li>Flexible backbone choices</li>
                                        <li>Extensible (pose estimation, etc.)</li>
                                    </ul>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="highlight-box" style="border-left-color: var(--color-crimson);">
                                    <h5><i class="fas fa-times-circle me-2 text-danger"></i>Weaknesses</h5>
                                    <ul class="mb-0">
                                        <li>Computationally expensive</li>
                                        <li>Slower than YOLO (5-10 FPS)</li>
                                        <li>Complex training setup</li>
                                        <li>High GPU memory requirements</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <h3 id="semantic-vs-instance">Semantic vs Instance Segmentation: When to Use Which?</h3>

                        <div class="table-responsive">
                            <table class="table table-bordered">
                                <thead class="table-dark">
                                    <tr>
                                        <th>Aspect</th>
                                        <th>Semantic Segmentation (U-Net)</th>
                                        <th>Instance Segmentation (Mask R-CNN)</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Output</strong></td>
                                        <td>All objects of same class share one mask</td>
                                        <td>Each object has its own unique mask</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Can count objects?</strong></td>
                                        <td>❌ No</td>
                                        <td>✅ Yes</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Speed</strong></td>
                                        <td>Faster</td>
                                        <td>Slower</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Complexity</strong></td>
                                        <td>Simpler</td>
                                        <td>More complex</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Best for</strong></td>
                                        <td>Roads, tumors, backgrounds, land cover</td>
                                        <td>People, cars, products, countable objects</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Framework</strong></td>
                                        <td>Segmentation Models PyTorch</td>
                                        <td>Detectron2, MMDetection</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>Rule of Thumb</h4>
                            <p><strong>Use Semantic Segmentation when:</strong> You care about regions, not individual objects. "Where is the road?" "What area is forest?"</p>
                            <p class="mb-0"><strong>Use Instance Segmentation when:</strong> You need to identify and count individual objects. "How many people?" "Track each car separately."</p>
                        </div>

                        <!-- Section 4: Generative Models -->
                        <h2 id="generative"><i class="fas fa-magic me-2"></i>Generative Models: Creating and Transforming Images</h2>

                        <p>While detection and segmentation analyze existing images, <strong>generative models create new visual content</strong>. This is perhaps the most exciting frontier in computer vision—machines that can imagine, create, and transform images in ways that were science fiction just a few years ago.</p>

                        <p>Generative models have two main capabilities:</p>
                        <ul>
                            <li><strong>Image Generation:</strong> Creating entirely new images from random noise or text descriptions</li>
                            <li><strong>Image Transformation:</strong> Modifying existing images—style transfer, enhancement, inpainting, super-resolution</li>
                        </ul>

                        <h3 id="gans">GANs (Generative Adversarial Networks)</h3>

                        <p>GANs, introduced by Ian Goodfellow in 2014, revolutionized generative AI with an elegant adversarial training approach. The core idea: <strong>two neural networks compete against each other</strong>, and through this competition, both get better.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-gamepad me-2"></i>The GAN Game: Generator vs Discriminator</h4>
                            <div class="card-meta">A Minimax Game Between Two Networks</div>
                            <div class="card-content">
                                <p><strong>The Generator (The Forger):</strong></p>
                                <ul>
                                    <li>Takes random noise as input</li>
                                    <li>Tries to create fake images that look real</li>
                                    <li>Goal: Fool the discriminator</li>
                                </ul>
                                
                                <p><strong>The Discriminator (The Detective):</strong></p>
                                <ul>
                                    <li>Receives both real images and generated fakes</li>
                                    <li>Tries to distinguish real from fake</li>
                                    <li>Goal: Catch the generator's fakes</li>
                                </ul>
                                
                                <p><strong>The Training Process:</strong></p>
                                <p class="mb-0">Both networks improve through competition. The generator creates better fakes to fool the discriminator, while the discriminator becomes better at spotting fakes. At equilibrium, the generator produces images so realistic that the discriminator can't tell them apart from real images (50% accuracy = random guessing).</p>
                            </div>
                        </div>

                        <h4>Popular GAN Variants</h4>

                        <div class="row">
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4>DCGAN</h4>
                                    <div class="card-meta">Deep Convolutional GAN</div>
                                    <div class="card-content">
                                        <p class="mb-0">The first successful architecture for generating realistic images using deep convolutional networks. Established best practices for stable GAN training.</p>
                                    </div>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4>StyleGAN</h4>
                                    <div class="card-meta">NVIDIA's Style-Based Generator</div>
                                    <div class="card-content">
                                        <p class="mb-0">Produces incredibly photorealistic faces. Introduces style mixing and fine-grained control over generated features. Used in "This Person Does Not Exist."</p>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <div class="row">
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4>CycleGAN</h4>
                                    <div class="card-meta">Unpaired Image-to-Image Translation</div>
                                    <div class="card-content">
                                        <p class="mb-0">Transforms images between domains without paired examples. Can turn horses into zebras, summer into winter, photos into paintings.</p>
                                    </div>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4>Pix2Pix</h4>
                                    <div class="card-meta">Paired Image-to-Image Translation</div>
                                    <div class="card-content">
                                        <p class="mb-0">Learns mappings between paired images. Sketch to photo, segmentation map to realistic image, day to night conversion.</p>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <h4>GAN Applications</h4>

                        <div class="card-tags mb-4">
                            <span class="bias-tag"><i class="fas fa-expand-arrows-alt me-2"></i>Image Super-Resolution</span>
                            <span class="bias-tag"><i class="fas fa-palette me-2"></i>Style Transfer</span>
                            <span class="bias-tag"><i class="fas fa-user me-2"></i>Face Generation</span>
                            <span class="bias-tag"><i class="fas fa-database me-2"></i>Data Augmentation</span>
                            <span class="bias-tag"><i class="fas fa-paint-brush me-2"></i>Image Inpainting</span>
                            <span class="bias-tag"><i class="fas fa-film me-2"></i>Video Synthesis</span>
                        </div>

                        <h4>GAN Challenges</h4>

                        <div class="highlight-box" style="border-left-color: var(--color-crimson);">
                            <h4><i class="fas fa-exclamation-triangle me-2"></i>Training Difficulties</h4>
                            <ul class="mb-0">
                                <li><strong>Training Instability:</strong> GANs are notoriously difficult to train. The generator and discriminator must be balanced—if one gets too strong, training collapses.</li>
                                <li><strong>Mode Collapse:</strong> The generator might learn to produce only a few types of outputs, ignoring the diversity in the training data.</li>
                                <li><strong>Evaluation Difficulty:</strong> There's no single metric to measure how "good" generated images are. FID and IS scores help but are imperfect.</li>
                            </ul>
                        </div>

                        <h3 id="diffusion">Diffusion Models (State of the Art)</h3>

                        <p>Diffusion models have <strong>dethroned GANs</strong> as the state-of-the-art for image generation. They power the AI art revolution—DALL·E, Midjourney, and Stable Diffusion are all diffusion models. The results are stunning: photorealistic images, creative artwork, and unprecedented control over generation.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-wind me-2"></i>How Diffusion Models Work</h4>
                            <div class="card-meta">Learning to Reverse Noise</div>
                            <div class="card-content">
                                <p>The core idea is beautifully simple:</p>
                                
                                <p><strong>Forward Process (Training):</strong></p>
                                <ul>
                                    <li>Start with a real image</li>
                                    <li>Gradually add Gaussian noise over many steps (e.g., 1000 steps)</li>
                                    <li>End with pure random noise</li>
                                </ul>
                                
                                <p><strong>Reverse Process (Generation):</strong></p>
                                <ul>
                                    <li>Start with pure random noise</li>
                                    <li>Neural network learns to predict and remove the noise</li>
                                    <li>Iteratively denoise step by step</li>
                                    <li>End with a realistic image</li>
                                </ul>
                                
                                <p class="mb-0"><strong>The Magic:</strong> The network learns to reverse the noising process. Given noisy input at any step, it predicts what the slightly-less-noisy version should look like. Chain these predictions together, and pure noise becomes a coherent image.</p>
                            </div>
                        </div>

                        <h4>Popular Diffusion Models</h4>

                        <div class="table-responsive">
                            <table class="table table-bordered">
                                <thead class="table-dark">
                                    <tr>
                                        <th>Model</th>
                                        <th>Developer</th>
                                        <th>Key Feature</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>DDPM</strong></td>
                                        <td>Google (2020)</td>
                                        <td>Original denoising diffusion probabilistic model</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Stable Diffusion</strong></td>
                                        <td>Stability AI</td>
                                        <td>Open-source, runs on consumer GPUs, text-to-image</td>
                                    </tr>
                                    <tr>
                                        <td><strong>DALL·E 2/3</strong></td>
                                        <td>OpenAI</td>
                                        <td>Best-in-class text-to-image quality</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Midjourney</strong></td>
                                        <td>Midjourney</td>
                                        <td>Artistic style, Discord-based interface</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Imagen</strong></td>
                                        <td>Google</td>
                                        <td>State-of-the-art photorealism</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <h4>Why Diffusion Models Beat GANs</h4>

                        <div class="row">
                            <div class="col-md-6">
                                <div class="highlight-box" style="border-left-color: var(--color-teal);">
                                    <h5><i class="fas fa-check-circle me-2 text-success"></i>Diffusion Advantages</h5>
                                    <ul class="mb-0">
                                        <li><strong>Training Stability:</strong> No adversarial training, no mode collapse</li>
                                        <li><strong>Higher Quality:</strong> More detail, fewer artifacts</li>
                                        <li><strong>Better Diversity:</strong> Produces more varied outputs</li>
                                        <li><strong>Controllability:</strong> Easy to guide with text, images, or other conditions</li>
                                        <li><strong>Theoretical Grounding:</strong> Based on well-understood probabilistic principles</li>
                                    </ul>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="highlight-box" style="border-left-color: var(--color-crimson);">
                                    <h5><i class="fas fa-times-circle me-2 text-danger"></i>Diffusion Drawbacks</h5>
                                    <ul class="mb-0">
                                        <li><strong>Slow Generation:</strong> Requires many denoising steps (though getting faster)</li>
                                        <li><strong>Compute Intensive:</strong> High memory and GPU requirements</li>
                                        <li><strong>Large Models:</strong> Billions of parameters</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <h4>Diffusion Model Applications</h4>

                        <div class="card-tags mb-4">
                            <span class="bias-tag"><i class="fas fa-keyboard me-2"></i>Text-to-Image</span>
                            <span class="bias-tag"><i class="fas fa-eraser me-2"></i>Image Inpainting</span>
                            <span class="bias-tag"><i class="fas fa-video me-2"></i>Video Generation</span>
                            <span class="bias-tag"><i class="fas fa-stethoscope me-2"></i>Medical Image Synthesis</span>
                            <span class="bias-tag"><i class="fas fa-expand me-2"></i>Image Outpainting</span>
                            <span class="bias-tag"><i class="fas fa-sync me-2"></i>Image-to-Image Translation</span>
                        </div>

                        <div class="highlight-box">
                            <h4><i class="fas fa-rocket me-2"></i>The Future is Diffusion</h4>
                            <p>Diffusion models have become the foundation for the generative AI revolution. They're being extended to video (Sora, Runway), 3D objects, audio, and even protein structure prediction. If you're learning generative AI in 2026, <strong>start with diffusion models</strong>—they're the new standard.</p>
                            <p class="mb-0"><strong>Best Framework:</strong> Hugging Face Diffusers library provides easy access to Stable Diffusion, SDXL, and many other diffusion models with just a few lines of Python code.</p>
                        </div>

                        <!-- Section 5: Frameworks & Tools -->
                        <h2 id="frameworks"><i class="fas fa-toolbox me-2"></i>Frameworks & Tools: Your CV Toolkit</h2>

                        <p>Computer vision development requires the right tools. The good news: the ecosystem is mature, well-documented, and largely open-source. Here's a comprehensive breakdown of frameworks by programming language, helping you choose the right stack for your needs.</p>

                        <h3><i class="fab fa-python me-2"></i>Python — The Industry Standard ⭐⭐⭐⭐⭐</h3>

                        <p>Python dominates computer vision for good reason: the best libraries, the most tutorials, and the largest community. If you're starting in CV, <strong>start with Python</strong>.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-cubes me-2"></i>Core Libraries</h4>
                            <div class="card-content">
                                <div class="table-responsive">
                                    <table class="table table-sm">
                                        <tbody>
                                            <tr>
                                                <td width="25%"><strong>OpenCV</strong></td>
                                                <td>The Swiss Army knife of CV. Image I/O, transformations, filters, feature detection, video processing. Essential for any CV project.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>NumPy</strong></td>
                                                <td>Foundation for numerical computing. Images are NumPy arrays. Every CV library builds on NumPy.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>Pillow (PIL)</strong></td>
                                                <td>Simple image loading, saving, and basic operations. Great for preprocessing.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>scikit-image</strong></td>
                                                <td>Scientific image processing. Segmentation algorithms, morphology, feature extraction.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>Matplotlib</strong></td>
                                                <td>Visualization. Display images, plot results, create figures for papers.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-brain me-2"></i>Deep Learning Frameworks</h4>
                            <div class="card-content">
                                <div class="table-responsive">
                                    <table class="table table-sm">
                                        <tbody>
                                            <tr>
                                                <td width="25%"><strong>PyTorch</strong></td>
                                                <td>The researcher's choice. Dynamic graphs, intuitive debugging, dominant in academia. Powers YOLO, diffusion models, most new research.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>TensorFlow/Keras</strong></td>
                                                <td>Production-ready with TensorFlow Serving. Strong mobile support (TFLite). Keras provides simple high-level API.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>JAX</strong></td>
                                                <td>High-performance research. Automatic differentiation, XLA compilation. Used by Google for cutting-edge work.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-search me-2"></i>Detection & Segmentation Libraries</h4>
                            <div class="card-content">
                                <div class="table-responsive">
                                    <table class="table table-sm">
                                        <tbody>
                                            <tr>
                                                <td width="30%"><strong>Ultralytics (YOLO)</strong></td>
                                                <td>Official YOLOv8 implementation. Detection, segmentation, classification, pose estimation in one package.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>Detectron2</strong></td>
                                                <td>Meta's detection library. Faster R-CNN, Mask R-CNN, and more. Research-grade quality.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>MMDetection</strong></td>
                                                <td>OpenMMLab's comprehensive detection toolbox. 200+ models, highly modular.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>TorchVision</strong></td>
                                                <td>PyTorch's official vision library. Pretrained models, transforms, datasets.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>Segmentation Models PyTorch</strong></td>
                                                <td>U-Net, FPN, DeepLab, and 400+ encoder-decoder combinations for segmentation.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-magic me-2"></i>Generative Models</h4>
                            <div class="card-content">
                                <div class="table-responsive">
                                    <table class="table table-sm">
                                        <tbody>
                                            <tr>
                                                <td width="25%"><strong>Diffusers</strong></td>
                                                <td>Hugging Face's diffusion library. Stable Diffusion, SDXL, ControlNet, and dozens more. The go-to for generative AI.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>CLIP</strong></td>
                                                <td>OpenAI's vision-language model. Zero-shot classification, image-text similarity.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>StyleGAN</strong></td>
                                                <td>NVIDIA's face generation. Highest quality GAN for faces.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </div>
                        </div>

                        <h3><i class="fas fa-microchip me-2"></i>C++ — High Performance & Edge Deployment</h3>

                        <p>When Python is too slow or you're deploying to embedded systems, C++ is the answer. Most CV libraries have C++ bindings or are written in C++.</p>

                        <div class="row">
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4>Core Libraries</h4>
                                    <div class="card-content">
                                        <ul class="mb-0">
                                            <li><strong>OpenCV (C++ API)</strong> — Native, fastest performance</li>
                                            <li><strong>Darknet</strong> — Original YOLO implementation</li>
                                            <li><strong>dlib</strong> — Face detection, landmark detection</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="experiment-card">
                                    <h4>Inference Engines</h4>
                                    <div class="card-content">
                                        <ul class="mb-0">
                                            <li><strong>TensorRT</strong> — NVIDIA GPU optimization</li>
                                            <li><strong>ONNX Runtime</strong> — Cross-platform inference</li>
                                            <li><strong>OpenVINO</strong> — Intel CPU/GPU optimization</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <div class="card-tags mb-4">
                            <p><strong>C++ Use Cases:</strong></p>
                            <span class="bias-tag"><i class="fas fa-car me-2"></i>Autonomous Vehicles</span>
                            <span class="bias-tag"><i class="fas fa-helicopter me-2"></i>Drones</span>
                            <span class="bias-tag"><i class="fas fa-robot me-2"></i>Robotics</span>
                            <span class="bias-tag"><i class="fas fa-camera me-2"></i>Embedded Systems</span>
                            <span class="bias-tag"><i class="fas fa-tachometer-alt me-2"></i>Real-time Processing</span>
                        </div>

                        <h3><i class="fab fa-js me-2"></i>JavaScript — Web-Based Vision</h3>

                        <p>Run CV directly in the browser! Great for demos, web apps, and edge AI without server costs.</p>

                        <div class="experiment-card">
                            <h4>JavaScript Libraries</h4>
                            <div class="card-content">
                                <div class="table-responsive">
                                    <table class="table table-sm">
                                        <tbody>
                                            <tr>
                                                <td width="25%"><strong>TensorFlow.js</strong></td>
                                                <td>Full TensorFlow in the browser. Train and run models client-side. WebGL acceleration.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>ONNX.js</strong></td>
                                                <td>Run ONNX models in browser. Import PyTorch/TensorFlow models.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>OpenCV.js</strong></td>
                                                <td>OpenCV compiled to WebAssembly. Image processing in the browser.</td>
                                            </tr>
                                            <tr>
                                                <td><strong>MediaPipe</strong></td>
                                                <td>Google's ML solutions. Face detection, hand tracking, pose estimation.</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </div>
                        </div>

                        <div class="card-tags mb-4">
                            <p><strong>JavaScript Use Cases:</strong></p>
                            <span class="bias-tag"><i class="fas fa-globe me-2"></i>Browser Apps</span>
                            <span class="bias-tag"><i class="fas fa-filter me-2"></i>Real-time Filters</span>
                            <span class="bias-tag"><i class="fas fa-vr-cardboard me-2"></i>AR/VR Web</span>
                            <span class="bias-tag"><i class="fas fa-lock me-2"></i>Privacy-First AI</span>
                        </div>

                        <h3><i class="fas fa-cog me-2"></i>Other Tools & Frameworks</h3>

                        <div class="table-responsive">
                            <table class="table table-bordered">
                                <thead class="table-dark">
                                    <tr>
                                        <th>Tool</th>
                                        <th>Purpose</th>
                                        <th>Best For</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>MATLAB</strong></td>
                                        <td>Image Processing Toolbox</td>
                                        <td>Academic prototyping, algorithm development</td>
                                    </tr>
                                    <tr>
                                        <td><strong>R (imager, magick)</strong></td>
                                        <td>Statistical image analysis</td>
                                        <td>Research, visualization</td>
                                    </tr>
                                    <tr>
                                        <td><strong>ROS</strong></td>
                                        <td>Robot Operating System</td>
                                        <td>Robotics CV integration</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Halide</strong></td>
                                        <td>Image processing DSL</td>
                                        <td>High-performance pipelines</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Label Studio</strong></td>
                                        <td>Data annotation</td>
                                        <td>Creating training datasets</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Roboflow</strong></td>
                                        <td>CV dataset management</td>
                                        <td>Annotation, augmentation, hosting</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <!-- Section 6: Choosing the Right Approach -->
                        <h2 id="choosing"><i class="fas fa-compass me-2"></i>Choosing the Right Approach</h2>

                        <p>With so many models and frameworks available, how do you choose? Here's a decision framework based on your specific task and constraints.</p>

                        <h3>Task-Based Selection Guide</h3>

                        <div class="table-responsive">
                            <table class="table table-bordered table-striped">
                                <thead class="table-dark">
                                    <tr>
                                        <th>Task</th>
                                        <th>Best Models</th>
                                        <th>Recommended Framework</th>
                                        <th>Notes</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Real-time Object Detection</strong></td>
                                        <td>YOLOv8, YOLO-NAS</td>
                                        <td>Ultralytics + PyTorch</td>
                                        <td>30-150+ FPS possible</td>
                                    </tr>
                                    <tr>
                                        <td><strong>High-Accuracy Detection</strong></td>
                                        <td>Faster R-CNN, DINO</td>
                                        <td>Detectron2</td>
                                        <td>When accuracy > speed</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Medical Segmentation</strong></td>
                                        <td>U-Net, nnU-Net</td>
                                        <td>MONAI, Segmentation Models PyTorch</td>
                                        <td>Works with limited data</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Instance Segmentation</strong></td>
                                        <td>Mask R-CNN, YOLOv8-seg</td>
                                        <td>Detectron2, Ultralytics</td>
                                        <td>Individual object masks</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Image Generation</strong></td>
                                        <td>Stable Diffusion, SDXL</td>
                                        <td>Diffusers (Hugging Face)</td>
                                        <td>Text-to-image, editing</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Edge/Mobile Deployment</strong></td>
                                        <td>YOLOv8n, MobileNet</td>
                                        <td>TensorRT, CoreML, TFLite</td>
                                        <td>Optimized for devices</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Video Analysis</strong></td>
                                        <td>YOLO + tracking</td>
                                        <td>Ultralytics + ByteTrack</td>
                                        <td>Object tracking included</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <h3>Decision Flowchart</h3>

                        <div class="highlight-box">
                            <h4><i class="fas fa-sitemap me-2"></i>Choosing Your Approach</h4>
                            <p><strong>Step 1: What's your core task?</strong></p>
                            <ul>
                                <li>Classify entire images → Image Classification (ResNet, EfficientNet)</li>
                                <li>Find and locate objects → Object Detection (YOLO or Faster R-CNN)</li>
                                <li>Precise pixel boundaries → Segmentation (U-Net or Mask R-CNN)</li>
                                <li>Create new images → Generative (Diffusion models)</li>
                            </ul>
                            
                            <p><strong>Step 2: What are your constraints?</strong></p>
                            <ul>
                                <li>Need real-time (>30 FPS)? → YOLO family</li>
                                <li>Maximum accuracy required? → Two-stage detectors (R-CNN family)</li>
                                <li>Limited training data? → U-Net or transfer learning</li>
                                <li>Deploying to mobile/edge? → Lightweight models + TensorRT/CoreML</li>
                            </ul>
                            
                            <p class="mb-0"><strong>Step 3: Choose framework based on ecosystem</strong></p>
                            <ul class="mb-0">
                                <li>Research/experimentation → PyTorch</li>
                                <li>Production deployment → TensorFlow Serving or ONNX</li>
                                <li>Quick prototyping → Ultralytics or Hugging Face</li>
                            </ul>
                        </div>

                        <h3>Speed vs Accuracy Tradeoffs</h3>

                        <div class="experiment-card">
                            <h4><i class="fas fa-balance-scale-right me-2"></i>The Fundamental Tradeoff</h4>
                            <div class="card-content">
                                <p>In computer vision, there's almost always a tradeoff between speed and accuracy. Here's how different models compare:</p>
                                
                                <p><strong>Speed Champions (Real-time capable):</strong></p>
                                <ul>
                                    <li>YOLOv8n (nano): 150+ FPS, good accuracy</li>
                                    <li>YOLOv8s (small): 100+ FPS, better accuracy</li>
                                    <li>MobileNet: 60+ FPS, lightweight</li>
                                </ul>
                                
                                <p><strong>Accuracy Champions (Best quality):</strong></p>
                                <ul>
                                    <li>DINO/DINOv2: State-of-the-art representations</li>
                                    <li>Mask R-CNN + ResNeXt: Best instance segmentation</li>
                                    <li>Swin Transformer: Excellent across tasks</li>
                                </ul>
                                
                                <p class="mb-0"><strong>Balanced Options:</strong></p>
                                <ul class="mb-0">
                                    <li>YOLOv8m/l: 30-60 FPS with strong accuracy</li>
                                    <li>EfficientDet: Scalable speed-accuracy</li>
                                </ul>
                            </div>
                        </div>

                        <!-- Section 7: Learning Path & Next Steps -->
                        <h2 id="learning-path"><i class="fas fa-road me-2"></i>Learning Path & Next Steps</h2>

                        <p>Ready to master computer vision? Here's a structured learning path that builds skills progressively, from fundamentals to deployment.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-flag-checkered me-2"></i>Stage 1: Foundations (2-4 weeks)</h4>
                            <div class="card-meta">Build your base</div>
                            <div class="card-content">
                                <p><strong>Skills to Develop:</strong></p>
                                <ul>
                                    <li>Python proficiency (NumPy, Matplotlib)</li>
                                    <li>OpenCV basics: reading/writing images, color spaces, filters</li>
                                    <li>Image fundamentals: pixels, channels, transformations</li>
                                    <li>Basic image processing: blur, edge detection, thresholding</li>
                                </ul>
                                <p class="mb-0"><strong>Project Idea:</strong> Build an image filter app (blur, sharpen, edge detect)</p>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-brain me-2"></i>Stage 2: Deep Learning for CV (4-6 weeks)</h4>
                            <div class="card-meta">Neural network fundamentals</div>
                            <div class="card-content">
                                <p><strong>Skills to Develop:</strong></p>
                                <ul>
                                    <li>PyTorch or TensorFlow basics</li>
                                    <li>Convolutional Neural Networks (CNNs)</li>
                                    <li>Transfer learning with pretrained models</li>
                                    <li>Image classification (MNIST, CIFAR-10, custom dataset)</li>
                                    <li>Data augmentation techniques</li>
                                </ul>
                                <p class="mb-0"><strong>Project Idea:</strong> Train a classifier for your own image dataset (pets, plants, products)</p>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-search-location me-2"></i>Stage 3: Object Detection (3-4 weeks)</h4>
                            <div class="card-meta">Finding and localizing objects</div>
                            <div class="card-content">
                                <p><strong>Skills to Develop:</strong></p>
                                <ul>
                                    <li>YOLO architecture and training</li>
                                    <li>Dataset annotation (COCO format)</li>
                                    <li>Evaluation metrics (mAP, IoU)</li>
                                    <li>Fine-tuning on custom datasets</li>
                                </ul>
                                <p class="mb-0"><strong>Project Idea:</strong> Build a real-time object detector for a specific use case (safety equipment, wildlife, vehicles)</p>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-th me-2"></i>Stage 4: Image Segmentation (3-4 weeks)</h4>
                            <div class="card-meta">Pixel-perfect understanding</div>
                            <div class="card-content">
                                <p><strong>Skills to Develop:</strong></p>
                                <ul>
                                    <li>U-Net architecture for semantic segmentation</li>
                                    <li>Mask R-CNN for instance segmentation</li>
                                    <li>Segmentation datasets and mask annotation</li>
                                    <li>Loss functions: Dice, IoU, Focal</li>
                                </ul>
                                <p class="mb-0"><strong>Project Idea:</strong> Satellite image land cover segmentation or medical image analysis</p>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-magic me-2"></i>Stage 5: Generative Models (4-6 weeks)</h4>
                            <div class="card-meta">Creating and transforming images</div>
                            <div class="card-content">
                                <p><strong>Skills to Develop:</strong></p>
                                <ul>
                                    <li>GAN fundamentals and training dynamics</li>
                                    <li>Diffusion models: theory and practice</li>
                                    <li>Stable Diffusion fine-tuning (LoRA, DreamBooth)</li>
                                    <li>Image inpainting, outpainting, style transfer</li>
                                </ul>
                                <p class="mb-0"><strong>Project Idea:</strong> Train a custom Stable Diffusion model on a specific domain or style</p>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-rocket me-2"></i>Stage 6: Deployment (2-3 weeks)</h4>
                            <div class="card-meta">From prototype to production</div>
                            <div class="card-content">
                                <p><strong>Skills to Develop:</strong></p>
                                <ul>
                                    <li>Model optimization: quantization, pruning</li>
                                    <li>ONNX export and TensorRT optimization</li>
                                    <li>Edge deployment: Jetson, mobile devices</li>
                                    <li>API creation with FastAPI or Flask</li>
                                    <li>Docker containerization</li>
                                </ul>
                                <p class="mb-0"><strong>Project Idea:</strong> Deploy your detector as a web API or mobile app</p>
                            </div>
                        </div>

                        <div class="highlight-box">
                            <h4><i class="fas fa-graduation-cap me-2"></i>Recommended Learning Resources</h4>
                            <p><strong>Courses:</strong></p>
                            <ul>
                                <li>fast.ai Practical Deep Learning — Hands-on, project-based</li>
                                <li>Stanford CS231n — Theoretical foundations</li>
                                <li>PyTorch tutorials — Official documentation</li>
                            </ul>
                            <p><strong>Books:</strong></p>
                            <ul>
                                <li>"Deep Learning for Vision Systems" by Mohamed Elgendy</li>
                                <li>"Programming Computer Vision with Python" by Jan Erik Solem</li>
                            </ul>
                            <p class="mb-0"><strong>Practice:</strong></p>
                            <ul class="mb-0">
                                <li>Kaggle competitions (image classification, detection)</li>
                                <li>Hugging Face Spaces — Deploy and share models</li>
                                <li>Papers With Code — Implement latest research</li>
                            </ul>
                        </div>

                        <!-- Section 8: Best Practices & Common Pitfalls -->
                        <h2 id="best-practices"><i class="fas fa-check-double me-2"></i>Best Practices & Common Pitfalls</h2>

                        <p>Learning from others' mistakes is the fastest path to mastery. Here are the most important best practices and pitfalls to avoid in computer vision projects.</p>

                        <h3>Data Preparation Best Practices</h3>

                        <div class="row">
                            <div class="col-md-6">
                                <div class="highlight-box" style="border-left-color: var(--color-teal);">
                                    <h5><i class="fas fa-check me-2 text-success"></i>DO</h5>
                                    <ul class="mb-0">
                                        <li>Collect diverse, representative data</li>
                                        <li>Balance classes in your dataset</li>
                                        <li>Use consistent annotation guidelines</li>
                                        <li>Split data properly (train/val/test)</li>
                                        <li>Apply data augmentation</li>
                                        <li>Validate annotations for quality</li>
                                    </ul>
                                </div>
                            </div>
                            <div class="col-md-6">
                                <div class="highlight-box" style="border-left-color: var(--color-crimson);">
                                    <h5><i class="fas fa-times me-2 text-danger"></i>DON'T</h5>
                                    <ul class="mb-0">
                                        <li>Use biased or non-representative samples</li>
                                        <li>Ignore class imbalance</li>
                                        <li>Mix training and test data</li>
                                        <li>Skip data quality checks</li>
                                        <li>Over-augment (unrealistic transforms)</li>
                                        <li>Forget edge cases and rare scenarios</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <h3>Model Training Best Practices</h3>

                        <div class="experiment-card">
                            <h4><i class="fas fa-dumbbell me-2"></i>Training Tips</h4>
                            <div class="card-content">
                                <ul>
                                    <li><strong>Start with pretrained models:</strong> Transfer learning almost always outperforms training from scratch, especially with limited data.</li>
                                    <li><strong>Use appropriate learning rates:</strong> Start with 1e-3 to 1e-4 for fine-tuning. Use learning rate schedulers (cosine, step decay).</li>
                                    <li><strong>Monitor validation metrics:</strong> Watch for overfitting. Use early stopping if validation loss increases.</li>
                                    <li><strong>Experiment systematically:</strong> Change one thing at a time. Log all experiments (Weights & Biases, MLflow).</li>
                                    <li><strong>Use mixed precision training:</strong> FP16 training speeds up training 2x and reduces memory with minimal accuracy loss.</li>
                                </ul>
                            </div>
                        </div>

                        <h3>Common Pitfalls to Avoid</h3>

                        <div class="experiment-card">
                            <h4><i class="fas fa-exclamation-circle me-2 text-danger"></i>Pitfall 1: Data Leakage</h4>
                            <div class="card-content">
                                <p><strong>The Problem:</strong> Information from your test set leaks into training, leading to overly optimistic performance estimates.</p>
                                <p><strong>Common Causes:</strong></p>
                                <ul>
                                    <li>Splitting after augmentation (augmented versions in both sets)</li>
                                    <li>Using test set for hyperparameter tuning</li>
                                    <li>Time-series data not split chronologically</li>
                                </ul>
                                <p class="mb-0"><strong>Solution:</strong> Always split before any processing. Keep test set completely isolated until final evaluation.</p>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-exclamation-circle me-2 text-danger"></i>Pitfall 2: Ignoring Domain Shift</h4>
                            <div class="card-content">
                                <p><strong>The Problem:</strong> Model performs well on test data but fails in production because real-world data is different.</p>
                                <p><strong>Common Causes:</strong></p>
                                <ul>
                                    <li>Training on stock photos, deploying on user photos</li>
                                    <li>Different lighting conditions, cameras, or angles</li>
                                    <li>Seasonal or temporal changes in data</li>
                                </ul>
                                <p class="mb-0"><strong>Solution:</strong> Test on data as close to production as possible. Use domain adaptation techniques. Continuously monitor production performance.</p>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-exclamation-circle me-2 text-danger"></i>Pitfall 3: Wrong Evaluation Metrics</h4>
                            <div class="card-content">
                                <p><strong>The Problem:</strong> Optimizing for the wrong metric leads to models that don't solve the real problem.</p>
                                <p><strong>Examples:</strong></p>
                                <ul>
                                    <li>Using accuracy for imbalanced datasets (99% accuracy when 99% is one class)</li>
                                    <li>Ignoring false negatives in safety-critical applications</li>
                                    <li>Using IoU thresholds that don't match application needs</li>
                                </ul>
                                <p class="mb-0"><strong>Solution:</strong> Understand what matters for your application. For detection: mAP, precision, recall. For segmentation: IoU, Dice. Consider business metrics.</p>
                            </div>
                        </div>

                        <h3>Deployment Best Practices</h3>

                        <div class="highlight-box">
                            <h4><i class="fas fa-server me-2"></i>Production Checklist</h4>
                            <ul>
                                <li><strong>Optimize your model:</strong> Quantization (INT8), pruning, ONNX export, TensorRT compilation</li>
                                <li><strong>Benchmark thoroughly:</strong> Measure latency, throughput, and memory on target hardware</li>
                                <li><strong>Handle edge cases:</strong> What happens with bad input? Empty images? Unexpected formats?</li>
                                <li><strong>Monitor in production:</strong> Track inference time, error rates, confidence distributions</li>
                                <li><strong>Version your models:</strong> Track which model version is deployed. Enable rollback.</li>
                                <li><strong>Plan for retraining:</strong> Set up pipelines for continuous improvement with new data</li>
                            </ul>
                        </div>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>Final Words of Wisdom</h4>
                            <p><strong>Data > Model:</strong> A simple model on great data beats a complex model on poor data. Invest in data quality.</p>
                            <p><strong>Start Simple:</strong> Begin with pretrained models and established architectures. Only go custom when needed.</p>
                            <p><strong>Iterate Fast:</strong> Get a baseline working quickly, then improve. Perfect is the enemy of good.</p>
                            <p class="mb-0"><strong>Stay Current:</strong> The field moves fast. Follow Papers With Code, Hugging Face releases, and top conferences (CVPR, ICCV, NeurIPS).</p>
                        </div>

                        <!-- Related Posts -->
                        <div class="related-posts">
                            <h3><i class="fas fa-book me-2"></i>Related Articles in This Series</h3>
                            <div class="related-post-item">
                                <h5 class="mb-2">PyTorch Deep Learning: Complete Beginner's Guide to Building Neural Networks</h5>
                                <p class="text-muted small mb-2">Master PyTorch from scratch. Learn tensors, autograd, neural networks, CNNs, RNNs, transfer learning, and deployment with hands-on examples.</p>
                                <a href="pytorch-deep-learning-guide.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">TensorFlow Deep Learning: Complete Beginner's Guide to Building Neural Networks</h5>
                                <p class="text-muted small mb-2">Master TensorFlow 2 and Keras for deep learning. Learn neural network architectures, training workflows, and deployment techniques.</p>
                                <a href="tensorflow-deep-learning-guide.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 4: Machine Learning with Scikit-learn</h5>
                                <p class="text-muted small mb-2">Build and evaluate machine learning models using Scikit-learn. Learn classification, regression, clustering, and best practices for real-world projects.</p>
                                <a href="../12/python-data-science-machine-learning.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">
                        I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.
                    </p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook">
                            <i class="fab fa-facebook-f"></i>
                        </a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter">
                            <i class="fab fa-twitter"></i>
                        </a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn">
                            <i class="fab fa-linkedin-in"></i>
                        </a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube">
                            <i class="fab fa-youtube"></i>
                        </a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram">
                            <i class="fab fa-instagram"></i>
                        </a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest">
                            <i class="fab fa-pinterest-p"></i>
                        </a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email">
                            <i class="fas fa-envelope"></i>
                        </a>
                    </div>
                </div>
            </div>

            <hr class="bg-secondary">

            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small mb-2">
                        <i class="fas fa-camera me-2"></i>Background photo by Max Andrey from <a href="https://www.pexels.com/" target="_blank" class="text-light">Pexels</a>
                    </p>
                    <p class="small">
                        <i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a>
                    </p>
                    <p class="small mt-3">
                        <a href="/" class="text-light text-decoration-none">Home</a> | 
                        <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | 
                        <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a>
                    </p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">
                        Updated by <strong>Wasil Zafar</strong> | <time>January 2, 2026</time>
                    </p>
                </div>
            </div>
        </div>
    </footer>

    <!-- Scroll-to-Top Button -->
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top">
        <i class="fas fa-arrow-up"></i>
    </button>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    
    <!-- Cookie Consent JS -->
    <script src="../../../js/cookie-consent.js"></script>
    
    <!-- Main JS -->
    <script src="../../../js/main.js"></script>
    
    <!-- Prism.js for Syntax Highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <!-- Scroll-to-Top Script -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const scrollToTopBtn = document.getElementById('scrollToTop');
            
            // Show/hide button on scroll
            window.addEventListener('scroll', function() {
                if (window.scrollY > 300) {
                    scrollToTopBtn.classList.add('show');
                } else {
                    scrollToTopBtn.classList.remove('show');
                }
            });
            
            // Smooth scroll to top on click
            scrollToTopBtn.addEventListener('click', function() {
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        });
    </script>

    <!-- Prism Theme Switcher -->
    <script>
        // Available themes with display names
        const themes = {
            'prism-theme': 'Tomorrow Night',
            'prism-default': 'Default',
            'prism-dark': 'Dark',
            'prism-twilight': 'Twilight',
            'prism-okaidia': 'Okaidia',
            'prism-solarizedlight': 'Solarized Light'
        };

        // Load saved theme from localStorage or use default
        const savedTheme = localStorage.getItem('prism-theme') || 'prism-theme';

        // Function to switch theme
        function switchTheme(themeId) {
            // Disable all themes
            Object.keys(themes).forEach(id => {
                const link = document.getElementById(id);
                if (link) {
                    link.disabled = true;
                }
            });
            
            // Enable selected theme
            const selectedLink = document.getElementById(themeId);
            if (selectedLink) {
                selectedLink.disabled = false;
                localStorage.setItem('prism-theme', themeId);
            }

            // Update all dropdowns on the page to match selected theme
            document.querySelectorAll('div.code-toolbar select').forEach(dropdown => {
                dropdown.value = themeId;
            });

            // Re-apply syntax highlighting with new theme
            setTimeout(() => {
                Prism.highlightAll();
            }, 10);
        }

        // Apply saved theme on page load
        document.addEventListener('DOMContentLoaded', function() {
            switchTheme(savedTheme);
        });

        // Add theme switcher to Prism toolbar
        Prism.plugins.toolbar.registerButton('theme-switcher', function(env) {
            const select = document.createElement('select');
            select.setAttribute('aria-label', 'Select code theme');
            select.className = 'prism-theme-selector';
            
            // Populate dropdown with themes
            Object.keys(themes).forEach(themeId => {
                const option = document.createElement('option');
                option.value = themeId;
                option.textContent = themes[themeId];
                if (themeId === savedTheme) {
                    option.selected = true;
                }
                select.appendChild(option);
            });
            
            // Handle theme change
            select.addEventListener('change', function(e) {
                switchTheme(e.target.value);
            });
            
            return select;
        });
    </script>

    <!-- Side Navigation TOC Script -->
    <script>
        // Open side navigation
        function openNav() {
            document.getElementById('tocSidenav').classList.add('open');
            document.getElementById('tocOverlay').classList.add('show');
            document.body.style.overflow = 'hidden'; // Prevent background scroll
        }

        // Close side navigation
        function closeNav() {
            document.getElementById('tocSidenav').classList.remove('open');
            document.getElementById('tocOverlay').classList.remove('show');
            document.body.style.overflow = 'auto';
        }

        // Close on ESC key
        document.addEventListener('keydown', function(e) {
            if (e.key === 'Escape') {
                closeNav();
            }
        });

        // Highlight active section in TOC based on scroll position
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('[id]');
            const tocLinks = document.querySelectorAll('.sidenav-toc a');
            
            function highlightActiveSection() {
                let currentSection = '';
                
                sections.forEach(section => {
                    const sectionTop = section.offsetTop;
                    const sectionHeight = section.clientHeight;
                    
                    if (window.scrollY >= sectionTop - 200) {
                        currentSection = section.getAttribute('id');
                    }
                });
                
                tocLinks.forEach(link => {
                    link.classList.remove('active');
                    if (link.getAttribute('href') === '#' + currentSection) {
                        link.classList.add('active');
                    }
                });
            }
            
            // Highlight on scroll
            window.addEventListener('scroll', highlightActiveSection);
            
            // Initial highlight
            highlightActiveSection();
            
            // Smooth scroll for TOC links
            tocLinks.forEach(link => {
                link.addEventListener('click', function(e) {
                    e.preventDefault();
                    const targetId = this.getAttribute('href');
                    const targetSection = document.querySelector(targetId);
                    
                    if (targetSection) {
                        const offsetTop = targetSection.offsetTop - 80; // Account for fixed navbar
                        window.scrollTo({
                            top: offsetTop,
                            behavior: 'smooth'
                        });
                    }
                    
                    // Close nav after clicking
                    setTimeout(closeNav, 300);
                });
            });
        });
    </script>
</body>
</html>
