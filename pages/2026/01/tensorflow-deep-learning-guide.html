<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Master TensorFlow 2 from scratch. Learn tensors, Keras models, training workflows, CNNs, RNNs, transfer learning, and deployment. Complete beginner-friendly guide with executable examples." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="TensorFlow, Keras, Deep Learning, Neural Networks, Python, Machine Learning, CNN, RNN, LSTM, Transfer Learning, GPU, Autograd, Model Training, Data Science" />
    <meta property="og:title" content="TensorFlow 2 Deep Learning: Complete Beginner's Guide from Basics to Production" />
    <meta property="og:description" content="Learn TensorFlow fundamentals: tensors, Keras APIs, training workflows, CNNs, RNNs, transfer learning, and deployment. Beginner-friendly with hands-on examples." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-01" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>TensorFlow 2 Deep Learning: Complete Beginner's Guide from Basics to Production - Wasil Zafar</title>

    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Font Awesome Icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />

    <!-- Custom Styles -->
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />

    <!-- Prism.js Syntax Highlighting -->
    <!-- Multiple themes for dynamic switching -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />

    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <!-- Google Consent Mode v2 -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        
        gtag('consent', 'default', {
            'ad_storage': 'denied',
            'ad_user_data': 'denied',
            'ad_personalization': 'denied',
            'analytics_storage': 'denied',
            'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE']
        });
        
        gtag('consent', 'default', {
            'ad_storage': 'granted',
            'ad_user_data': 'granted',
            'ad_personalization': 'granted',
            'analytics_storage': 'granted'
        });
        
        gtag('set', 'url_passthrough', true);
    </script>

    <!-- Google Tag Manager -->
    <script>
        (function(w, d, s, l, i) {
            w[l] = w[l] || [];
            w[l].push({
                'gtm.start': new Date().getTime(),
                event: 'gtm.js'
            });
            var f = d.getElementsByTagName(s)[0],
                j = d.createElement(s),
                dl = l != 'dataLayer' ? '&l=' + l : '';
            j.async = true;
            j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    <style>
        /* Blog Post Specific Styles */
        .blog-hero {
            background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%);
            color: white;
            padding: 80px 0;
        }

        .blog-header {
            margin-bottom: 2rem;
        }

        .blog-meta {
            font-size: 0.95rem;
            color: var(--color-teal);
            margin-bottom: 1rem;
        }

        .blog-meta span {
            margin-right: 1.5rem;
        }

        .blog-content {
            max-width: 900px;
            margin: 0 auto;
            font-size: 1.05rem;
            line-height: 1.8;
            color: #333;
        }

        .blog-content h2 {
            font-size: 1.8rem;
            font-weight: 700;
            margin-top: 2.5rem;
            margin-bottom: 1.5rem;
            color: var(--color-navy);
            border-bottom: 3px solid var(--color-teal);
            padding-bottom: 0.5rem;
        }

        .blog-content h3 {
            font-size: 1.3rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: var(--color-blue);
        }

        .blog-content p {
            margin-bottom: 1.2rem;
            text-align: justify;
        }

        .blog-content strong {
            color: var(--color-crimson);
        }

        .highlight-box {
            background: rgba(59, 151, 151, 0.1);
            border-left: 4px solid var(--color-teal);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 4px;
        }

        .highlight-box h4 {
            color: var(--color-navy);
            font-weight: 600;
            margin-bottom: 1rem;
        }

        .toc-box {
            background: white;
            border: 2px solid var(--color-teal);
            padding: 2rem;
            margin: 2rem 0;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        .toc-box h3 {
            color: var(--color-navy);
            font-weight: 700;
            margin-bottom: 1.5rem;
            font-size: 1.5rem;
        }

        .toc-box ol {
            counter-reset: item;
            list-style-type: none;
            padding-left: 0;
        }

        .toc-box ol > li {
            counter-increment: item;
            margin-bottom: 0.8rem;
            padding-left: 2rem;
            position: relative;
        }

        .toc-box ol > li::before {
            content: counter(item) ".";
            position: absolute;
            left: 0;
            color: var(--color-teal);
            font-weight: 700;
        }

        .toc-box ol ol {
            margin-top: 0.5rem;
            margin-left: 1rem;
        }

        .toc-box ol ol > li {
            font-size: 0.95rem;
            margin-bottom: 0.5rem;
        }

        .toc-box a {
            color: var(--color-blue);
            text-decoration: none;
            transition: color 0.3s ease;
        }

        .toc-box a:hover {
            color: var(--color-crimson);
            text-decoration: underline;
        }

        .reading-time {
            display: inline-block;
            background: var(--color-crimson);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 4px;
            font-size: 0.9rem;
        }

        .back-link {
            display: inline-block;
            color: white;
            text-decoration: none;
            transition: all 0.3s ease;
            margin-bottom: 1rem;
            opacity: 0.9;
        }

        .back-link:hover {
            color: var(--color-teal);
            opacity: 1;
            transform: translateX(-5px);
        }

        .related-posts {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 2rem;
            margin-top: 3rem;
        }

        .related-posts h3 {
            color: var(--color-navy);
            margin-bottom: 1.5rem;
        }

        .related-post-item {
            padding: 1rem;
            border-left: 3px solid var(--color-teal);
            margin-bottom: 1rem;
            transition: all 0.3s ease;
        }

        .related-post-item:hover {
            background: white;
            border-left-color: var(--color-crimson);
        }

        .related-post-item a {
            color: var(--color-blue);
            text-decoration: none;
            font-weight: 600;
        }

        .related-post-item a:hover {
            color: var(--color-crimson);
        }

        /* Toolbar styling */
        div.code-toolbar > .toolbar {
            opacity: 1;
            display: flex;
            gap: 0.5rem;
        }

        div.code-toolbar > .toolbar > .toolbar-item > button {
            background: var(--color-teal);
            color: white;
            border: none;
            padding: 0.4rem 0.8rem;
            border-radius: 4px;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        div.code-toolbar > .toolbar > .toolbar-item > button:hover {
            background: var(--color-blue);
            transform: translateY(-1px);
        }

        div.code-toolbar > .toolbar > .toolbar-item > button:focus {
            outline: 2px solid var(--color-teal);
            outline-offset: 2px;
        }

        /* Theme switcher dropdown */
        div.code-toolbar > .toolbar > .toolbar-item > select {
            background: var(--color-navy);
            color: white;
            border: 1px solid var(--color-teal);
            padding: 0.4rem 0.8rem;
            border-radius: 4px;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.3s ease;
            outline: none;
        }

        div.code-toolbar > .toolbar > .toolbar-item > select:hover {
            background: var(--color-blue);
            border-color: var(--color-crimson);
        }

        div.code-toolbar > .toolbar > .toolbar-item > select:focus {
            outline: 2px solid var(--color-teal);
            outline-offset: 2px;
        }

        div.code-toolbar > .toolbar > .toolbar-item > select option {
            background: var(--color-navy);
            color: white;
        }

        pre[class*="language-"] {
            border-radius: 8px;
            margin: 1.5rem 0;
            padding-top: 3rem;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }

        /* Scroll-to-Top Button */
        .scroll-to-top {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            background: var(--color-teal);
            color: white;
            border: none;
            border-radius: 50%;
            font-size: 1.2rem;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            box-shadow: 0 4px 12px rgba(59, 151, 151, 0.3);
            z-index: 999;
        }

        .scroll-to-top.show {
            opacity: 1;
            visibility: visible;
        }

        .scroll-to-top:hover {
            background: var(--color-crimson);
            transform: translateY(-3px);
            box-shadow: 0 6px 16px rgba(191, 9, 47, 0.4);
        }

        .scroll-to-top:active {
            transform: translateY(-1px);
        }

        @media (max-width: 768px) {
            .scroll-to-top {
                bottom: 1rem;
                right: 1rem;
                width: 45px;
                height: 45px;
                font-size: 1rem;
            }
        }
    </style>
</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript>
        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe>
    </noscript>

    <!-- Cookie Consent Banner -->
    <div id="cookieBanner" class="light display-bottom" style="display: none;">
        <div id="closeIcon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
                <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"></path>
            </svg>
        </div>
        
        <div class="content-wrap">
            <div class="msg-wrap">
                <div class="title-wrap">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="20" height="20">
                        <path fill="#3B9797" d="M510.52 255.82c-69.97-.85-126.47-57.69-126.47-127.86-70.17 0-127-56.49-127.86-126.45-27.26-4.14-55.13.3-79.72 12.82l-69.13 35.22a132.221 132.221 0 0 0-57.79 57.81l-35.1 68.88a132.645 132.645 0 0 0-12.82 80.95l12.08 76.27a132.521 132.521 0 0 0 37.16 70.37l54.64 54.64a132.036 132.036 0 0 0 70.37 37.16l76.27 12.15c27.51 4.36 55.7-.11 80.95-12.8l68.88-35.08a132.166 132.166 0 0 0 57.79-57.81l35.1-68.88c12.56-24.64 17.01-52.58 12.91-79.91zM176 368c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32zm32-160c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32zm160 128c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32z"></path>
                    </svg>
                    <h4 style="margin: 0; font-size: 18px; color: var(--color-navy); font-weight: 700;">Cookie Consent</h4>
                </div>
                <p style="font-size: 14px; line-height: 1.6; color: var(--color-navy); margin-bottom: 15px;">
                    We use cookies to enhance your browsing experience, serve personalized content, and analyze our traffic. 
                    By clicking "Accept All", you consent to our use of cookies. See our 
                    <a href="/privacy-policy.html" style="color: var(--color-teal); border-bottom: 1px dotted var(--color-teal);">Privacy Policy</a> 
                    for more information.
                </p>
                
                <div id="cookieSettings" style="display: none;">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="14" height="14">
                        <path fill="currentColor" d="M487.4 315.7l-42.6-24.6c4.3-23.2 4.3-47 0-70.2l42.6-24.6c4.9-2.8 7.1-8.6 5.5-14-11.1-35.6-30-67.8-54.7-94.6-3.8-4.1-10-5.1-14.8-2.3L380.8 110c-17.9-15.4-38.5-27.3-60.8-35.1V25.8c0-5.6-3.9-10.5-9.4-11.7-36.7-8.2-74.3-7.8-109.2 0-5.5 1.2-9.4 6.1-9.4 11.7V75c-22.2 7.9-42.8 19.8-60.8 35.1L88.7 85.5c-4.9-2.8-11-1.9-14.8 2.3-24.7 26.7-43.6 58.9-54.7 94.6-1.7 5.4.6 11.2 5.5 14L67.3 221c-4.3 23.2-4.3 47 0 70.2l-42.6 24.6c-4.9 2.8-7.1 8.6-5.5 14 11.1 35.6 30 67.8 54.7 94.6 3.8 4.1 10 5.1 14.8 2.3l42.6-24.6c17.9 15.4 38.5 27.3 60.8 35.1v49.2c0 5.6 3.9 10.5 9.4 11.7 36.7 8.2 74.3 7.8 109.2 0 5.5-1.2 9.4-6.1 9.4-11.7v-49.2c22.2-7.9 42.8-19.8 60.8-35.1l42.6 24.6c4.9 2.8 11 1.9 14.8-2.3 24.7-26.7 43.6-58.9 54.7-94.6 1.5-5.5-.7-11.3-5.6-14.1zM256 336c-44.1 0-80-35.9-80-80s35.9-80 80-80 80 35.9 80 80-35.9 80-80 80z"></path>
                    </svg>
                    <span style="margin-left: 5px; font-size: 12px; font-weight: 600; color: var(--color-navy);">Customize Settings</span>
                </div>
                
                <div id="cookieTypes" style="display: none; margin-top: 15px; padding-top: 15px; border-top: 1px solid rgba(59, 151, 151, 0.2);">
                    <h5 style="font-size: 12px; font-weight: 700; color: var(--color-navy); margin-bottom: 10px; text-transform: uppercase;">Cookie Preferences</h5>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" checked disabled style="margin-top: 2px; margin-right: 8px; cursor: not-allowed;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Essential Cookies (Required)</strong>
                                <span style="font-size: 12px; color: #666;">Necessary for the website to function properly.</span>
                            </div>
                        </label>
                    </div>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" id="analyticsCookies" checked style="margin-top: 2px; margin-right: 8px;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Analytics Cookies</strong>
                                <span style="font-size: 12px; color: #666;">Help us understand how you interact with the website.</span>
                            </div>
                        </label>
                    </div>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" id="marketingCookies" style="margin-top: 2px; margin-right: 8px;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Marketing Cookies</strong>
                                <span style="font-size: 12px; color: #666;">Used to deliver relevant advertisements.</span>
                            </div>
                        </label>
                    </div>
                </div>
            </div>
            
            <div class="btn-wrap">
                <button id="cookieAccept" style="background: var(--color-teal); color: white; font-weight: 600;">Accept All</button>
                <button id="cookieReject" style="background: transparent; color: var(--color-navy); border: 2px solid var(--color-teal); font-weight: 600;">Reject All</button>
                <button id="cookieSave" style="background: var(--color-blue); color: white; font-weight: 600; display: none;">Save Preferences</button>
            </div>
        </div>
    </div>

    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/">
                <span class="gradient-text">Wasil Zafar</span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="/">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#about">About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#interests">Interests</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link active" href="/pages/categories/technology.html">Technology</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
            <a href="/pages/categories/technology.html" class="back-link">
                <i class="fas fa-arrow-left me-2"></i>Back to Technology
            </a>
                <h1 class="display-4 fw-bold mb-3">TensorFlow 2 Deep Learning: Complete Beginner's Guide from Basics to Production</h1>
                
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 1, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-1"></i>50 min read</span>
                </div>
                
                <p class="lead mb-0">Master neural networks with TensorFlow and Keras—from tensors to deployment. Learn the fundamentals of deep learning with hands-on, executable examples that run independently.</p>
            </div>
        </div>
    </section>

    <!-- Main Content -->
    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="blog-content">
                        
                        <!-- Table of Contents -->
                <div class="toc-box">
                    <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
                    <ol>
                        <li><a href="#introduction">Introduction: What is TensorFlow?</a>
                            <ol>
                                <li><a href="#what-is-tensorflow">What is TensorFlow and Why Use It?</a></li>
                                <li><a href="#where-tf-fits">Where TensorFlow Fits in Data Science & AI</a></li>
                                <li><a href="#installation">Installation & Setup Verification</a></li>
                            </ol>
                        </li>
                        <li><a href="#part1">Part 1: Foundations</a>
                            <ol>
                                <li><a href="#tensors-eager">Core Concepts: Tensors & Eager Execution</a></li>
                                <li><a href="#tensor-operations">Tensor Operations & Transformations</a></li>
                                <li><a href="#variables-autodiff">Variables & Automatic Differentiation</a></li>
                                <li><a href="#building-models">Building Your First Models</a></li>
                            </ol>
                        </li>
                        <li><a href="#part2">Part 2: Training & Optimization</a>
                            <ol>
                                <li><a href="#layers-custom">Layers & Custom Layers</a></li>
                                <li><a href="#activations-regularization">Activations, Regularization & Best Practices</a></li>
                                <li><a href="#loss-functions">Loss Functions & Custom Losses</a></li>
                                <li><a href="#optimizers">Optimizers & Learning Rate Schedules</a></li>
                                <li><a href="#data-pipelines">Data Pipelines with tf.data</a></li>
                            </ol>
                        </li>
                        <li><a href="#part3">Part 3: Training Workflows</a>
                            <ol>
                                <li><a href="#training-loops">Training: model.fit() vs Custom Loops</a></li>
                                <li><a href="#callbacks">Callbacks for Training Control</a></li>
                                <li><a href="#saving-loading">Model Persistence: Saving & Loading</a></li>
                                <li><a href="#tensorboard">TensorBoard Visualization</a></li>
                                <li><a href="#custom-metrics">Custom Metrics & Monitoring</a></li>
                            </ol>
                        </li>
                        <li><a href="#part4">Part 4: Practical Applications</a>
                            <ol>
                                <li><a href="#transfer-learning">Transfer Learning with Pretrained Models</a></li>
                                <li><a href="#computer-vision">Computer Vision: CNN for Image Classification</a></li>
                                <li><a href="#nlp-basics">Natural Language Processing Basics</a></li>
                                <li><a href="#time-series">Time Series Forecasting</a></li>
                            </ol>
                        </li>
                        <li><a href="#part5">Part 5: Advanced Topics</a>
                            <ol>
                                <li><a href="#attention-layers">Attention Layers & MultiHeadAttention</a></li>
                                <li><a href="#transformer-keras">Building Transformers with Keras</a></li>
                                <li><a href="#distributed-training">Distributed Training & Multi-GPU</a></li>
                                <li><a href="#performance">Performance Optimization</a></li>
                                <li><a href="#interpretability">Model Interpretability</a></li>
                                <li><a href="#deployment">Deployment & Serving</a></li>
                                <li><a href="#best-practices">Best Practices & Next Steps</a></li>
                            </ol>
                        </li>
                        <li><a href="#glossary">Key Terms Glossary</a></li>
                        <li><a href="#pitfalls">Common Pitfalls Reference</a></li>
                        <li><a href="#quick-reference">Quick Reference Cheat Sheet</a></li>
                    </ol>
                </div>

                <!-- Introduction -->
                <h2 id="introduction">Introduction: What is TensorFlow?</h2>

                <h3 id="what-is-tensorflow">What is TensorFlow and Why Use It?</h3>
                
                <p>TensorFlow is Google's open-source deep learning framework that has become the industry standard for building and deploying neural networks at scale. Originally released in 2015, TensorFlow 2 (launched in 2019) revolutionized the framework by making it more Pythonic and beginner-friendly through <strong>eager execution</strong>—meaning operations are evaluated immediately rather than requiring a separate session.</p>

                <p>Unlike traditional machine learning libraries that work well for tabular data and classical algorithms (like scikit-learn), TensorFlow excels at handling complex neural network architectures for tasks like image recognition, natural language processing, time series forecasting, and generative AI. Its tight integration with <strong>Keras</strong> (a high-level API) makes it accessible to beginners while still offering low-level control for researchers.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-lightbulb me-2"></i>Why Choose TensorFlow?</h4>
                    <ul>
                        <li><strong>Production-Ready:</strong> Seamless deployment with TensorFlow Serving, TensorFlow Lite (mobile), and TensorFlow.js (web)</li>
                        <li><strong>Scalability:</strong> Built-in support for distributed training across multiple GPUs and TPUs</li>
                        <li><strong>Ecosystem:</strong> TensorBoard for visualization, TensorFlow Hub for pretrained models, TensorFlow Datasets for ready-to-use data</li>
                        <li><strong>Industry Adoption:</strong> Used by Google, Airbnb, Twitter, Intel, and thousands of companies worldwide</li>
                        <li><strong>Keras Integration:</strong> High-level API that's beginner-friendly yet powerful enough for complex architectures</li>
                    </ul>
                </div>

                <h3 id="where-tf-fits">Where TensorFlow Fits in Data Science & AI</h3>

                <p>Understanding where TensorFlow fits in the broader data science ecosystem is crucial for beginners. Think of it as a hierarchy:</p>

                <p><strong>1. Data Exploration & Cleaning:</strong> Use pandas and NumPy to load, clean, and explore your data. TensorFlow doesn't replace these—it builds on them.</p>

                <p><strong>2. Classical Machine Learning:</strong> For structured/tabular data, scikit-learn often performs better with algorithms like Random Forests, Gradient Boosting, and SVMs. Use TensorFlow when you need deep learning.</p>

                <p><strong>3. Deep Learning:</strong> This is TensorFlow's domain. When you have large datasets, complex patterns (images, text, sequences), or need neural networks, TensorFlow shines. It handles automatic differentiation, GPU acceleration, and model optimization.</p>

                <p><strong>4. Generative AI:</strong> Large language models (LLMs) and transformers can be built with TensorFlow, though PyTorch has gained popularity here. TensorFlow's ecosystem (TF-Agents for reinforcement learning, TensorFlow Probability for Bayesian methods) extends its capabilities.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-brain me-2"></i>Key TensorFlow Concepts for Beginners</h4>
                    <ul>
                        <li><strong>Tensors:</strong> Multi-dimensional arrays (like NumPy arrays) that flow through your neural network</li>
                        <li><strong>Keras Models:</strong> High-level abstractions for building networks (Sequential, Functional, Subclassing)</li>
                        <li><strong>Training Loop:</strong> <code>model.compile()</code> + <code>model.fit()</code> handles optimization automatically</li>
                        <li><strong>Data Pipelines:</strong> <code>tf.data</code> efficiently loads and preprocesses data (batch, shuffle, prefetch)</li>
                        <li><strong>Callbacks:</strong> Monitor and improve training (EarlyStopping, ModelCheckpoint, TensorBoard)</li>
                        <li><strong>Deployment:</strong> SavedModel format for serving models in production environments</li>
                    </ul>
                </div>

                <h3 id="installation">Installation & Setup Verification</h3>

                <p>Before diving in, ensure TensorFlow is installed. If you haven't installed it yet, run this command in your terminal:</p>

<pre><code class="language-bash">pip install tensorflow tensorflow-datasets tensorboard</code></pre>

                <p>This installs TensorFlow 2, TensorFlow Datasets (curated datasets), and TensorBoard (visualization toolkit). Now let's verify the installation and check available hardware:</p>

<pre><code class="language-python">import tensorflow as tf
import numpy as np

# Verify TensorFlow version
print('TensorFlow version:', tf.__version__)

# Check eager execution (should be True by default in TF 2)
print('Eager execution:', tf.executing_eagerly())

# Check available GPUs
print('Available GPUs:', len(tf.config.list_physical_devices('GPU')))

# List all physical devices
print('All devices:', tf.config.list_physical_devices())
</code></pre>

                <p>Expected output shows TensorFlow 2.x, eager execution enabled, and lists available devices (CPU, GPU if present). If GPUs are detected, TensorFlow will automatically use them for operations—no manual configuration needed for most cases.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-exclamation-triangle me-2"></i>GPU Setup Notes</h4>
                    <p>If you have an NVIDIA GPU, ensure CUDA and cuDNN are installed for GPU acceleration. For Apple Silicon (M1/M2), TensorFlow uses Metal Performance Shaders automatically. Cloud platforms like Google Colab provide free GPU/TPU access—perfect for learning without local hardware requirements.</p>
                </div>

                <!-- Part 1: Foundations -->
                <h2 id="part1">Part 1: Foundations</h2>

                <h3 id="tensors-eager">Core Concepts: Tensors & Eager Execution</h3>

                <p>At the heart of TensorFlow are <strong>tensors</strong>—multi-dimensional arrays similar to NumPy's ndarrays but with superpowers: automatic differentiation, GPU acceleration, and seamless integration with neural network operations. Think of a tensor as a generalization of matrices: scalars are 0D, vectors are 1D, matrices are 2D, and higher-dimensional arrays are tensors.</p>

                <div class="highlight-box">
                    <i class="fas fa-lightbulb"></i>
                    <strong>Eager Execution:</strong> Unlike TensorFlow 1.x which required building a static computational graph before running anything, TensorFlow 2 uses <strong>eager execution</strong> by default. This means operations execute immediately like normal Python code. You can print values, use debuggers, and write intuitive code without complex graph syntax.
                </div>

                <p>Let's create basic tensors and understand their fundamental properties:</p>

<pre><code class="language-python">import tensorflow as tf

# METHOD 1: Create a 1D tensor (vector) from a list
# tf.constant() creates immutable tensor from Python data
# Parameters:
#   - value: the data (list, NumPy array, etc.)
#   - dtype: data type (tf.int32, tf.float32, tf.float64, etc.)
a = tf.constant([1, 2, 3], dtype=tf.int32)
print('1D tensor a:', a)
# Output: tf.Tensor([1 2 3], shape=(3,), dtype=int32)

# Inspect tensor properties
print('Shape:', a.shape)        # (3,) - one dimension with 3 elements
print('Rank:', tf.rank(a).numpy())  # 1 - one dimension means rank=1

# METHOD 2: Create a 2D tensor (matrix) from nested list
b = tf.constant([[1.0, 2.0], [3.0, 4.0]])
print('\n2D tensor b:\n', b)
# Output: 2x2 matrix with 4 elements total

# Inspect 2D tensor properties
print('Shape:', b.shape)        # (2, 2) - 2 rows, 2 columns
print('Rank:', tf.rank(b).numpy())  # 2 - two dimensions means rank=2
print('Data type:', b.dtype)    # <dtype: 'float32'>

# METHOD 3: Create tensors with specific values
zeros = tf.zeros((3, 4))    # 3x4 matrix of zeros
ones = tf.ones((2, 3))      # 2x3 matrix of ones
print('\nZeros shape:', zeros.shape)
print('Ones shape:', ones.shape)</code></pre>

                <p><strong>Key Tensor Properties:</strong></p>
                <ul>
                    <li><code>shape</code>: Dimensions of the tensor (e.g., (3, 4) means 3 rows, 4 columns)</li>
                    <li><code>rank</code>: Number of dimensions (0=scalar, 1=vector, 2=matrix, 3+=higher-order)</li>
                    <li><code>dtype</code>: Data type (int32, float32, float64, etc.). float32 is standard; float64 for high precision</li>
                    <li><code>device</code>: Where tensor lives (CPU or GPU). Must match model device to avoid errors</li>
                </ul>

                <h4>Converting Between NumPy and TensorFlow</h4>

                <p>Since NumPy is the foundation of Python data science, TensorFlow offers seamless conversion. This is essential for loading data with NumPy or exporting results:</p>

<pre><code class="language-python">import tensorflow as tf
import numpy as np

# METHOD 1: NumPy array to TensorFlow tensor
# tf.convert_to_tensor() wraps NumPy data as TensorFlow tensor
np_array = np.array([[10, 20], [30, 40]], dtype=np.float32)
tensor = tf.convert_to_tensor(np_array)
print('Converted to tensor:', tensor)
print('Type:', type(tensor))  # <class 'tensorflow.python.framework.ops.EagerTensor'>

# METHOD 2: TensorFlow tensor to NumPy array
# .numpy() extracts underlying NumPy array from tensor
back_to_numpy = tensor.numpy()
print('Back to NumPy:', back_to_numpy)
print('Type:', type(back_to_numpy))  # <class 'numpy.ndarray'>

# Use case: After inference, export predictions as NumPy for visualization
predictions = tensor  # Some model output
predictions_np = predictions.numpy()  # Convert to NumPy for matplotlib
import matplotlib.pyplot as plt
# plt.plot(predictions_np)  # Now can plot with matplotlib</code></pre>

                <p><code>.numpy()</code> is essential for exporting TensorFlow results to visualization libraries (matplotlib, seaborn) or data tools (pandas). However, note that extracting to NumPy loses GPU benefits.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-exclamation-triangle me-2"></i>Type Conversions</h4>
                    <p>TensorFlow and NumPy dtypes don't always align perfectly. Common issue: np.array([1,2,3]) defaults to int64, but TensorFlow defaults to int32. Always explicitly set dtype to avoid surprises: <code>tf.convert_to_tensor(data, dtype=tf.float32)</code></p>
                </div>

                <h3 id="tensor-operations">Tensor Operations & Transformations</h3>

                <p>TensorFlow provides hundreds of operations mirroring NumPy for familiarity. Common transformations include reshaping, transposing, slicing, casting, and concatenating tensors. These are the building blocks for preprocessing data:</p>

<pre><code class="language-python">import tensorflow as tf

# Create a range of values (like NumPy arange)
# tf.range(start, limit, delta)
# Parameters:
#   - start: starting value (inclusive)
#   - limit: ending value (exclusive, not included)
#   - delta: step size
x = tf.range(12)  # [0, 1, 2, ..., 11]
print('Original tensor:', x)

# RESHAPE: Change dimensions without changing data
# tf.reshape(tensor, shape)
# Important: product of new shape must equal total elements
# Example: 12 elements → (3, 4) or (2, 6) or (12,) all valid
x_reshaped = tf.reshape(x, (3, 4))  # Reshape to 3 rows, 4 columns
print('Reshaped to 3x4:\n', x_reshaped)
# Output: [[0, 1, 2, 3],
#          [4, 5, 6, 7],
#          [8, 9, 10, 11]]

# TRANSPOSE: Swap dimensions (flip rows and columns for 2D)
# tf.transpose(tensor) - 2D becomes (cols, rows)
x_transposed = tf.transpose(x_reshaped)  # Was (3, 4), now (4, 3)
print('Transposed (4x3):\n', x_transposed)

# CAST: Change data type
# tf.cast(tensor, dtype)
# Use case: Convert int to float before division or neural network
x_float = tf.cast(x, tf.float32)  # Convert integers to floats
print('Casted to float32:', x_float)</code></pre>

                <div class="experiment-card">
                    <div class="card-meta mb-2">
                        <span class="badge bg-teal text-white">Common Tensor Operations</span>
                    </div>
                    <div class="card-content">
                        <ul>
                            <li><strong>tf.reshape():</strong> Change shape without changing data. Useful for flattening or batching</li>
                            <li><strong>tf.transpose():</strong> Swap dimensions. Essential for matrix multiplication alignment</li>
                            <li><strong>tf.cast():</strong> Convert data type. Common: int→float before neural nets</li>
                            <li><strong>tf.concat():</strong> Join multiple tensors along existing axis</li>
                            <li><strong>tf.stack():</strong> Join tensors along new axis. Creates extra dimension</li>
                        </ul>
                    </div>
                </div>                <h4>Type Casting and Slicing</h4>

                <p>Convert between data types with <code>tf.cast()</code> and extract subsets using Python-style indexing:</p>

<pre><code class="language-python">import tensorflow as tf

# Create integer tensor
x = tf.constant([[1, 2, 3], [4, 5, 6]])

# Cast to float32
x_float = tf.cast(x, tf.float32)
print('Cast to float32:', x_float.dtype)

# Slicing: extract row 1
print('Row 1:', x[1])

# Slicing: extract column 2
print('Column 2:', x[:, 2])

# Slicing: extract submatrix (rows 0-1, cols 1-2)
print('Submatrix:\n', x[0:2, 1:3])
</code></pre>

                <h4>Concatenation and Stacking</h4>

                <p>Combine tensors along existing or new dimensions:</p>

<pre><code class="language-python">import tensorflow as tf

# Create two tensors
a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 6], [7, 8]])

# Concatenate along rows (axis=0)
concat_rows = tf.concat([a, b], axis=0)
print('Concatenated rows (4x2):\n', concat_rows)

# Concatenate along columns (axis=1)
concat_cols = tf.concat([a, b], axis=1)
print('Concatenated columns (2x4):\n', concat_cols)

# Stack creates a new dimension
stacked = tf.stack([a, b], axis=0)
print('Stacked (2x2x2):\n', stacked)
</code></pre>

                <p>Use <code>tf.concat()</code> to merge along existing axes and <code>tf.stack()</code> to create a new dimension—critical for batching data in neural networks.</p>

                <h3 id="variables-autodiff">Variables & Automatic Differentiation</h3>

                <p><strong>tf.Variable</strong> represents mutable tensors used for model parameters (weights, biases). Unlike immutable <code>tf.constant</code>, variables can be updated during training. TensorFlow's <strong>GradientTape</strong> records operations to compute gradients automatically—the backbone of backpropagation and neural network training.</p>

                <div class="highlight-box">
                    <i class="fas fa-lightbulb"></i>
                    <strong>Variables vs Constants:</strong> <code>tf.constant</code> is immutable (can't change), used for fixed data. <code>tf.Variable</code> is mutable (can change), used for learnable parameters. During training, you update Variables using gradients from GradientTape.
                </div>

<pre><code class="language-python">import tensorflow as tf

# Create VARIABLES for model parameters
# Variables are mutable—they will be updated during training
# Parameters:
#   - initial_value: starting values (tensor or initializer)
#   - name: optional name for debugging
#   - trainable: whether to include in optimizer updates (default: True)
W = tf.Variable(tf.random.normal([3, 3]), name='weights')
# Shape [3, 3] means: 3 input features, 3 output features

b = tf.Variable(tf.zeros([3]), name='bias')
# Shape [3] means: 3 bias terms (one per output)

print('Weight shape:', W.shape)  # torch.Size([3, 3])
print('Bias shape:', b.shape)    # torch.Size([3])
print('Is trainable:', W.trainable)  # True (can be updated)
print('Variable dtype:', W.dtype)    # <dtype: 'float32'>

# Create CONSTANTS for fixed values
x_constant = tf.constant([[1.0, 2.0, 3.0]])
print('\nConstant x:', x_constant)
# Constants are immutable—can't update them</code></pre>

                <h4>Automatic Differentiation with GradientTape</h4>

                <p>GradientTape is TensorFlow's mechanism for automatic differentiation (computing derivatives). It records all operations inside its context, then traces backwards to compute how changes in variables affect the loss:</p>

<pre><code class="language-python">import tensorflow as tf

# Create variables (model parameters)
W = tf.Variable(tf.random.normal([3, 3]), name='W')
b = tf.Variable(tf.zeros([3]), name='b')

# Create sample input data (batch of 5 samples, 3 features each)
x = tf.random.normal([5, 3])

# GradientTape records all operations inside 'with' block
# Think of it as "recording a video" of computations
with tf.GradientTape() as tape:
    # Forward pass: compute predictions
    # y = x @ W + b (matrix multiplication then add bias)
    y = tf.matmul(x, W) + b  # Output shape: [5, 3]
    
    # Compute loss (mean squared error)
    # MSE = mean((predictions - 0)^2)
    # We're using zero as target (just for this example)
    loss = tf.reduce_mean(tf.square(y))
    # loss is a scalar (single number)

# Compute gradients
# tape.gradient(output, variables) computes ∂output/∂variables
# This is THE key operation: tells us how to adjust W and b to reduce loss
grads = tape.gradient(loss, [W, b])

# Results: grads[0] = ∂loss/∂W, grads[1] = ∂loss/∂b
print('Loss:', loss.numpy())  # Single scalar value
print('Gradient W shape:', grads[0].shape)  # [3, 3] - same as W
print('Gradient b shape:', grads[1].shape)  # [3] - same as b

# These gradients tell us:
# - Which direction to move W to reduce loss
# - Which direction to move b to reduce loss
# An optimizer will use these to update the variables</code></pre>

                <h4>Manual Gradient Descent Step</h4>

                <p>While optimizers automate this, let's see the basic principle: update weights using gradients and a learning rate:</p>

<pre><code class="language-python">import tensorflow as tf

# Variables (our model parameters)
W = tf.Variable([[1.0, 2.0], [3.0, 4.0]], name='W')
b = tf.Variable([0.5, 0.5], name='b')

# Input and target data
x = tf.constant([[1.0, 2.0]])        # 1 sample, 2 features
target = tf.constant([[5.0, 6.0]])   # 1 sample, 2 outputs

# Hyperparameter: controls how big a step we take
# Too small: training is slow
# Too large: weights oscillate and don't converge
learning_rate = 0.01

# STEP 1: Forward pass and compute loss
with tf.GradientTape() as tape:
    # Predict: pred = x @ W + b
    prediction = tf.matmul(x, W) + b  # Output shape: [1, 2]
    
    # Loss: MSE = mean((prediction - target)^2)
    # Measures how far prediction is from target
    loss = tf.reduce_mean(tf.square(target - prediction))

# STEP 2: Compute gradients
# ∂loss/∂W tells us how W affects the loss
# ∂loss/∂b tells us how b affects the loss
grads = tape.gradient(loss, [W, b])

print('Initial loss:', loss.numpy())

# STEP 3: Update variables (the "learning" happens here)
# Gradient descent: new_value = old_value - learning_rate * gradient
# assign_sub() means "subtract and assign": W -= learning_rate * grad_W
W.assign_sub(learning_rate * grads[0])  # W = W - lr * ∂loss/∂W
b.assign_sub(learning_rate * grads[1])  # b = b - lr * ∂loss/∂b

print('Updated W:\n', W.numpy())
print('Updated b:', b.numpy())
print('\nAfter one gradient step, loss should be smaller')</code></pre>

                <div class="experiment-card">
                    <div class="card-meta mb-2">
                        <span class="badge bg-teal text-white">GradientTape Concepts</span>
                    </div>
                    <div class="card-content">
                        <ul>
                            <li><strong>tf.Variable:</strong> Mutable tensor for model parameters. Updated during training</li>
                            <li><strong>tf.constant:</strong> Immutable tensor for fixed data. Cannot be updated</li>
                            <li><strong>GradientTape context:</strong> Records operations for gradient computation. Everything inside 'with' block is tracked</li>
                            <li><strong>tape.gradient():</strong> Computes partial derivatives. Returns same shape as variables</li>
                            <li><strong>Learning rate:</strong> Controls step size. Critical hyperparameter—wrong value = poor training</li>
                            <li><strong>.assign_sub():</strong> In-place subtraction. W.assign_sub(grad) ≡ W -= grad</li>
                        </ul>
                    </div>
                </div>                <p>The <code>assign_sub()</code> method updates variables in-place—this is the core of training algorithms like SGD, Adam, etc.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-code me-2"></i>GradientTape Best Practices</h4>
                    <ul>
                        <li>Watch only trainable variables (automatically tracked)</li>
                        <li>Tape is consumed after <code>gradient()</code> call—use <code>persistent=True</code> for multiple gradient calls</li>
                        <li>Non-differentiable ops (like argmax) stop gradient flow—handle carefully</li>
                        <li>Always call <code>gradient()</code> inside the tape context to avoid memory leaks</li>
                    </ul>
                </div>

                <h3 id="building-models">Building Your First Models</h3>

                <p>Keras, now fully integrated into TensorFlow, offers three APIs for building models: <strong>Sequential</strong> (simple stacks), <strong>Functional</strong> (complex graphs), and <strong>Subclassing</strong> (full control). Start with Sequential for learning, graduate to Functional for real projects, and use Subclassing for research.</p>

                <div class="highlight-box">
                    <i class="fas fa-info-circle"></i>
                    <strong>Model Building Conceptually:</strong> A neural network is a series of mathematical transformations. Each layer applies: <code>output = activation(weights @ input + bias)</code>. Keras layers automate weight creation and initialization. Your job is to stack them in the right order.
                </div>

                <h4>Sequential API: Linear Stack of Layers</h4>

                <p>Perfect for feedforward networks where data flows linearly through layers:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Build a simple neural network
# keras.Sequential = a linear stack of layers where output of one feeds into next
model = keras.Sequential([
    # Dense layer: fully connected layer
    # Parameters:
    #   - units: number of neurons (output dimensionality)
    #   - activation: activation function (relu, sigmoid, tanh, etc.)
    #   - input_shape: input dimensionality (only on first layer)
    # Computes: output = relu(input @ W + b) where W is [16, 32]
    layers.Dense(32, activation='relu', input_shape=(16,)),
    
    # Second hidden layer: input shape auto-inferred from previous layer output
    # Input shape is [32] (from previous Dense output)
    # Computes: output = relu(input @ W + b) where W is [32, 16]
    layers.Dense(16, activation='relu'),
    
    # Output layer: no activation for regression (raw prediction)
    # For classification: use activation='softmax' for probabilities
    layers.Dense(1)
])

# Display the model architecture
# Shows layer types, output shapes, and parameter counts
model.summary()
</code></pre>

                <h4>Understanding Dense Layers</h4>

                <p>A <code>Dense</code> layer is a fully connected layer where every input connects to every output. It performs: <code>output = activation(input @ weights + bias)</code>.</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers

# Create a Dense layer
# Units = 32 means: output will have 32 values (32 neurons)
layer = layers.Dense(
    units=32,                  # Output dimension
    activation='relu',         # Apply ReLU: max(0, x)
    input_shape=(16,)         # Input dimension (16 features)
)

# What happens internally:
# - weights shape: [16, 32]  (each of 16 inputs connects to 32 outputs)
# - bias shape: [32]          (one bias per output neuron)
# - computation: output = relu(input @ weights + bias)
#
# Example:
# input shape: [batch_size=5, features=16]
# output shape: [batch_size=5, units=32]

# Test it
sample_input = tf.random.normal([5, 16])  # 5 samples, 16 features each
output = layer(sample_input)
print('Input shape:', sample_input.shape)   # [5, 16]
print('Output shape:', output.shape)        # [5, 32]
print('Number of parameters:', layer.count_params())  # (16 * 32) + 32 = 544
</code></pre>

                <h4>Compiling the Model</h4>

                <p>Compilation configures the optimizer (how to update weights), loss function (what to minimize), and metrics (what to track). Think of it as "getting the model ready to learn":</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Build model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dense(10, activation='softmax')  # 10-class classification
])

# COMPILE: Configure the training process
# Parameters explained:
#   - optimizer: algorithm to update weights
#     * 'adam': Adaptive learning rate (best for beginners, works for most problems)
#     * Can also pass custom: keras.optimizers.Adam(learning_rate=0.001)
#   - loss: what function to minimize during training
#     * 'sparse_categorical_crossentropy': for integer labels [0, 1, 2, ..., 9]
#     * 'categorical_crossentropy': for one-hot labels [[1,0,0,...], [0,1,0,...], ...]
#     * 'mse': mean squared error for regression
#     * 'binary_crossentropy': for binary classification (2 classes)
#   - metrics: quantities to monitor during training (not used for optimization)
#     * 'accuracy': fraction of correct predictions
#     * Can include multiple: metrics=['accuracy', 'precision', 'recall']
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print('Model compiled. Architecture defined. Ready for training.')

# TRAINING FLOW:
# 1. Load training data (X_train, y_train)
# 2. model.fit(X_train, y_train, epochs=10, batch_size=32)
# 3. For each epoch:
#    - For each batch:
#      * Forward pass: predictions = model(batch_X)
#      * Compute loss: loss_val = loss_function(predictions, batch_y)
#      * Backward pass: compute gradients via GradientTape
#      * Update: weights -= optimizer(learning_rate * gradients)
#    - Print metrics
# 4. Done! Weights learned from data
</code></pre>

                <h4>Choosing Optimizer, Loss, and Metrics</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, optimizers

# OPTIMIZER CHOICES:
# For most problems: 'adam' (Adaptive Moment Estimation)
# - Maintains per-parameter learning rates
# - Convergence is usually fast and stable
model = keras.Sequential([layers.Dense(10, activation='softmax')])

# Option 1: String shortcut (simple)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Option 2: Instance with custom learning rate (more control)
model.compile(
    optimizer=optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy'
)

# LOSS FUNCTION CHOICES:
# Classification with integer labels (e.g., [0, 1, 2, 3]):
loss_classification = 'sparse_categorical_crossentropy'

# Classification with one-hot labels (e.g., [[1,0,0], [0,1,0]]):
loss_onehot = 'categorical_crossentropy'

# Binary classification (2 classes only):
loss_binary = 'binary_crossentropy'

# Regression (continuous values):
loss_regression = 'mse'  # Mean squared error
loss_regression_alt = 'mae'  # Mean absolute error

# METRICS: purely for monitoring (don't affect training)
# Common metrics:
metrics_classification = ['accuracy']  # Fraction correct
metrics_detailed = ['accuracy', 'precision', 'recall']  # More detail
metrics_regression = ['mae', 'mse']  # Error measures
</code></pre>

                <h4>Functional API: Complex Architectures</h4>

                <p>Build models with multiple inputs/outputs, skip connections, or branching paths:</p>

                <h4>Functional API: Complex Architectures</h4>

                <p>Build models with multiple inputs/outputs, skip connections, or branching paths. In Functional API, layers are functions that transform tensors:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# FUNCTIONAL API: Treat layers as functions on tensors
# Useful for: multiple inputs, skip connections, multi-output, branching

# Step 1: Define input tensor
# keras.Input(shape=(16,)) creates an abstract input (no data yet, just shape info)
inputs = keras.Input(shape=(16,))  # input_shape = [batch_size, 16]

# Step 2: Apply layers as transformations (functional style)
# Each layer call: tensor_out = layer(tensor_in)
x = layers.Dense(
    units=32,           # output dimension
    activation='relu'   # activation function
)(inputs)              # Apply to inputs
# x shape: [batch_size, 32]

# Dropout: regularization to prevent overfitting
# parameters:
#   - rate (0.2): probability of dropping each neuron
#   - During training: randomly set 20% of inputs to 0
#   - During inference: all neurons active, but scaled to compensate
x = layers.Dropout(rate=0.2)(x)
# x shape still: [batch_size, 32] (no dimension change, just regularization)

# Another dense layer
x = layers.Dense(16, activation='relu')(x)
# x shape: [batch_size, 16]

# Output layer (no activation for regression)
outputs = layers.Dense(1)(x)
# outputs shape: [batch_size, 1]

# Step 3: Create model by specifying inputs and outputs
# keras.Model is a container that represents: inputs → computations → outputs
model = keras.Model(inputs=inputs, outputs=outputs, name='functional_model')

# View architecture
model.summary()
</code></pre>

                <h4>Functional API Example: Multi-Input Model</h4>

                <p>Process different types of data separately, then combine:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Multi-input example: combine images and metadata for better predictions
# E.g., real estate price prediction: image of house + metadata (size, rooms, etc.)

# INPUT 1: Image-like features (e.g., 28x28 image flattened to 784)
image_inputs = keras.Input(shape=(784,), name='image_input')
image_branch = layers.Dense(128, activation='relu')(image_inputs)
image_branch = layers.Dense(64, activation='relu')(image_branch)

# INPUT 2: Metadata (e.g., house size, rooms, age)
meta_inputs = keras.Input(shape=(5,), name='metadata_input')
meta_branch = layers.Dense(32, activation='relu')(meta_inputs)

# COMBINE: concatenate branches
combined = layers.Concatenate()([image_branch, meta_branch])
# combined shape: [batch_size, 64 + 32] = [batch_size, 96]

# Final layers after combining
output = layers.Dense(32, activation='relu')(combined)
output = layers.Dense(1)(output)

# Create model with MULTIPLE INPUTS and ONE OUTPUT
model = keras.Model(inputs=[image_inputs, meta_inputs], outputs=output)
model.summary()

# TRAINING: pass list of input arrays
# model.fit([X_image, X_metadata], y_price, epochs=10)
</code></pre>

                <h4>Model Subclassing: Full Python Control</h4>

                <p>For research or custom training loops, subclass <code>keras.Model</code> and define <code>call()</code>:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# MODEL SUBCLASSING: Full Python control
# Inherit from keras.Model and define call() method
class CustomModel(keras.Model):
    def __init__(self):
        # Call parent constructor
        super().__init__()
        
        # Define layers as instance variables (in __init__)
        # They will be automatically tracked for:
        # - Weight updates during training
        # - Model summary display
        # - Saving/loading
        self.dense1 = layers.Dense(32, activation='relu')
        self.dropout = layers.Dropout(0.2)
        self.dense2 = layers.Dense(1)
    
    def call(self, inputs, training=False):
        # Forward pass logic (called when model(data) is invoked)
        # Parameters:
        #   - inputs: input tensor [batch_size, features]
        #   - training: boolean flag (True during training, False during inference)
        #     * Used by layers like Dropout and BatchNormalization
        #     * Dropout: active during training (randomly drop neurons)
        #     * Dropout: inactive during inference (use all neurons)
        
        # Forward pass with explicit training flag control
        x = self.dense1(inputs)                    # [batch_size, 32]
        x = self.dropout(x, training=training)    # Conditional dropout
        output = self.dense2(x)                    # [batch_size, 1]
        return output

# Create model instance
model = CustomModel()

# Test forward pass
sample_input = tf.random.normal([4, 16])      # 4 samples, 16 features
output_train = model(sample_input, training=True)   # Training mode (dropout active)
output_infer = model(sample_input, training=False)  # Inference mode (no dropout)

print('Input shape:', sample_input.shape)      # [4, 16]
print('Output shape:', output_train.shape)     # [4, 1]
print('Training mode uses different dropout than inference mode')
</code></pre>

                <p>The <code>training</code> flag controls behavior during training vs inference (e.g., dropout on/off, batch normalization statistics). Subclassing offers maximum flexibility but requires more boilerplate.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-check-circle me-2"></i>When to Use Each API</h4>
                    <ul>
                        <li><strong>Sequential:</strong> Quick prototyping, simple feedforward networks (90% of beginner use cases)</li>
                        <li><strong>Functional:</strong> Production models, transfer learning, multi-input/output, skip connections (recommended for real projects)</li>
                        <li><strong>Subclassing:</strong> Research, custom training loops, dynamic architectures (advanced users only)</li>
                    </ul>
                </div>

                <!-- Part 2: Training & Optimization -->
                <h2 id="part2">Part 2: Training & Optimization</h2>

                <h3 id="layers-custom">Layers & Custom Layers</h3>

                <p>Keras provides dozens of built-in layers for common tasks: <code>Dense</code> (fully connected), <code>Conv2D</code> (2D convolution), <code>LSTM</code> (recurrent), <code>Dropout</code> (regularization), <code>BatchNormalization</code> (normalize activations), and more. When built-in layers aren't sufficient, create custom layers by subclassing <code>layers.Layer</code>.</p>

                <h4>Creating a Custom Layer</h4>

                <p>Custom layers define their own weights and forward pass logic:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers

class ScaledDense(layers.Layer):
    def __init__(self, units, scale=1.0):
        super().__init__()
        self.units = units
        self.scale = scale
    
    def build(self, input_shape):
        # Create weights lazily (called on first forward pass)
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer='glorot_uniform',
            trainable=True,
            name='kernel'
        )
        self.b = self.add_weight(
            shape=(self.units,),
            initializer='zeros',
            trainable=True,
            name='bias'
        )
    
    def call(self, inputs):
        # Forward pass: scale * (input @ w + b)
        return self.scale * (tf.matmul(inputs, self.w) + self.b)

# Test the custom layer
layer = ScaledDense(units=8, scale=0.5)
output = layer(tf.random.normal([2, 16]))
print('Custom layer output shape:', output.shape)
</code></pre>

                <p>The <code>build()</code> method creates weights based on input shape (lazy initialization). <code>add_weight()</code> registers parameters for automatic gradient tracking. <code>call()</code> defines the forward pass transformation.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-cog me-2"></i>Weight Initialization Strategies</h4>
                    <ul>
                        <li><strong>glorot_uniform (Xavier):</strong> Good default for sigmoid/tanh activations; maintains variance across layers</li>
                        <li><strong>he_normal:</strong> Best for ReLU activations; accounts for ReLU's non-linearity</li>
                        <li><strong>zeros/ones:</strong> Typically used for biases; avoid for weights (breaks symmetry)</li>
                        <li><strong>random_normal:</strong> General purpose; specify mean and standard deviation</li>
                    </ul>
                </div>

                <h3 id="activations-regularization">Activations, Regularization & Best Practices</h3>

                <p>Activation functions introduce non-linearity, enabling neural networks to learn complex patterns. Regularization techniques prevent overfitting by constraining model complexity.</p>

                <h4>Common Activation Functions</h4>

                <p>Activation functions decide what the neuron "fires" at. Without them, stacking layers just does linear transformations (mathematically equivalent to one layer). Activations are what enable deep learning:</p>

<pre><code class="language-python">import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Visualize common activation functions
x = np.linspace(-5, 5, 100)

# ReLU: max(0, x)
# - Returns 0 for negative inputs, x for positive
# - Default for hidden layers (fast, works well)
# - Problem: "dead neurons" if all inputs are negative
relu_out = np.maximum(0, x)

# Sigmoid: 1 / (1 + e^(-x))
# - Outputs probability [0, 1]
# - Used for binary classification output layer
# - Saturates (slopes → 0) for extreme values, slowing training
sigmoid_out = 1 / (1 + np.exp(-x))

# Tanh: (e^x - e^-x) / (e^x + e^-x)
# - Outputs [-1, 1], zero-centered
# - Better than sigmoid for hidden layers (but ReLU usually better)
tanh_out = np.tanh(x)

# Leaky ReLU: max(0.1*x, x)
# - Like ReLU but allows small negative slope (0.1x for x < 0)
# - Prevents "dead neurons" problem
leaky_relu_out = np.where(x > 0, x, 0.1 * x)

print('Activation functions:')
print('ReLU range:', relu_out.min(), 'to', relu_out.max())      # 0 to 5
print('Sigmoid range:', sigmoid_out.min(), 'to', sigmoid_out.max())  # ~0 to 1
print('Tanh range:', tanh_out.min(), 'to', tanh_out.max())      # -1 to 1
print('Leaky ReLU range:', leaky_relu_out.min(), 'to', leaky_relu_out.max())  # -0.5 to 5
</code></pre>

                <h4>Using Activations in a Model</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers

# Build model demonstrating different activations
model = tf.keras.Sequential([
    # Hidden layers: use ReLU (fast, works well)
    # ReLU: rectified linear unit
    # Computation: output = max(0, input @ weights + bias)
    layers.Dense(64, activation='relu', input_shape=(20,)),
    
    # Alternative: Leaky ReLU (prevents dead neurons)
    # Computation: output = max(0.2 * input, input) @ weights + bias
    layers.Dense(64, activation=tf.nn.leaky_relu),
    
    # Alternative: Tanh (zero-centered, works for normalized data)
    # Range: [-1, 1], good when data is centered around 0
    layers.Dense(32, activation='tanh'),
    
    # Output layer for binary classification: Sigmoid
    # Sigmoid: squashes to [0, 1] probability range
    # Computation: output = 1 / (1 + e^(-x))
    layers.Dense(1, activation='sigmoid')
])

model.summary()

# KEY RULE:
# Hidden layers: almost always ReLU (or LeakyReLU)
# Output layer depends on task:
#   - Binary classification: sigmoid
#   - Multi-class classification: softmax
#   - Regression: no activation (linear)
</code></pre>

                <h4>Softmax for Multi-class Classification</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers
import numpy as np

# SOFTMAX: Converts class scores to probabilities (sum to 1)
# Formula: softmax(x_i) = e^(x_i) / sum(e^(x_j))
# Output: probability distribution over all classes

# Example: 3 classes (dog, cat, bird)
logits = np.array([2.0, 1.0, 0.1])  # Raw scores from Dense layer
# logits[0] = 2.0 (most confident about dog)
# logits[1] = 1.0 (moderate confidence about cat)
# logits[2] = 0.1 (low confidence about bird)

# Apply softmax (manually)
exp_logits = np.exp(logits)
softmax_probs = exp_logits / exp_logits.sum()
print('Logits:', logits)
print('Softmax probs:', softmax_probs)  # [0.659, 0.242, 0.099] (sums to 1)

# Or use TensorFlow softmax
tf_softmax = tf.nn.softmax(logits).numpy()
print('TensorFlow softmax:', tf_softmax)  # Same result

# In a model:
model = tf.keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dense(10, activation='softmax')  # 10 classes, outputs probabilities
])

# For training, use sparse_categorical_crossentropy with integer labels
# For inference, argmax(predictions) gives class with highest probability
</code></pre>

                <h4>Regularization: Dropout and L1/L2</h4>

                <p>Regularization prevents overfitting (memorizing training data instead of learning patterns). It constrains model complexity by penalizing large weights or randomly deactivating neurons:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Model with regularization techniques
model = keras.Sequential([
    # LAYER 1: L2 Regularization (Weight Penalty)
    layers.Dense(
        units=64,                              # output size
        activation='relu',
        # L2 regularization: adds lambda * sum(weights^2) to loss
        # Effect: larger lambda → smaller weights → simpler model
        # This prevents weights from growing too large (overfitting)
        kernel_regularizer=keras.regularizers.l2(0.0001),
        input_shape=(20,)
    ),
    
    # DROPOUT: Regularization by random neuron deactivation
    # During training: randomly drop (set to 0) 30% of neurons
    # During inference: use all neurons (no dropout)
    # Effect: Forces network to learn redundant representations
    #         Prevents co-adaptation of neurons
    layers.Dropout(rate=0.3),
    
    # LAYER 2: L1 Regularization (Sparsity)
    layers.Dense(
        units=32,
        activation='relu',
        # L1 regularization: adds lambda * sum(|weights|) to loss
        # Effect: drives some weights to exactly 0 (feature selection)
        # Result: sparse weights, simpler model
        kernel_regularizer=keras.regularizers.l1(0.00001),
    ),
    
    # Another dropout layer
    layers.Dropout(rate=0.2),
    
    # Output layer: 10-class classification with softmax
    layers.Dense(10, activation='softmax')
])

model.summary()

# REGULARIZATION COMPARISON:
# L2 (Ridge):  Shrinks weights toward zero, all non-zero
#              Good for: general-purpose regularization
# L1 (Lasso):  Drives some weights to exactly zero
#              Good for: feature selection, sparse models
# Dropout:     Randomly deactivates neurons
#              Good for: preventing co-adaptation, very effective
</code></pre>

                <h4>Understanding Dropout in Detail</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers
import numpy as np

# DROPOUT: Randomly zero activations with probability p
# E.g., Dropout(0.3) means: 30% chance each neuron is dropped

# Simulate dropout manually
activations = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
print('Original activations:', activations)

# Training phase: randomly drop with probability p=0.3
dropout_rate = 0.3
mask = np.random.binomial(1, 1 - dropout_rate, size=activations.shape)
dropped = activations * mask / (1 - dropout_rate)  # Scale to maintain expectation
print('After dropout (training):', dropped)

# Inference phase: keep all activations (no dropout)
print('After dropout (inference):', activations)

# In TensorFlow:
layer = layers.Dropout(0.3)
test_input = tf.constant([[1.0, 2.0, 3.0, 4.0, 5.0]])

# During training
output_train = layer(test_input, training=True)
print('\nTensorFlow dropout (training):', output_train.numpy())

# During inference
output_infer = layer(test_input, training=False)
print('TensorFlow dropout (inference):', output_infer.numpy())
# Same input, same output during inference
</code></pre>

                <div class="highlight-box">
                    <h4><i class="fas fa-bolt me-2"></i>Activation Function Quick Reference</h4>
                    <ul>
                        <li><strong>ReLU:</strong> Fast default for many networks; watch for dead neurons (outputs always 0)</li>
                        <li><strong>Leaky ReLU:</strong> Fixes dead neurons by allowing small negative slope</li>
                        <li><strong>Sigmoid/Tanh:</strong> Use in gates (LSTMs) or bounded outputs; can saturate (vanishing gradients)</li>
                        <li><strong>Softmax:</strong> Converts logits to probabilities for multi-class classification (always in output layer)</li>
                    </ul>
                </div>

                <h3 id="loss-functions">Loss Functions & Custom Losses</h3>

                <p>Loss functions measure how wrong the model's predictions are. The optimizer minimizes this loss by updating weights. Choosing the correct loss function is critical—it directly affects what the model learns:</p>

                <h4>Common Built-in Losses</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import numpy as np

# LOSS FUNCTION SELECTION GUIDE:
# Each task requires a different loss function

# ===== BINARY CLASSIFICATION (2 classes) =====
# Example: Spam detection (Spam vs Not Spam)
binary_model = keras.Sequential([
    keras.layers.Dense(16, activation='relu', input_shape=(10,)),
    keras.layers.Dense(1, activation='sigmoid')  # sigmoid → [0, 1]
])
binary_model.compile(
    optimizer='adam',
    loss='binary_crossentropy',  # Use for 2-class problems
    metrics=['accuracy']
)

# ===== MULTI-CLASS CLASSIFICATION (3+ classes) =====
# Example: Image classification (10 digit classes 0-9)
# Two variants based on label format:

# VARIANT 1: Integer labels [0, 1, 2, ..., 9]
multiclass_integer = keras.Sequential([
    keras.layers.Dense(32, activation='relu', input_shape=(20,)),
    keras.layers.Dense(10, activation='softmax')  # 10 output neurons
])
multiclass_integer.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',  # Use for integer labels
    metrics=['accuracy']
)

# VARIANT 2: One-hot encoded labels [[1,0,0,...], [0,1,0,...], ...]
multiclass_onehot = keras.Sequential([
    keras.layers.Dense(32, activation='relu', input_shape=(20,)),
    keras.layers.Dense(10, activation='softmax')
])
multiclass_onehot.compile(
    optimizer='adam',
    loss='categorical_crossentropy',  # Use for one-hot labels
    metrics=['accuracy']
)

# ===== REGRESSION (continuous values) =====
# Example: House price prediction (output: $100k-$500k)
regression_model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(15,)),
    keras.layers.Dense(1)  # No activation: can output any value
])
regression_model.compile(
    optimizer='adam',
    loss='mse',  # Mean Squared Error: emphasis on large errors
    metrics=['mae']  # Monitor MAE: Mean Absolute Error
)

print('Models compiled with appropriate loss functions.')

# LOSS FUNCTION COMPARISON:
print('\nLoss functions explained:')
print('binary_crossentropy: For binary classification')
print('sparse_categorical_crossentropy: Multi-class, integer labels')
print('categorical_crossentropy: Multi-class, one-hot labels')
print('mse: Regression, penalizes large errors heavily')
print('mae: Regression, treats all errors equally')
</code></pre>

                <h4>Understanding Loss Functions Mathematically</h4>

<pre><code class="language-python">import tensorflow as tf
import numpy as np

# BINARY CROSS-ENTROPY: -[y*log(p) + (1-y)*log(1-p)]
# Where: y = true label (0 or 1), p = predicted probability [0, 1]

y_true = np.array([1, 0, 1])        # Ground truth labels
y_pred = np.array([0.9, 0.2, 0.8]) # Predicted probabilities

# Compute manually
loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
print('Binary cross-entropy (per sample):', loss)
print('Average loss:', loss.mean())

# TensorFlow version
tf_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
print('TensorFlow binary_crossentropy:', tf_loss.numpy())

# CATEGORICAL CROSS-ENTROPY: -sum(y_true * log(y_pred))
# Where: y_true = one-hot [1,0,0], y_pred = softmax probabilities

y_true_onehot = np.array([[1, 0, 0], [0, 1, 0]])  # True labels
y_pred_softmax = np.array([[0.7, 0.2, 0.1], [0.1, 0.8, 0.1]])  # Predictions

# Loss = -log(correct_class_prob)
loss_class1 = -np.log(y_pred_softmax[0, 0])  # Sample 1: -log(0.7)
loss_class2 = -np.log(y_pred_softmax[1, 1])  # Sample 2: -log(0.8)
print('\nCategorical cross-entropy:', [loss_class1, loss_class2])

# Mean Squared Error (MSE) for regression:
y_true_regression = np.array([100, 200, 150])  # House prices
y_pred_regression = np.array([105, 195, 160])  # Predictions

mse = ((y_true_regression - y_pred_regression) ** 2).mean()
print('\nMSE (regression):', mse)
</code></pre>

                <h4>Creating a Custom Loss Function</h4>

                <p>Implement custom loss as a function accepting <code>y_true</code> and <code>y_pred</code>:</p>

<pre><code class="language-python">import tensorflow as tf

def custom_mse(y_true, y_pred):
    """Custom mean squared error with optional weighting."""
    squared_diff = tf.square(y_true - y_pred)
    return tf.reduce_mean(squared_diff)

# Test the custom loss
y_true = tf.constant([[1.0], [2.0], [3.0]])
y_pred = tf.constant([[1.5], [1.8], [3.2]])

loss_value = custom_mse(y_true, y_pred)
print('Custom MSE:', loss_value.numpy())

# Use in model compilation
model = tf.keras.Sequential([
    tf.keras.layers.Dense(16, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(1)
])
model.compile(optimizer='adam', loss=custom_mse)
</code></pre>

                <p>Custom losses are useful for domain-specific objectives like weighted errors, focal loss for imbalanced data, or contrastive loss for metric learning.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-chart-line me-2"></i>Loss Function Selection Guide</h4>
                    <ul>
                        <li><strong>Regression:</strong> MSE (penalizes large errors more), MAE (robust to outliers), Huber (combines both)</li>
                        <li><strong>Binary Classification:</strong> binary_crossentropy (use with sigmoid output)</li>
                        <li><strong>Multi-class Classification:</strong> sparse_categorical_crossentropy (integer labels) or categorical_crossentropy (one-hot)</li>
                        <li><strong>from_logits=True:</strong> Use when output layer has no activation (numerically stable)</li>
                    </ul>
                </div>

                <h3 id="optimizers">Optimizers & Learning Rate Schedules</h3>

                <p>Optimizers update weights based on gradients to minimize loss. Different optimizers have different strategies: Adam adapts learning rates per parameter, SGD uses a fixed rate with momentum, AdamW decouples weight decay from gradients. Choosing the right optimizer and learning rate affects convergence speed and final accuracy:</p>

                <h4>Common Optimizers and Their Strategies</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

# ===== ADAM: Default, Adaptive Learning Rate =====
# Combines: momentum (moving average of gradients)
#          + RMSprop (per-parameter learning rates)
# Parameters:
#   - learning_rate: initial step size (typical: 0.001)
#   - beta_1: momentum decay (default 0.9)
#   - beta_2: RMSprop decay (default 0.999)
adam_optimizer = keras.optimizers.Adam(
    learning_rate=0.001,  # Initial step size
    beta_1=0.9,           # Momentum: smooth out oscillations
    beta_2=0.999          # RMSprop: adapt per-parameter
)

# ===== SGD: Stochastic Gradient Descent with Momentum =====
# Simpler than Adam, sometimes more stable
# Parameters:
#   - learning_rate: step size (typically larger than Adam, ~0.01)
#   - momentum: fraction of previous gradient to keep (default 0.0)
#   - nesterov: use Nesterov momentum (look-ahead) (default False)
sgd_optimizer = keras.optimizers.SGD(
    learning_rate=0.01,   # Step size for SGD
    momentum=0.9,         # Include 90% of previous gradient
    nesterov=True         # Look-ahead version
)

# ===== ADAMW: Adam with Decoupled Weight Decay =====
# Better than Adam for modern architectures (Transformers, Vision Transformers)
# Parameters:
#   - weight_decay: L2 regularization strength
adamw_optimizer = keras.optimizers.AdamW(
    learning_rate=0.001,
    weight_decay=0.01     # Regularization: penalties large weights
)

# ===== RMSprop: Root Mean Square Propagation =====
# Good for recurrent neural networks (RNNs, LSTMs)
# Adapts learning rate based on magnitude of recent gradients
rmsprop_optimizer = keras.optimizers.RMSprop(
    learning_rate=0.001,
    rho=0.9               # Decay rate for moving average
)

print('Optimizers created.')

# QUICK DECISION GUIDE:
print('\nOptimizer selection:')
print('- Default/Safe choice: Adam (learning_rate=0.001)')
print('- Fine-tuning (transfer learning): Adam with lower learning_rate')
print('- Modern architectures: AdamW with weight decay')
print('- RNNs/LSTMs: RMSprop')
print('- When Adam fails: Try SGD with momentum')
</code></pre>

                <h4>Learning Rate Schedules</h4>

                <p>Learning rate often needs adjustment during training—start high for rapid progress, decrease later for fine-tuning. Learning rate schedules automate this:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

# ===== EXPONENTIAL DECAY =====
# Multiply learning rate by a factor every N steps
# Formula: lr(t) = lr_0 * decay_rate ^ (t / decay_steps)
# Effect: gradual decrease in learning rate

initial_learning_rate = 0.1
decay_steps = 1000          # Decay every 1000 steps
decay_rate = 0.96          # Multiply by 0.96 each time
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate,
    decay_steps,
    decay_rate
)
optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)

# ===== STEP DECAY =====
# Multiply learning rate at specific steps
# Example: halve learning rate at epochs [10, 20, 30]

# Using callback (see Callbacks section for more)
from tensorflow.keras.callbacks import ReduceLROnPlateau
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',     # Metric to monitor
    factor=0.5,             # Multiply LR by 0.5
    patience=5,             # Wait 5 epochs of no improvement
    min_lr=0.00001          # Lower bound
)

# ===== POLYNOMIAL DECAY =====
# Learning rate decreases polynomially: lr = lr_0 * (1 - t/total)^p

total_steps = 10000
power = 1.0  # Linear decay (power=1); quadratic (power=2)
lr_schedule_poly = keras.optimizers.schedules.PolynomialDecay(
    0.1,           # Initial LR
    total_steps,
    end_learning_rate=0.00001,
    power=power
)
optimizer = keras.optimizers.Adam(learning_rate=lr_schedule_poly)

print('Learning rate schedules created.')

# RULE OF THUMB:
# - Start with Adam(learning_rate=0.001)
# - If loss plateaus: decrease LR or use schedule
# - If loss oscillates: decrease LR
# - If training too slow: increase LR (carefully!)
</code></pre>

                <h4>Learning Rate Schedules</h4>

                <p>Decay learning rate over time for better convergence:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

# Exponential decay: lr = initial_lr * decay_rate^(step/decay_steps)
lr_schedule = keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.01,
    decay_steps=1000,
    decay_rate=0.96,
    staircase=True  # Discretize decay steps
)

# Create optimizer with schedule
optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)

# Check learning rate at different steps
print('LR at step 0:', lr_schedule(0).numpy())
print('LR at step 1000:', lr_schedule(1000).numpy())
print('LR at step 2000:', lr_schedule(2000).numpy())
</code></pre>

                <p>Other schedules: <code>PiecewiseConstantDecay</code> (step-wise), <code>PolynomialDecay</code>, <code>CosineDecay</code> (warm restarts). Use <code>ReduceLROnPlateau</code> callback for adaptive decay based on validation metrics.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-balance-scale me-2"></i>Optimizer Quick Reference</h4>
                    <ul>
                        <li><strong>Adam:</strong> Good default; adaptive learning rates; works well for most tasks</li>
                        <li><strong>SGD + Momentum:</strong> Stable and simple; requires proper learning rate schedule; use for very large datasets</li>
                        <li><strong>AdamW:</strong> Decoupled weight decay; helpful for Transformers and large models</li>
                        <li><strong>RMSprop:</strong> Good for recurrent networks (LSTMs, GRUs)</li>
                    </ul>
                </div>

                <h3 id="data-pipelines">Data Pipelines with tf.data</h3>

                <p>The <code>tf.data</code> API builds efficient input pipelines that overlap data loading with model training (prefetching), shuffle data for better generalization, and batch samples for GPU efficiency. Always use <code>tf.data</code> for datasets that don't fit in memory or require complex preprocessing.</p>

                <h4>Basic Pipeline: Batch and Prefetch</h4>

<pre><code class="language-python">import tensorflow as tf
import numpy as np

# CREATE DATASET from in-memory tensor
# tf.data.Dataset is the standard way to feed data to keras.fit()
data = tf.random.uniform([1000, 16])  # 1000 samples, 16 features

# Convert to Dataset object
dataset = tf.data.Dataset.from_tensor_slices(data)

# BATCH: Group samples into mini-batches
# Parameters:
#   - batch_size: samples per gradient update (32, 64, 128 typical)
#   - Larger batch → faster computation but less frequent updates
#   - Smaller batch → slower but noisier gradients (can be good for regularization)
dataset = dataset.batch(32)

# PREFETCH: Load next batch while training current batch
# Parameters:
#   - buffer_size: how many batches to prefetch
#   - tf.data.AUTOTUNE: Let TensorFlow pick optimal value automatically
# Effect: Overlaps I/O (loading) and computation (training) → huge speedup
dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)

# ITERATE through batches
# dataset.take(3) grabs first 3 batches without loading entire dataset
for batch in dataset.take(3):
    print('Batch shape:', batch.shape)  # [32, 16] for first 2, [8, 16] for last
    # Total samples: 1000
    # 1000 / 32 = 31.25 batches
    # Batch 1: 32 samples
    # Batch 2: 32 samples
    # ...
    # Batch 31: 8 samples (remainder)
</code></pre>

                <h4>Complete Pipeline: Shuffle → Batch → Prefetch</h4>

<pre><code class="language-python">import tensorflow as tf
import numpy as np

# Create sample dataset with features and labels
X = np.random.randn(500, 10).astype('float32')  # 500 samples, 10 features
y = np.random.randint(0, 2, (500,)).astype('int32')  # Binary labels

# BUILD PIPELINE in correct order
dataset = tf.data.Dataset.from_tensor_slices((X, y))

# STEP 1: SHUFFLE - randomize order for better generalization
# Parameters:
#   - buffer_size: how many samples to shuffle together
#   - Use buffer_size >= dataset size for perfect shuffling
#   - Larger buffer = better randomization but more memory
#   - Must come BEFORE batching to avoid sorting by class
# Why? Prevents bias where model learns ordering instead of features
dataset = dataset.shuffle(buffer_size=500)

# STEP 2: BATCH - group into mini-batches
# Group 32 consecutive samples together
# After shuffling, batch will have mixed samples (not sorted)
dataset = dataset.batch(32)

# STEP 3: PREFETCH - load next batch during training
# Overlaps I/O and computation for speed
dataset = dataset.prefetch(tf.data.AUTOTUNE)

# VERIFY pipeline
for features, labels in dataset.take(1):
    print('Features shape:', features.shape)  # (32, 10)
    print('Labels shape:', labels.shape)      # (32,)
    print('First batch labels:', labels.numpy())  # Mixed 0s and 1s (correct!)

# PIPELINE SUMMARY:
# Dataset has 500 samples
# After shuffle + batch(32) + prefetch: 16 batches
# - Batches 1-15: 32 samples each
# - Batch 16: 4 samples (remainder)
</code></pre>

                <h4>Advanced: Preprocessing with map()</h4>

                <p>Apply preprocessing functions with <code>map()</code> for data augmentation and normalization on-the-fly:</p>

<pre><code class="language-python">import tensorflow as tf
import numpy as np

# Create image-like dataset (100 images, 28x28 pixels, grayscale)
X = np.random.randint(0, 256, [100, 28, 28], dtype='uint8')
y = np.random.randint(0, 10, [100], dtype='int32')

dataset = tf.data.Dataset.from_tensor_slices((X, y))

# PREPROCESSING FUNCTION
# This function is applied to each element in the dataset
def preprocess_image(x, y):
    # Convert to float and normalize to [0, 1]
    x = tf.cast(x, tf.float32)     # Convert uint8 → float32
    x = x / 255.0                   # Normalize: [0, 256] → [0, 1]
    
    # Data augmentation: add random rotation-like transform
    # In practice, use tf.image.rot90, tf.image.flip_left_right, etc.
    noise = tf.random.normal([28, 28], stddev=0.05)
    x = x + noise
    
    return x, y

# APPLY TRANSFORMATION with map()
# Parameters:
#   - function: preprocessing function to apply to each element
#   - num_parallel_calls: how many samples to process in parallel
#     tf.data.AUTOTUNE: Let TensorFlow decide
# Effect: Parallelizes preprocessing across CPU cores
dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
dataset = dataset.batch(32)
dataset = dataset.prefetch(tf.data.AUTOTUNE)

# COMPLETE PIPELINE for training:
# Original data (100, 28, 28) → preprocess → batch(32) → prefetch
# For model.fit(dataset, epochs=10)
for batch_x, batch_y in dataset.take(1):
    print('Batch shape after preprocessing:', batch_x.shape)  # (32, 28, 28)
    print('Pixel range [0, 1]:', batch_x.min().numpy(), '-', batch_x.max().numpy())
</code></pre>

                <h4>Pipeline Performance Tips</h4>

<pre><code class="language-python">import tensorflow as tf

# GOOD: Correct order for best performance
dataset = tf.data.Dataset.from_tensor_slices(range(1000))
dataset = dataset.shuffle(1000)          # Randomize
dataset = dataset.batch(32)              # Group
dataset = dataset.prefetch(tf.data.AUTOTUNE)  # Prefetch

# BAD: Prefetch before batch (batches aren't prefetched!)
dataset_bad = tf.data.Dataset.from_tensor_slices(range(1000))
dataset_bad = dataset_bad.prefetch(tf.data.AUTOTUNE)  # Wrong place!
dataset_bad = dataset_bad.batch(32)      # Should be before prefetch

# OPTIMIZATION: Cache preprocessed data if it fits in memory
# .cache() stores all data in memory after preprocessing
dataset_cached = tf.data.Dataset.from_tensor_slices(range(100))
dataset_cached = dataset_cached.map(lambda x: x ** 2)  # Expensive operation
dataset_cached = dataset_cached.cache()  # Store in memory after this point
dataset_cached = dataset_cached.batch(32)
dataset_cached = dataset_cached.prefetch(tf.data.AUTOTUNE)
# Second epoch will use cached data (much faster!)

print('Dataset pipeline optimized for speed')
</code></pre>

                <div class="highlight-box">
                    <h4><i class="fas fa-database me-2"></i>tf.data Best Practices</h4>
                    <ul>
                        <li>Always use <code>prefetch(AUTOTUNE)</code> at the end of your pipeline</li>
                        <li>Shuffle before batching: <code>dataset.shuffle().batch().prefetch()</code></li>
                        <li>Use <code>cache()</code> to store preprocessed data in memory (if it fits)</li>
                        <li>Parallelize expensive ops with <code>map(..., num_parallel_calls=AUTOTUNE)</code></li>
                        <li>For large datasets, use <code>TFRecordDataset</code> for efficient serialization</li>
                    </ul>
                </div>

                <!-- Part 3: Training Workflows -->
                <h2 id="part3">Part 3: Training Workflows</h2>

                <h3 id="training-loops">Training: model.fit() vs Custom Loops</h3>

                <p>Keras provides <code>model.fit()</code> for high-level training—perfect for most use cases. It handles epochs, batches, metrics tracking, and callbacks automatically. For research or custom training logic (e.g., GANs, reinforcement learning), use custom loops with <code>GradientTape</code>.</p>

                <h4>High-Level Training with model.fit()</h4>

                <p>The simplest and most common way to train models. Keras handles all the complexity (batching, gradient computation, weight updates, metric tracking):</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Create synthetic dataset (200 samples, 16 features)
X_train = np.random.randn(200, 16).astype('float32')  # Training data
y_train = np.random.randn(200, 1).astype('float32')   # Training labels

# Build neural network
model = keras.Sequential([
    layers.Dense(32, activation='relu', input_shape=(16,)),
    layers.Dense(16, activation='relu'),
    layers.Dense(1)  # Output layer (regression)
])

# Compile: configure optimizer, loss, metrics
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# TRAIN with model.fit()
# Parameters:
#   - X_train, y_train: training data and labels
#   - epochs: number of passes through entire dataset
#   - batch_size: samples per gradient update
#   - validation_split: fraction of data for validation (e.g., 0.2 = 20%)
#   - verbose: 0 (silent), 1 (progress bar), 2 (one line per epoch)
history = model.fit(
    X_train, y_train,
    epochs=5,                    # 5 complete passes through training data
    batch_size=32,               # Update weights every 32 samples
    validation_split=0.2,        # Reserve 20% for validation
    verbose=1                    # Show progress bar
)

# HISTORY: contains loss/metrics at each epoch
print('Training losses:', history.history['loss'])        # Per-epoch training loss
print('Validation losses:', history.history['val_loss'])  # Per-epoch validation loss
print('Training MAE:', history.history['mae'])
print('Validation MAE:', history.history['val_mae'])

# What happens under the hood in model.fit():
# For each epoch:
#   1. Shuffle training data
#   2. Split into batches
#   3. For each batch:
#      - Forward pass: predictions = model(batch_X)
#      - Compute loss: loss = loss_function(predictions, batch_y)
#      - Backward pass: compute gradients via GradientTape
#      - Update: weights -= optimizer(learning_rate * gradients)
#   4. Evaluate on validation set
#   5. Print metrics
</code></pre>

                <h4>Custom Training Loop with Fine-Grained Control</h4>

                <p>When you need custom training logic (GANs, reinforcement learning, multi-task learning), write the loop manually using GradientTape:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Data
X = np.random.randn(200, 16).astype('float32')
y = np.random.randn(200, 1).astype('float32')

# Model
model = keras.Sequential([
    layers.Dense(32, activation='relu', input_shape=(16,)),
    layers.Dense(1)
])

# Optimizer
optimizer = keras.optimizers.Adam(learning_rate=0.001)

# Loss function
loss_fn = keras.losses.MeanSquaredError()

# Hyperparameters
epochs = 5
batch_size = 32

# CUSTOM TRAINING LOOP: Full control over each step
for epoch in range(epochs):
    print(f'\nEpoch {epoch+1}/{epochs}')
    epoch_loss = 0
    num_batches = 0
    
    # Iterate through batches manually
    for i in range(0, len(X), batch_size):
        x_batch = X[i:i+batch_size]
        y_batch = y[i:i+batch_size]
        
        # STEP 1: Forward pass with GradientTape recording
        # Everything inside 'with' block is recorded for gradient computation
        with tf.GradientTape() as tape:
            # Forward: make predictions
            predictions = model(x_batch, training=True)
            # Compute loss
            loss_value = loss_fn(y_batch, predictions)
            # Can add custom losses here, weighted combinations, etc.
        
        # STEP 2: Backward pass: compute gradients
        # Tells us how each weight affects the loss
        grads = tape.gradient(loss_value, model.trainable_variables)
        
        # STEP 3: Update weights
        # apply_gradients: W = W - learning_rate * gradient
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
        
        epoch_loss += loss_value.numpy()
        num_batches += 1
    
    print(f'Average loss: {epoch_loss/num_batches:.4f}')

print('Training complete!')

# ADVANTAGES of custom loops:
# 1. Full control over gradients (e.g., gradient clipping, manipulation)
# 2. Support custom loss combinations
# 3. Implement GAN training (separate generator/discriminator updates)
# 4. Multi-task learning with weighted losses
# 5. Research-specific training procedures

# DISADVANTAGES:
# 1. More boilerplate code
# 2. Harder to debug
# 3. Easy to make mistakes (forgot training flag, gradient accumulation, etc.)
# 4. Slower than optimized model.fit()
</code></pre>

                <h4>Comparison: model.fit() vs Custom Loop</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers

# MODEL.FIT() - Recommended for most cases
# Pros:
#   - Concise, readable code
#   - Automatic metric tracking and callbacks
#   - Optimized for speed
#   - Less room for bugs
# Cons:
#   - Less flexibility for advanced scenarios

# Build model
model = tf.keras.Sequential([layers.Dense(10, activation='relu')])
model.compile(optimizer='adam', loss='mse')
# history = model.fit(X, y, epochs=10, batch_size=32)

# CUSTOM LOOP - For research/advanced use
# Pros:
#   - Full control over training
#   - Support complex scenarios (GANs, multi-task, etc.)
#   - Custom gradient manipulation
# Cons:
#   - More code to write and debug
#   - Easier to make mistakes
#   - Slower if not carefully optimized

# When to use each:
print('Use model.fit() if:')
print('  - Standard supervised learning (classification, regression)')
print('  - You want simple, clean code')
print('  - You are a beginner')
print('')
print('Use custom loop if:')
print('  - Adversarial training (GANs, adversarial examples)')
print('  - Multi-task learning (different losses per task)')
print('  - Reinforcement learning (state-value functions, policy gradients)')
print('  - Meta-learning (learn-to-learn)')
print('  - Custom gradient computation needed')
</code></pre>

                <div class="highlight-box">
                    <h4><i class="fas fa-code-branch me-2"></i>When to Use Custom Loops</h4>
                    <ul>
                        <li><strong>Use model.fit():</strong> Standard supervised learning, classification, regression (95% of cases)</li>
                        <li><strong>Use custom loops:</strong> GANs (alternating generator/discriminator updates), reinforcement learning, custom gradient clipping, multi-optimizer scenarios</li>
                        <li>Custom loops require manual metric tracking and validation—more boilerplate code</li>
                    </ul>
                </div>

                <h3 id="callbacks">Callbacks for Training Control</h3>

                <p>Callbacks are functions executed at specific training stages (epoch end, batch end) to monitor, modify, or stop training. Keras provides powerful built-in callbacks: <code>EarlyStopping</code>, <code>ModelCheckpoint</code>, <code>ReduceLROnPlateau</code>, <code>TensorBoard</code>, and more.</p>

                <h4>EarlyStopping & ModelCheckpoint</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Data
X = np.random.randn(300, 20).astype('float32')
y = np.random.randint(0, 2, (300,)).astype('int32')

# Model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dropout(0.3),
    layers.Dense(32, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define callbacks
callbacks = [
    # Stop training when validation loss stops improving
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=3,  # Stop after 3 epochs without improvement
        restore_best_weights=True,  # Restore weights from best epoch
        verbose=1
    ),
    
    # Save model when validation loss improves
    keras.callbacks.ModelCheckpoint(
        filepath='best_model.keras',
        monitor='val_loss',
        save_best_only=True,
        verbose=1
    ),
    
    # Reduce learning rate when metric plateaus
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,  # Multiply LR by 0.5
        patience=2,  # Wait 2 epochs before reducing
        min_lr=1e-6,
        verbose=1
    )
]

# Train with callbacks
history = model.fit(
    X, y,
    epochs=20,
    batch_size=32,
    validation_split=0.2,
    callbacks=callbacks,
    verbose=0  # Suppress epoch logs (callbacks provide updates)
)

print(f'\nTraining completed. Best epoch restored.')
print(f'Total epochs run: {len(history.history["loss"])}')
</code></pre>

                <p><code>EarlyStopping</code> prevents overfitting by halting training when validation metrics degrade. <code>ModelCheckpoint</code> saves the best model version—crucial for long training runs. <code>ReduceLROnPlateau</code> adapts learning rate when progress stalls.</p>

                <h4>Custom Callback</h4>

                <p>Create custom callbacks for domain-specific logging or actions:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import numpy as np

class CustomLogger(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        # Log custom metrics at epoch end
        print(f'\nEpoch {epoch+1} complete.')
        print(f'  Training loss: {logs["loss"]:.4f}')
        if 'val_loss' in logs:
            print(f'  Validation loss: {logs["val_loss"]:.4f}')
    
    def on_train_end(self, logs=None):
        print('\nTraining finished!')

# Sample model and data
X = np.random.randn(100, 10).astype('float32')
y = np.random.randn(100, 1).astype('float32')

model = keras.Sequential([
    keras.layers.Dense(16, activation='relu', input_shape=(10,)),
    keras.layers.Dense(1)
])
model.compile(optimizer='adam', loss='mse')

# Train with custom callback
model.fit(X, y, epochs=3, callbacks=[CustomLogger()], verbose=0)
</code></pre>

                <p>Custom callbacks enable logging to external systems, dynamic hyperparameter adjustments, or early experiment termination based on custom criteria.</p>

                <h3 id="saving-loading">Model Persistence: Saving & Loading</h3>

                <p>Save models for deployment, transfer learning, or resuming training. TensorFlow supports multiple formats: <strong>SavedModel</strong> (recommended for production), <strong>.keras</strong> (Keras native format), and legacy <strong>.h5</strong> (HDF5).</p>

                <h4>Saving and Loading Models</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import numpy as np

# Build and train a simple model
model = keras.Sequential([
    keras.layers.Dense(32, activation='relu', input_shape=(16,)),
    keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train briefly
X = np.random.randn(100, 16).astype('float32')
y = np.random.randint(0, 2, (100,)).astype('int32')
model.fit(X, y, epochs=2, verbose=0)

# Save in .keras format (recommended)
model.save('my_model.keras')
print('Model saved to my_model.keras')

# Load the model
loaded_model = keras.models.load_model('my_model.keras')
print('Model loaded successfully.')

# Verify predictions match
original_pred = model.predict(X[:5], verbose=0)
loaded_pred = loaded_model.predict(X[:5], verbose=0)
print('Predictions match:', np.allclose(original_pred, loaded_pred))
</code></pre>

                <p>The <code>.keras</code> format saves architecture, weights, optimizer state, and training config—everything needed to resume training or deploy.</p>

                <h4>Saving Weights Only</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import numpy as np

# Model
model = keras.Sequential([
    keras.layers.Dense(16, activation='relu', input_shape=(10,)),
    keras.layers.Dense(1)
])

# Save weights only (smaller file, requires architecture separately)
model.save_weights('weights_only.weights.h5')
print('Weights saved.')

# Load weights into same architecture
new_model = keras.Sequential([
    keras.layers.Dense(16, activation='relu', input_shape=(10,)),
    keras.layers.Dense(1)
])
new_model.load_weights('weights_only.weights.h5')
print('Weights loaded into new model.')
</code></pre>

                <p>Saving weights only is useful for transfer learning (reuse pretrained weights with modified architecture) or reducing file size when architecture is known.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-save me-2"></i>Model Saving Best Practices</h4>
                    <ul>
                        <li><strong>.keras format:</strong> Use for development and Keras-specific workflows</li>
                        <li><strong>SavedModel:</strong> Use for production deployment (TensorFlow Serving, TF Lite)</li>
                        <li>Save checkpoints during training with <code>ModelCheckpoint</code> callback</li>
                        <li>Version your models: include timestamp or metrics in filename</li>
                        <li>Test loaded models with sample data before deployment</li>
                    </ul>
                </div>

                <h3 id="tensorboard">TensorBoard Visualization</h3>

                <p>TensorBoard is TensorFlow's visualization toolkit for monitoring training metrics, visualizing model architectures, profiling performance, and debugging. It runs as a web server displaying real-time or logged data.</p>

                <h4>Logging to TensorBoard</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import numpy as np
import datetime

# Create log directory with timestamp
log_dir = 'logs/fit/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')

# TensorBoard callback
tensorboard_callback = keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,  # Log weight histograms every epoch
    write_graph=True,  # Visualize model graph
    update_freq='epoch'  # Log after each epoch
)

# Sample model and data
X = np.random.randn(200, 16).astype('float32')
y = np.random.randn(200, 1).astype('float32')

model = keras.Sequential([
    keras.layers.Dense(32, activation='relu', input_shape=(16,)),
    keras.layers.Dense(16, activation='relu'),
    keras.layers.Dense(1)
])
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train with TensorBoard logging
model.fit(
    X, y,
    epochs=5,
    validation_split=0.2,
    callbacks=[tensorboard_callback],
    verbose=0
)

print(f'\nTensorBoard logs written to {log_dir}')
print('Launch TensorBoard with: tensorboard --logdir=logs/fit')
</code></pre>

                <p>After training, run <code>tensorboard --logdir=logs/fit</code> in your terminal and navigate to <code>http://localhost:6006</code> in your browser. You'll see training/validation curves, histograms, and model graphs.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-chart-area me-2"></i>TensorBoard Features</h4>
                    <ul>
                        <li><strong>Scalars:</strong> Loss and metric curves over time</li>
                        <li><strong>Graphs:</strong> Visualize model architecture and tensor flow</li>
                        <li><strong>Histograms:</strong> Track weight and gradient distributions</li>
                        <li><strong>Images:</strong> Log input samples and model predictions</li>
                        <li><strong>Profiler:</strong> Identify performance bottlenecks (CPU/GPU usage)</li>
                    </ul>
                </div>

                <h3 id="custom-metrics">Custom Metrics & Monitoring</h3>

                <p>While Keras provides built-in metrics (accuracy, precision, recall), custom metrics track domain-specific performance. Subclass <code>keras.metrics.Metric</code> to create stateful metrics that accumulate results across batches.</p>

                <h4>Creating a Custom Metric</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras

class MeanAbsoluteDifference(keras.metrics.Metric):
    def __init__(self, name='mean_abs_diff', **kwargs):
        super().__init__(name=name, **kwargs)
        # Create state variables
        self.total = self.add_weight(name='total', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
    
    def update_state(self, y_true, y_pred, sample_weight=None):
        # Accumulate absolute differences
        values = tf.abs(y_true - y_pred)
        self.total.assign_add(tf.reduce_sum(values))
        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))
    
    def result(self):
        # Compute final metric
        return self.total / self.count
    
    def reset_states(self):
        # Reset between epochs
        self.total.assign(0.0)
        self.count.assign(0.0)

# Test the custom metric
metric = MeanAbsoluteDifference()

# Simulate batch updates
y_true = tf.constant([[1.0], [2.0], [3.0]])
y_pred = tf.constant([[1.5], [2.2], [2.8]])
metric.update_state(y_true, y_pred)

print('Custom metric result:', metric.result().numpy())
metric.reset_states()
print('After reset:', metric.result().numpy())
</code></pre>

                <h4>Using Custom Metrics in Training</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
import numpy as np

# Custom metric (simplified version)
class MeanAbsError(keras.metrics.Metric):
    def __init__(self, name='mae', **kwargs):
        super().__init__(name=name, **kwargs)
        self.total = self.add_weight(name='total', initializer='zeros')
        self.count = self.add_weight(name='count', initializer='zeros')
    
    def update_state(self, y_true, y_pred, sample_weight=None):
        values = tf.abs(y_true - y_pred)
        self.total.assign_add(tf.reduce_sum(values))
        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))
    
    def result(self):
        return self.total / self.count
    
    def reset_states(self):
        self.total.assign(0.0)
        self.count.assign(0.0)

# Build model with custom metric
X = np.random.randn(100, 10).astype('float32')
y = np.random.randn(100, 1).astype('float32')

model = keras.Sequential([
    keras.layers.Dense(16, activation='relu', input_shape=(10,)),
    keras.layers.Dense(1)
])

model.compile(
    optimizer='adam',
    loss='mse',
    metrics=[MeanAbsError()]  # Add custom metric
)

# Train and monitor custom metric
history = model.fit(X, y, epochs=3, verbose=1)
</code></pre>

                <p>Custom metrics appear in training logs and TensorBoard alongside built-in metrics. They're essential for business-specific KPIs (e.g., customer churn rate, conversion probability) that don't map to standard ML metrics.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-tachometer-alt me-2"></i>Metric Design Tips</h4>
                    <ul>
                        <li>Use <code>add_weight()</code> to create persistent state variables</li>
                        <li><code>update_state()</code> accumulates results across batches (called multiple times per epoch)</li>
                        <li><code>result()</code> computes final metric value (called once per epoch)</li>
                        <li><code>reset_states()</code> clears state between epochs</li>
                        <li>Handle sample weights for class imbalance scenarios</li>
                    </ul>
                </div>

                <!-- Part 4: Practical Applications -->
                <h2 id="part4">Part 4: Practical Applications</h2>

                <h3 id="transfer-learning">Transfer Learning with Pretrained Models</h3>

                <p><strong>Transfer learning</strong> reuses knowledge from models trained on large datasets (ImageNet, COCO) for new tasks with limited data. Keras provides pretrained models through <code>keras.applications</code>: ResNet, MobileNet, EfficientNet, VGG, Inception, and more.</p>

                <p>The typical workflow: (1) Load pretrained base without top classification layer, (2) Freeze base weights, (3) Add custom classifier, (4) Train top layers, (5) Optionally fine-tune base layers.</p>

                <h4>Transfer Learning Example</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Load pretrained MobileNetV2 (without top classification layer)
# Note: Using weights=None for demonstration; use weights='imagenet' in practice
base_model = keras.applications.MobileNetV2(
    weights=None,  # Change to 'imagenet' to download pretrained weights
    include_top=False,  # Exclude classification head
    input_shape=(96, 96, 3)
)

# Freeze base model weights
base_model.trainable = False

# Build custom classifier on top
inputs = keras.Input(shape=(96, 96, 3))
x = base_model(inputs, training=False)  # Run base in inference mode
x = layers.GlobalAveragePooling2D()(x)  # Reduce spatial dimensions
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(10, activation='softmax')(x)  # 10-class classification

model = keras.Model(inputs, outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

print('Transfer learning model created.')
print(f'Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights])}')
print(f'Frozen parameters: {sum([tf.size(w).numpy() for w in model.non_trainable_weights])}')
</code></pre>

                <p><code>GlobalAveragePooling2D</code> converts feature maps (e.g., 3×3×1280) to a 1D vector (1280) by averaging each channel. This reduces parameters and prevents overfitting compared to flattening.</p>

                <h4>Fine-Tuning Strategy</h4>

                <p>After training top layers, unfreeze base layers and fine-tune with lower learning rate:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Assume model from previous example exists
# Create base and build model
base = keras.applications.MobileNetV2(
    weights=None,
    include_top=False,
    input_shape=(96, 96, 3)
)
base.trainable = False

inputs = keras.Input(shape=(96, 96, 3))
x = base(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
outputs = layers.Dense(10, activation='softmax')(x)
model = keras.Model(inputs, outputs)

# Initial training (top layers only)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Simulate: Train top layers first
# model.fit(train_data, epochs=5)

# Fine-tuning: Unfreeze base and train with lower LR
base.trainable = True

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=1e-5),  # Lower LR for fine-tuning
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print('Model ready for fine-tuning.')
print('Use lower learning rate (1e-5) to avoid destroying pretrained features.')
</code></pre>

                <p>Fine-tuning improves performance by adapting pretrained features to your specific domain. Use learning rates 10-100× smaller than initial training to preserve learned representations.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-exchange-alt me-2"></i>Transfer Learning Best Practices</h4>
                    <ul>
                        <li>Start with base frozen; train only top layers initially</li>
                        <li>Use GlobalAveragePooling2D instead of Flatten to reduce parameters</li>
                        <li>Fine-tune with learning rate 10-100× smaller (e.g., 1e-5 vs 1e-3)</li>
                        <li>Monitor validation loss carefully—easy to overfit with small datasets</li>
                        <li>Consider data augmentation (random flips, rotations) to increase effective dataset size</li>
                    </ul>
                </div>

                <h3 id="computer-vision">Computer Vision: CNN for Image Classification</h3>

                <p>Convolutional Neural Networks (CNNs) are the standard for image tasks. They use convolutional layers to detect spatial patterns (edges, textures, objects) and pooling layers to reduce dimensions while maintaining invariance.</p>

                <h4>Building a CNN from Scratch</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Build CNN for 32x32 RGB images (CIFAR-10 style)
model = keras.Sequential([
    # First convolutional block
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),
    
    # Second convolutional block
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    
    # Third convolutional block
    layers.Conv2D(128, (3, 3), activation='relu'),
    
    # Global pooling instead of flatten
    layers.GlobalAveragePooling2D(),
    
    # Dense classifier
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')  # 10 classes
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()
</code></pre>

                <p><code>Conv2D(32, (3,3))</code> creates 32 filters of size 3×3 that slide across the image detecting patterns. <code>MaxPooling2D</code> downsamples by taking maximum value in 2×2 windows, reducing spatial dimensions while keeping important features.</p>

                <h4>Training on Synthetic Data</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# CNN model
model = keras.Sequential([
    layers.Conv2D(16, 3, activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, activation='relu'),
    layers.GlobalAveragePooling2D(),
    layers.Dense(10, activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# Synthetic training data
images = np.random.rand(100, 32, 32, 3).astype('float32')
labels = np.random.randint(0, 10, (100,))

# Train
history = model.fit(images, labels, epochs=3, batch_size=32, verbose=1)

print('\nCNN trained on synthetic images.')
print(f'Final accuracy: {history.history["accuracy"][-1]:.2%}')
</code></pre>

                <p>For real datasets, use <code>tf.keras.datasets</code> (MNIST, CIFAR-10) or <code>tensorflow_datasets</code> (hundreds of ready-to-use datasets with preprocessing).</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-image me-2"></i>CNN Architecture Principles</h4>
                    <ul>
                        <li><strong>Convolution → Activation → Pooling</strong> is the standard block pattern</li>
                        <li>Increase filter count as you go deeper (16 → 32 → 64 → 128)</li>
                        <li>Use small filters (3×3) stacked; more effective than large filters (5×5, 7×7)</li>
                        <li>GlobalAveragePooling reduces parameters vs Flatten+Dense</li>
                        <li>Data augmentation (flips, rotations, crops) dramatically improves generalization</li>
                    </ul>
                </div>

                <h3 id="nlp-basics">Natural Language Processing Basics</h3>

                <p>NLP with TensorFlow involves tokenization (converting text to integer sequences), embedding (mapping tokens to dense vectors), and sequential models (LSTM, GRU) to capture context. Keras provides <code>TextVectorization</code> for preprocessing and <code>Embedding</code> layer for learnable word representations.</p>

                <h4>Text Classification with LSTM</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Hyperparameters
vocab_size = 1000  # Total unique words
seq_length = 20    # Fixed sequence length
embedding_dim = 32 # Dense vector size per word

# Build NLP model
model = keras.Sequential([
    # Embedding: vocab_size words → embedding_dim dimensional vectors
    layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),
    
    # LSTM: process sequence and output hidden state
    layers.LSTM(64, return_sequences=False),  # return_sequences=False → single output
    
    # Classifier
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')  # Binary sentiment classification
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()
</code></pre>

                <p><code>Embedding</code> converts integer token IDs to dense vectors (learnable during training). <code>LSTM</code> processes sequences while maintaining memory of previous tokens—critical for context understanding.</p>

                <h4>Training on Synthetic Sequences</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

vocab_size = 1000
seq_length = 20

# Model
model = keras.Sequential([
    layers.Embedding(vocab_size, 32, input_length=seq_length),
    layers.LSTM(32),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Synthetic data: random token sequences
X_sequences = np.random.randint(0, vocab_size, (200, seq_length))
y_labels = np.random.randint(0, 2, (200,))

# Train
history = model.fit(X_sequences, y_labels, epochs=3, batch_size=32, verbose=1)

print('\nNLP model trained on synthetic token sequences.')
</code></pre>

                <p>For real text data, use <code>tf.keras.layers.TextVectorization</code> to automatically build vocabulary and convert text to sequences.</p>

                <h4>Bidirectional LSTM for Better Context</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Bidirectional LSTM processes sequence forward and backward
model = keras.Sequential([
    layers.Embedding(1000, 32, input_length=20),
    layers.Bidirectional(layers.LSTM(32)),  # Wraps LSTM to process both directions
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

print('\nBidirectional LSTM captures context from both past and future tokens.')
</code></pre>

                <p>Bidirectional models improve performance by seeing the full context (past and future) but double the parameters and computation time.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-comments me-2"></i>NLP Quick Tips</h4>
                    <ul>
                        <li><strong>Embedding dimension:</strong> Start with 32-128; larger for huge vocabularies (50k+ words)</li>
                        <li><strong>LSTM vs GRU:</strong> GRU is faster (fewer parameters); LSTM slightly better on complex sequences</li>
                        <li><strong>Bidirectional:</strong> Use when full context is available (not for real-time generation)</li>
                        <li><strong>Transformers:</strong> For state-of-the-art NLP, explore TensorFlow's Transformer implementations or Hugging Face</li>
                    </ul>
                </div>

                <h3 id="time-series">Time Series Forecasting</h3>

                <p>Time series prediction uses historical data windows to forecast future values. Create windowed datasets by sliding a window across the series, then use dense layers, RNNs (LSTM/GRU), or CNNs for pattern recognition.</p>

                <h4>Creating Windowed Dataset</h4>

<pre><code class="language-python">import tensorflow as tf
import numpy as np

# Generate synthetic time series (sine wave with noise)
time_steps = 200
series = np.sin(np.linspace(0, 10, time_steps)) + 0.1 * np.random.randn(time_steps)

# Create windowed dataset
window_size = 10
X_windows = []
y_targets = []

for i in range(len(series) - window_size):
    X_windows.append(series[i:i+window_size])
    y_targets.append(series[i+window_size])

X_windows = np.array(X_windows)
y_targets = np.array(y_targets)

print(f'Created {len(X_windows)} windows.')
print(f'Window shape: {X_windows.shape}')
print(f'Target shape: {y_targets.shape}')
</code></pre>

                <p>Each window contains <code>window_size</code> historical values; the target is the next value. This converts a time series into a supervised learning problem: <code>X</code> (past) → <code>y</code> (future).</p>

                <h4>Training a Forecasting Model</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Synthetic time series
series = np.sin(np.linspace(0, 10, 200)) + 0.1 * np.random.randn(200)
window_size = 10

# Create windows
X = []
y = []
for i in range(len(series) - window_size):
    X.append(series[i:i+window_size])
    y.append(series[i+window_size])
X = np.array(X)
y = np.array(y)

# Build forecasting model
model = keras.Sequential([
    layers.Dense(32, activation='relu', input_shape=(window_size,)),
    layers.Dropout(0.2),
    layers.Dense(16, activation='relu'),
    layers.Dense(1)  # Single output: next value
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train
history = model.fit(X, y, epochs=10, batch_size=16, validation_split=0.2, verbose=0)

print('Time series model trained.')
print(f'Final validation MAE: {history.history["val_mae"][-1]:.4f}')
</code></pre>

                <p>For more complex patterns, replace dense layers with LSTM or CNN layers. LSTMs excel at capturing long-term dependencies; CNNs are faster for local patterns.</p>

                <h4>Using LSTM for Time Series</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Data preparation (same as before)
series = np.sin(np.linspace(0, 10, 200)) + 0.1 * np.random.randn(200)
window_size = 10
X = []
y = []
for i in range(len(series) - window_size):
    X.append(series[i:i+window_size])
    y.append(series[i+window_size])
X = np.array(X).reshape(-1, window_size, 1)  # Reshape for LSTM: (samples, timesteps, features)
y = np.array(y)

# LSTM model
model = keras.Sequential([
    layers.LSTM(32, input_shape=(window_size, 1)),
    layers.Dense(16, activation='relu'),
    layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')

# Train
model.fit(X, y, epochs=5, batch_size=16, verbose=0)

print('LSTM time series model trained.')
print('LSTM input shape: (samples, timesteps, features)')
</code></pre>

                <p>LSTM expects 3D input: (batch_size, timesteps, features). Reshape windows accordingly. For multivariate time series (multiple features), increase the last dimension.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-clock me-2"></i>Time Series Best Practices</h4>
                    <ul>
                        <li>Choose window size based on domain knowledge (daily/weekly/monthly patterns)</li>
                        <li>Normalize/standardize data before training for better convergence</li>
                        <li>Use train/validation/test split chronologically (don't shuffle time series)</li>
                        <li>LSTM for long-term dependencies; Dense/CNN for short-term patterns</li>
                        <li>Consider seasonality, trends, and external factors in feature engineering</li>
                    </ul>
                </div>

                <!-- Part 5: Advanced Topics -->
                <h2 id="part5">Part 5: Advanced Topics</h2>

                <!-- Attention Layers & MultiHeadAttention -->
                <h3 id="attention-layers">Attention Layers & MultiHeadAttention</h3>

                <p>Attention mechanisms allow models to focus on relevant parts of input sequences. Keras provides <code>keras.layers.MultiHeadAttention</code>, a production-ready implementation of scaled dot-product attention with multiple heads for parallel processing of different representation subspaces.</p>

                <h4>Basic Multi-Head Attention</h4>

                <p>The MultiHeadAttention layer computes attention weights over a sequence or between two different sequences:</p>

                <pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Create multi-head attention layer
attention = layers.MultiHeadAttention(
    num_heads=8,      # Number of attention heads
    key_dim=64,       # Dimension of each head
    dropout=0.1       # Dropout for regularization
)

# Prepare inputs
batch_size = 32
seq_length = 20
feature_dim = 512

# Query, Key, Value tensors
query = tf.random.normal([batch_size, seq_length, feature_dim])
key = tf.random.normal([batch_size, seq_length, feature_dim])
value = tf.random.normal([batch_size, seq_length, feature_dim])

# Self-attention (query, key, value from same source)
attn_output = attention(query, value, key=key, return_attention_scores=False)
print(f'Attention output shape: {attn_output.shape}')  # [32, 20, 512]

# Get attention weights
attn_output, attn_weights = attention(query, value, key=key, return_attention_scores=True)
print(f'Attention weights shape: {attn_weights.shape}')  # [32, 8, 20, 20]</code></pre>

                <p><strong>Cross-Attention:</strong> Use different sequences for Query vs Key/Value, useful for encoder-decoder architectures:</p>

                <pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers

attention = layers.MultiHeadAttention(
    num_heads=8,
    key_dim=64
)

# Encoder output (context for attention)
encoder_output = tf.random.normal([16, 30, 512])  # (batch, src_len, d_model)

# Decoder query
decoder_query = tf.random.normal([16, 20, 512])  # (batch, tgt_len, d_model)

# Cross-attention: Query from decoder, Key/Value from encoder
cross_attn = attention(
    query=decoder_query,
    value=encoder_output,
    key=encoder_output
)

print(f'Cross-attention output shape: {cross_attn.shape}')  # [16, 20, 512]</code></pre>

                <p><strong>Attention Masking:</strong> Prevent attention to padding tokens or future positions:</p>

                <pre><code class="language-python">import tensorflow as tf
from tensorflow.keras import layers
import numpy as np

attention = layers.MultiHeadAttention(
    num_heads=8,
    key_dim=64
)

# Input sequences
query = tf.random.normal([4, 10, 512])
key = tf.random.normal([4, 10, 512])
value = tf.random.normal([4, 10, 512])

# Create causal mask (prevent attention to future tokens)
mask = tf.linalg.band_part(tf.ones((10, 10)), -1, 0)  # Lower triangular
mask = (1.0 - mask) * -1e9  # Mask future positions with large negative values

# Apply attention with mask
attn_output = attention(
    query,
    value,
    key=key,
    attention_mask=mask
)
print(f'Masked attention output: {attn_output.shape}')  # [4, 10, 512]</code></pre>

                <div class="experiment-card">
                    <div class="card-meta mb-2">
                        <span class="badge bg-teal text-white">Attention Concepts</span>
                    </div>
                    <div class="card-content">
                        <ul>
                            <li><strong>Self-Attention:</strong> Query, Key, Value from same sequence; learns dependencies within sequence</li>
                            <li><strong>Cross-Attention:</strong> Query from different sequence than Key/Value; fuses information from encoder to decoder</li>
                            <li><strong>Multi-Head:</strong> Multiple parallel attention heads capture different types of relationships</li>
                            <li><strong>Scaled Dot-Product:</strong> Scores = softmax(Q·K^T / √d_k), prevents gradient vanishing with large dimensions</li>
                            <li><strong>Masking:</strong> Causal mask prevents future information leak; padding mask ignores padding tokens</li>
                        </ul>
                    </div>
                </div>

                <!-- Building Transformers with Keras -->
                <h3 id="transformer-keras">Building Transformers with Keras</h3>

                <p>Combine MultiHeadAttention with feed-forward networks and normalization layers to build complete Transformer encoder-decoder architectures.</p>

                <h4>Transformer Encoder Block</h4>

                <p>A typical encoder block has self-attention and feed-forward layers with residual connections and layer normalization:</p>

                <pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class TransformerEncoderBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):
        super().__init__()
        self.att = layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=embed_dim // num_heads,
            dropout=dropout
        )
        self.ffn = keras.Sequential([
            layers.Dense(ff_dim, activation='relu'),
            layers.Dense(embed_dim)
        ])
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(dropout)
        self.dropout2 = layers.Dropout(dropout)

    def call(self, inputs, training=False):
        # Self-attention block with residual connection
        attn_output = self.att(inputs, inputs, training=training)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)

        # Feed-forward block with residual connection
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)

        return out2

# Test the encoder block
encoder_block = TransformerEncoderBlock(
    embed_dim=512,
    num_heads=8,
    ff_dim=2048,
    dropout=0.1
)

input_seq = tf.random.normal([8, 20, 512])  # (batch, seq_len, embed_dim)
output = encoder_block(input_seq, training=True)
print(f'Encoder block output shape: {output.shape}')  # [8, 20, 512]</code></pre>

                <h4>Complete Transformer Model</h4>

                <p>Stack encoder blocks and add embeddings for a complete sequence model:</p>

                <pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

class PositionalEmbedding(layers.Layer):
    def __init__(self, max_len, d_model):
        super().__init__()
        self.d_model = d_model
        # Create positional encodings
        pe = np.zeros((max_len, d_model))
        position = np.arange(0, max_len, dtype=np.float32)[:, np.newaxis]
        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
        pe[:, 0::2] = np.sin(position * div_term)
        pe[:, 1::2] = np.cos(position * div_term)
        self.pe = tf.constant(pe[np.newaxis, ...], dtype=tf.float32)

    def call(self, x):
        return x + self.pe[:, :tf.shape(x)[1], :]

class TransformerModel(keras.Model):
    def __init__(self, vocab_size, max_len, d_model, num_heads, num_layers, ff_dim):
        super().__init__()
        self.embedding = layers.Embedding(vocab_size, d_model)
        self.pos_embedding = PositionalEmbedding(max_len, d_model)
        self.encoder_blocks = [
            TransformerEncoderBlock(d_model, num_heads, ff_dim)
            for _ in range(num_layers)
        ]
        self.final_norm = layers.LayerNormalization(epsilon=1e-6)
        self.output_dense = layers.Dense(vocab_size)

    def call(self, inputs, training=False):
        # Embed and add positional encodings
        x = self.embedding(inputs)
        x = self.pos_embedding(x)

        # Pass through encoder blocks
        for encoder_block in self.encoder_blocks:
            x = encoder_block(x, training=training)

        # Final normalization and output
        x = self.final_norm(x)
        x = self.output_dense(x)
        return x

# Create and use model
model = TransformerModel(
    vocab_size=10000,
    max_len=100,
    d_model=512,
    num_heads=8,
    num_layers=6,
    ff_dim=2048
)

# Forward pass
input_ids = tf.random.uniform([8, 50], minval=0, maxval=10000, dtype=tf.int32)
output = model(input_ids, training=True)
print(f'Transformer output shape: {output.shape}')  # [8, 50, 10000]

# Count parameters
total_params = sum([tf.size(w).numpy() for w in model.trainable_weights])
print(f'Total parameters: {total_params / 1e6:.1f}M')</code></pre>

                <h4>Vision Transformer (ViT) with Keras</h4>

                <p>Apply Transformers to image patches for state-of-the-art vision models:</p>

                <pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class PatchEmbedding(layers.Layer):
    def __init__(self, patch_size, embed_dim):
        super().__init__()
        self.patch_size = patch_size
        self.embed_dim = embed_dim
        self.projection = layers.Dense(embed_dim)

    def call(self, images):
        # images shape: (batch, height, width, channels)
        batch_size = tf.shape(images)[0]
        
        # Extract patches
        patches = tf.image.extract_patches(
            images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding='VALID'
        )
        
        # Flatten patches
        num_patches = tf.shape(patches)[1] * tf.shape(patches)[2]
        patch_dim = tf.shape(patches)[-1]
        patches = tf.reshape(patches, [batch_size, num_patches, patch_dim])
        
        # Project to embedding dimension
        embeddings = self.projection(patches)
        return embeddings

class VisionTransformer(keras.Model):
    def __init__(self, image_size, patch_size, num_classes, d_model, num_heads, num_layers, ff_dim):
        super().__init__()
        num_patches = (image_size // patch_size) ** 2
        
        self.patch_embed = PatchEmbedding(patch_size, d_model)
        self.cls_token = self.add_weight(
            'cls_token',
            shape=[1, 1, d_model],
            initializer='random_normal'
        )
        self.pos_embed = self.add_weight(
            'pos_embed',
            shape=[1, num_patches + 1, d_model],
            initializer='random_normal'
        )
        
        self.encoder_blocks = [
            TransformerEncoderBlock(d_model, num_heads, ff_dim)
            for _ in range(num_layers)
        ]
        
        self.norm = layers.LayerNormalization(epsilon=1e-6)
        self.head = layers.Dense(num_classes)

    def call(self, images, training=False):
        batch_size = tf.shape(images)[0]
        
        # Embed patches
        x = self.patch_embed(images)
        
        # Add class token
        cls_tokens = tf.broadcast_to(self.cls_token, [batch_size, 1, tf.shape(x)[-1]])
        x = tf.concat([cls_tokens, x], axis=1)
        
        # Add positional embeddings
        x = x + self.pos_embed
        
        # Transformer encoder blocks
        for encoder_block in self.encoder_blocks:
            x = encoder_block(x, training=training)
        
        # Classification from [CLS] token
        x = self.norm(x)
        x = x[:, 0]  # Take [CLS] token
        x = self.head(x)
        return x

# Create ViT model
vit = VisionTransformer(
    image_size=224,
    patch_size=16,
    num_classes=1000,
    d_model=768,
    num_heads=12,
    num_layers=12,
    ff_dim=3072
)

# Forward pass
images = tf.random.normal([4, 224, 224, 3])
logits = vit(images, training=True)
print(f'ViT output shape: {logits.shape}')  # [4, 1000]</code></pre>

                <div class="experiment-card">
                    <div class="card-meta mb-2">
                        <span class="badge bg-teal text-white">Transformer Architecture</span>
                    </div>
                    <div class="card-content">
                        <ul>
                            <li><strong>Patch Embedding:</strong> Convert images to sequence of patch embeddings (224×224 → 14×14 patches)</li>
                            <li><strong>Positional Encoding:</strong> Add learnable or sinusoidal position embeddings to preserve sequence order</li>
                            <li><strong>Multi-Head Attention:</strong> Self-attention allows global receptive field from first layer</li>
                            <li><strong>Feed-Forward:</strong> Per-token MLPs with ReLU activation capture non-linear patterns</li>
                            <li><strong>Layer Norm & Residuals:</strong> Normalize before sublayers (pre-norm) and skip connections enable deep networks</li>
                        </ul>
                    </div>
                </div>

                <!-- Distributed Training -->
                <h3 id="distributed-training">Distributed Training & Multi-GPU</h3>

                <p>Scale model training across multiple GPUs or TPUs using TensorFlow's distribution strategies. Keras models automatically support distributed training with minimal code changes.</p>

                <h4>Single-Machine Multi-GPU with MirroredStrategy</h4>

                <p>Replicate model on all GPUs and sync gradients after each batch:</p>

                <pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Create distribution strategy
strategy = tf.distribute.MirroredStrategy()

print(f'Number of devices: {strategy.num_replicas_in_sync}')
print(f'Devices: {tf.config.list_physical_devices("GPU")}')

# Build model inside strategy scope
with strategy.scope():
    model = keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(20,)),
        layers.Dropout(0.2),
        layers.Dense(32, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

# Data preparation
X = np.random.randn(1000, 20).astype('float32')
y = np.eye(10)[np.random.randint(0, 10, 1000)]

# Training automatically uses all GPUs
history = model.fit(
    X, y,
    epochs=5,
    batch_size=32,  # Per-GPU batch size
    verbose=1
)

print('Training complete with multi-GPU acceleration.')</code></pre>

                <h4>Multi-Machine with ParameterServerStrategy</h4>

                <p>Distribute training across multiple machines with parameter servers:</p>

                <pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Setup for multi-machine training
# Requires cluster configuration (TF_CONFIG environment variable)

if tf.config.list_physical_devices('GPU'):
    # Multi-GPU strategy
    strategy = tf.distribute.MultiWorkerMirroredStrategy()
else:
    # Fallback: single machine
    strategy = tf.distribute.get_strategy()

print(f'Devices: {strategy.num_replicas_in_sync}')

# Build and compile inside strategy scope
with strategy.scope():
    model = keras.Sequential([
        layers.Dense(128, activation='relu', input_shape=(100,)),
        layers.Dropout(0.3),
        layers.Dense(64, activation='relu'),
        layers.Dense(32, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

print('Model ready for multi-machine distributed training')
print('Set TF_CONFIG environment variable with cluster specification')</code></pre>

                <h4>Custom Training Loop with Distribution</h4>

                <p>For fine-grained control, use <code>strategy.run()</code> with custom training steps:</p>

                <pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

strategy = tf.distribute.MirroredStrategy()

# Data preparation
X = np.random.randn(1000, 32).astype('float32')
y = np.random.randint(0, 10, 1000)

dataset = tf.data.Dataset.from_tensor_slices((X, y))
dataset = dataset.shuffle(1000).batch(32)
dist_dataset = strategy.experimental_distribute_dataset(dataset)

# Build model
with strategy.scope():
    model = keras.Sequential([
        layers.Dense(64, activation='relu'),
        layers.Dense(10, activation='softmax')
    ])
    
    optimizer = keras.optimizers.Adam()
    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)

# Custom training step
def train_step(batch_x, batch_y):
    with tf.GradientTape() as tape:
        logits = model(batch_x, training=True)
        loss_value = loss_fn(batch_y, logits)
        scaled_loss = loss_value / strategy.num_replicas_in_sync
    
    grads = tape.gradient(scaled_loss, model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
    
    return loss_value

@tf.function
def distributed_train_step(dist_inputs):
    per_replica_losses = strategy.run(train_step, args=(dist_inputs[0], dist_inputs[1]))
    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)

# Training loop
epochs = 3
for epoch in range(epochs):
    total_loss = 0.0
    num_batches = 0
    
    for dist_batch in dist_dataset:
        loss = distributed_train_step(dist_batch)
        total_loss += loss
        num_batches += 1
    
    avg_loss = total_loss / num_batches
    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')

print('Custom distributed training complete')</code></pre>

                <p><strong>Gradient Synchronization:</strong> TensorFlow automatically synchronizes gradients across replicas. Batch size is per-replica; total batch = batch_size × num_replicas.</p>

                <div class="experiment-card">
                    <div class="card-meta mb-2">
                        <span class="badge bg-teal text-white">Distribution Strategies</span>
                    </div>
                    <div class="card-content">
                        <ul>
                            <li><strong>MirroredStrategy:</strong> Single machine, multi-GPU; fastest for one node with many GPUs</li>
                            <li><strong>MultiWorkerMirroredStrategy:</strong> Multiple machines with all-reduce gradient sync</li>
                            <li><strong>ParameterServerStrategy:</strong> Distribute parameters across servers; good for large models with many workers</li>
                            <li><strong>TPUStrategy:</strong> Optimized for Google Cloud TPUs; minimal code changes</li>
                            <li><strong>Automatic Batch Scaling:</strong> Batch size is per-replica; total batch = batch_size × num_replicas</li>
                        </ul>
                    </div>
                </div>

                <h3 id="performance">Performance Optimization</h3>

                <p>TensorFlow offers multiple performance optimizations: <strong>@tf.function</strong> for graph compilation, <strong>mixed precision</strong> for faster GPU training, and <strong>distributed strategies</strong> for multi-GPU scaling. These techniques can deliver 2-10× speedups with minimal code changes.</p>

                <h4>Graph Compilation with @tf.function</h4>

                <p>Convert Python functions to optimized TensorFlow graphs for faster execution:</p>

<pre><code class="language-python">import tensorflow as tf
import time

# Regular Python function (eager execution)
def slow_function(x):
    return tf.reduce_sum(x * x)

# Graph-compiled function
@tf.function
def fast_function(x):
    return tf.reduce_sum(x * x)

# Benchmark
x = tf.random.normal([10000])

# Warm-up
fast_function(x)

# Time eager execution
start = time.time()
for _ in range(100):
    slow_function(x)
eager_time = time.time() - start

# Time graph execution
start = time.time()
for _ in range(100):
    fast_function(x)
graph_time = time.time() - start

print(f'Eager execution: {eager_time:.4f}s')
print(f'Graph execution: {graph_time:.4f}s')
print(f'Speedup: {eager_time/graph_time:.2f}x')
</code></pre>

                <p><code>@tf.function</code> traces the Python function once, builds a graph, and reuses it for subsequent calls—eliminating Python overhead. Use for training loops, inference functions, and data preprocessing.</p>

                <h4>Mixed Precision Training</h4>

                <p>Train with float16 for speed while keeping critical ops in float32 for stability:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, mixed_precision

# Enable mixed precision (requires compatible GPU)
# Note: This may not show benefits without GPU
mixed_precision.set_global_policy('mixed_float16')

print('Mixed precision policy:', mixed_precision.global_policy())

# Build model (automatically uses float16 where beneficial)
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(32,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(10)  # Output layer uses float32 for stability
])

# Loss scaling prevents underflow in gradients
# (handled automatically in model.compile with mixed precision)
model.compile(
    optimizer='adam',
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True)
)

print('Model built with mixed precision.')

# Reset to float32 for rest of examples
mixed_precision.set_global_policy('float32')
</code></pre>

                <p>Mixed precision can deliver 2-3× speedups on modern GPUs (V100, A100) with minimal accuracy loss. TensorFlow handles loss scaling automatically to prevent gradient underflow.</p>

                <h4>Distributed Training with MirroredStrategy</h4>

                <p>Synchronous multi-GPU training with minimal code changes:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Create distribution strategy (auto-detects GPUs)
strategy = tf.distribute.MirroredStrategy()

print(f'Number of devices: {strategy.num_replicas_in_sync}')

# Build model inside strategy scope
with strategy.scope():
    model = keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(20,)),
        layers.Dense(32, activation='relu'),
        layers.Dense(1)
    ])
    
    model.compile(optimizer='adam', loss='mse')

print('Model created for distributed training.')
print('Note: Benefits appear with multiple GPUs; falls back to single device if unavailable.')
</code></pre>

                <p><code>MirroredStrategy</code> replicates model on all GPUs, splits batches across devices, and averages gradients. Near-linear scaling up to 8 GPUs for large batch training.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-rocket me-2"></i>Performance Optimization Checklist</h4>
                    <ul>
                        <li>Use <code>@tf.function</code> for training loops and data preprocessing</li>
                        <li>Enable mixed precision on modern GPUs (V100, A100, RTX 30 series)</li>
                        <li>Prefetch data with <code>tf.data.AUTOTUNE</code> to overlap I/O and compute</li>
                        <li>Use <code>MirroredStrategy</code> for multi-GPU; <code>TPUStrategy</code> for Google TPUs</li>
                        <li>Profile with TensorBoard Profiler to identify bottlenecks</li>
                    </ul>
                </div>

                <h3 id="interpretability">Model Interpretability</h3>

                <p>Understanding <em>why</em> models make predictions is critical for debugging, trust, and compliance. <strong>Grad-CAM</strong> (Gradient-weighted Class Activation Mapping) visualizes important regions in images for CNN decisions. For other interpretability methods, explore SHAP, LIME, and attention weights.</p>

                <h4>Grad-CAM Concept</h4>

                <p>Grad-CAM computes gradients of class predictions with respect to convolutional layer activations, highlighting important spatial regions:</p>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Simple CNN for demonstration
model = keras.Sequential([
    layers.Conv2D(16, 3, activation='relu', input_shape=(32, 32, 3)),
    layers.Conv2D(32, 3, activation='relu', name='target_conv'),
    layers.GlobalAveragePooling2D(),
    layers.Dense(10, activation='softmax')
])

# Simplified Grad-CAM stub (conceptual demonstration)
def grad_cam_stub(model, img_tensor, class_index=0):
    """Simplified Grad-CAM for educational purposes."""
    # Get conv layer and predictions
    conv_layer = model.get_layer('target_conv')
    grad_model = keras.Model(
        inputs=model.inputs,
        outputs=[conv_layer.output, model.output]
    )
    
    # Compute gradients
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_tensor)
        loss = predictions[:, class_index]
    
    # Get gradients of loss w.r.t. conv outputs
    grads = tape.gradient(loss, conv_outputs)
    
    # Weight feature maps by gradient importance
    weights = tf.reduce_mean(grads, axis=(0, 1, 2))
    cam = tf.reduce_sum(tf.multiply(weights, conv_outputs[0]), axis=-1)
    
    # Normalize heatmap
    cam = tf.maximum(cam, 0)
    cam = cam / tf.reduce_max(cam)
    
    return cam.numpy()

# Test with random image
test_img = tf.random.normal([1, 32, 32, 3])
heatmap = grad_cam_stub(model, test_img, class_index=0)

print('Grad-CAM heatmap shape:', heatmap.shape)
print('Heatmap highlights important regions for class prediction.')
</code></pre>

                <p>In practice, overlay the heatmap on the original image using matplotlib's <code>imshow</code> with transparency. Bright regions indicate areas the model focused on for classification.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-search me-2"></i>Interpretability Tools</h4>
                    <ul>
                        <li><strong>Grad-CAM:</strong> Visual explanations for CNN image predictions</li>
                        <li><strong>Attention Weights:</strong> For Transformers/NLP, visualize which tokens influence predictions</li>
                        <li><strong>SHAP:</strong> Game-theory based feature importance (works for any model)</li>
                        <li><strong>LIME:</strong> Local approximations explaining individual predictions</li>
                        <li><strong>TensorFlow Integrated Gradients:</strong> Attribute predictions to input features</li>
                    </ul>
                </div>

                <h3 id="deployment">Deployment & Serving</h3>

                <p>Models are only valuable when deployed to production. TensorFlow offers multiple deployment options: <strong>TensorFlow Serving</strong> (high-throughput servers), <strong>TensorFlow Lite</strong> (mobile/edge), <strong>TensorFlow.js</strong> (web browsers), and cloud platforms (AWS SageMaker, Google AI Platform).</p>

                <h4>Saving for Deployment</h4>

<pre><code class="language-python">import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# Build and train simple model
model = keras.Sequential([
    layers.Dense(32, activation='relu', input_shape=(16,)),
    layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy')

# Simulate training
X = np.random.randn(100, 16).astype('float32')
y = np.random.randint(0, 2, (100,)).astype('int32')
model.fit(X, y, epochs=2, verbose=0)

# Save as SavedModel (recommended for production)
model.save('production_model', save_format='tf')
print('Model saved in SavedModel format for TensorFlow Serving.')

# Load and verify
loaded = keras.models.load_model('production_model')
test_input = np.random.randn(5, 16).astype('float32')
predictions = loaded.predict(test_input, verbose=0)
print('Predictions shape:', predictions.shape)
</code></pre>

                <p>SavedModel format includes architecture, weights, and computation graph—everything needed for inference in production environments.</p>

                <h4>TensorFlow Serving Overview</h4>

                <p>TensorFlow Serving is a high-performance serving system for production ML:</p>

<pre><code class="language-bash"># Install TensorFlow Serving (Docker recommended)
docker pull tensorflow/serving

# Serve a SavedModel
docker run -p 8501:8501 \
  --mount type=bind,source=/path/to/production_model,target=/models/my_model \
  -e MODEL_NAME=my_model \
  -t tensorflow/serving

# Query the REST API
curl -X POST http://localhost:8501/v1/models/my_model:predict \
  -d '{"instances": [[1.0, 2.0, ..., 16.0]]}'
</code></pre>

                <p>TensorFlow Serving handles versioning, batching, and GPU utilization automatically. It supports gRPC (low latency) and REST APIs (easy integration).</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-cloud me-2"></i>Deployment Options Comparison</h4>
                    <ul>
                        <li><strong>TensorFlow Serving:</strong> High-throughput server inference (data centers, cloud)</li>
                        <li><strong>TensorFlow Lite:</strong> Mobile (iOS/Android) and edge devices (Raspberry Pi)</li>
                        <li><strong>TensorFlow.js:</strong> Run models in web browsers (JavaScript)</li>
                        <li><strong>Cloud Platforms:</strong> Managed services (AWS SageMaker, GCP AI Platform, Azure ML)</li>
                        <li><strong>ONNX:</strong> Export to ONNX format for deployment in non-TensorFlow runtimes</li>
                    </ul>
                </div>

                <h3 id="best-practices">Best Practices & Next Steps</h3>

                <p>Mastering TensorFlow requires balancing theory, practice, and production awareness. Here's a comprehensive guide to solidify your foundation and advance your skills.</p>

                <h4>Production Deployment Checklist</h4>

                <ol>
                    <li><strong>Data Pipelines:</strong> Always use <code>tf.data</code> with prefetch for efficient I/O</li>
                    <li><strong>Validation:</strong> Monitor training vs validation metrics—stop when validation degrades</li>
                    <li><strong>Checkpointing:</strong> Save model checkpoints during training (use <code>ModelCheckpoint</code>)</li>
                    <li><strong>TensorBoard:</strong> Log metrics early; visualize training curves to diagnose issues</li>
                    <li><strong>Regularization:</strong> Apply dropout, L1/L2, or early stopping to combat overfitting</li>
                    <li><strong>Testing:</strong> Evaluate on held-out test set; report confidence intervals for metrics</li>
                    <li><strong>Versioning:</strong> Version models (semantic versioning) and track training config</li>
                </ol>

                <h4>Advanced Learning Paths</h4>

                <p><strong>Computer Vision:</strong> Explore object detection (YOLO, Faster R-CNN), semantic segmentation (U-Net, Mask R-CNN), and generative models (GANs, diffusion models).</p>

                <p><strong>NLP & Transformers:</strong> Dive into Transformer architectures (BERT, GPT), use Hugging Face libraries with TensorFlow backend, or implement attention mechanisms from scratch.</p>

                <p><strong>Reinforcement Learning:</strong> Use TF-Agents for policy gradient methods, DQN, and actor-critic algorithms in game environments or robotics.</p>

                <p><strong>Optimization & Deployment:</strong> Profile with TensorBoard Profiler, quantize models for edge deployment (TensorFlow Lite), experiment with pruning and knowledge distillation.</p>

                <p><strong>Research:</strong> Implement papers from arXiv, contribute to TensorFlow addons, or build custom training loops for novel architectures.</p>

                <div class="highlight-box">
                    <h4><i class="fas fa-graduation-cap me-2"></i>Recommended Resources</h4>
                    <ul>
                        <li><strong>Official TensorFlow Tutorials:</strong> tensorflow.org/tutorials (comprehensive, up-to-date)</li>
                        <li><strong>TensorFlow Datasets:</strong> Hundreds of ready-to-use datasets for practice</li>
                        <li><strong>TensorFlow Hub:</strong> Pretrained models for transfer learning</li>
                        <li><strong>Kaggle Competitions:</strong> Apply skills to real-world problems with community feedback</li>
                        <li><strong>Papers with Code:</strong> Reproduce state-of-the-art research with code examples</li>
                    </ul>
                </div>

                <!-- Supporting Sections -->
                <h2 id="glossary">Key Terms Glossary</h2>

                <div class="highlight-box">
                    <ul>
                        <li><strong>Tensor:</strong> n-dimensional array; fundamental unit of data in TensorFlow</li>
                        <li><strong>Eager Execution:</strong> Immediate operation evaluation (default in TF 2); simplifies debugging</li>
                        <li><strong>Layer:</strong> Building block transforming inputs (Dense, Conv2D, LSTM)</li>
                        <li><strong>Model:</strong> Composition of layers; built via Sequential, Functional, or Subclassing APIs</li>
                        <li><strong>Optimizer:</strong> Algorithm updating weights to minimize loss (Adam, SGD, AdamW)</li>
                        <li><strong>Loss Function:</strong> Objective to minimize; measures prediction error</li>
                        <li><strong>Callback:</strong> Training hooks for monitoring/modifying behavior (EarlyStopping, ModelCheckpoint)</li>
                        <li><strong>GradientTape:</strong> Records operations for automatic differentiation (backpropagation)</li>
                        <li><strong>tf.data:</strong> API for building efficient input pipelines (batch, shuffle, prefetch)</li>
                        <li><strong>Transfer Learning:</strong> Reusing pretrained model knowledge for new tasks</li>
                        <li><strong>Embedding:</strong> Mapping discrete tokens to dense continuous vectors</li>
                        <li><strong>SavedModel:</strong> TensorFlow's serialization format for deployment</li>
                        <li><strong>Mixed Precision:</strong> Training with float16 for speed while maintaining float32 stability</li>
                        <li><strong>TensorBoard:</strong> Visualization toolkit for metrics, graphs, and profiling</li>
                    </ul>
                </div>

                <h2 id="pitfalls">Common Pitfalls Reference</h2>

                <div class="highlight-box">
                    <h4><i class="fas fa-exclamation-triangle me-2"></i>Watch Out For:</h4>
                    <ul>
                        <li><strong>Shape Mismatches:</strong> Verify batch dimensions match between data and model inputs</li>
                        <li><strong>from_logits Confusion:</strong> Use <code>from_logits=True</code> when output layer has no activation</li>
                        <li><strong>No Data Shuffling:</strong> Always shuffle training data (except time series)</li>
                        <li><strong>Overfitting:</strong> Monitor validation loss; apply regularization, dropout, or early stopping</li>
                        <li><strong>Learning Rate Too High:</strong> Loss explodes or NaN; reduce LR by 10× and retry</li>
                        <li><strong>No Validation Set:</strong> Can't detect overfitting without validation monitoring</li>
                        <li><strong>Forgetting Training Mode:</strong> Set <code>training=True</code> in custom loops for dropout/batch norm</li>
                        <li><strong>Not Normalizing Data:</strong> Scale inputs to [0,1] or standardize for faster convergence</li>
                        <li><strong>Wrong Loss Function:</strong> Binary vs categorical crossentropy; MSE vs MAE for regression</li>
                        <li><strong>No Checkpointing:</strong> Long training crashes without save—use ModelCheckpoint</li>
                    </ul>
                </div>

                <h2 id="quick-reference">Quick Reference Cheat Sheet</h2>

                <div class="highlight-box">
                    <h4><i class="fas fa-book me-2"></i>Essential Code Patterns</h4>
                    
                    <p><strong>Build Sequential Model:</strong></p>
                    <pre><code class="language-python">model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(20,)),
    layers.Dropout(0.3),
    layers.Dense(10, activation='softmax')
])</code></pre>

                    <p><strong>Compile & Train:</strong></p>
                    <pre><code class="language-python">model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[...])</code></pre>

                    <p><strong>Create tf.data Pipeline:</strong></p>
                    <pre><code class="language-python">dataset = tf.data.Dataset.from_tensor_slices((X, y))
dataset = dataset.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)</code></pre>

                    <p><strong>Custom Training Loop:</strong></p>
                    <pre><code class="language-python">with tf.GradientTape() as tape:
    predictions = model(x_batch, training=True)
    loss = loss_fn(y_batch, predictions)
grads = tape.gradient(loss, model.trainable_variables)
optimizer.apply_gradients(zip(grads, model.trainable_variables))</code></pre>

                    <p><strong>Save & Load:</strong></p>
                    <pre><code class="language-python">model.save('model.keras')
loaded = keras.models.load_model('model.keras')</code></pre>

                    <p><strong>Common Callbacks:</strong></p>
                    <pre><code class="language-python">callbacks = [
    keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),
    keras.callbacks.ModelCheckpoint('best.keras', save_best_only=True),
    keras.callbacks.TensorBoard(log_dir='logs')
]</code></pre>
                </div>

                <div class="highlight-box">
                    <h4><i class="fas fa-fire me-2"></i>Activation Functions</h4>
                    <ul>
                        <li><strong>ReLU:</strong> Fast default for hidden layers; watch dead neurons</li>
                        <li><strong>Sigmoid/Tanh:</strong> Use in gates or bounded outputs; can saturate</li>
                        <li><strong>Softmax:</strong> Converts logits to probabilities for multi-class classification</li>
                        <li><strong>Leaky ReLU:</strong> Fixes dead neurons with small negative slope</li>
                    </ul>
                </div>

                <div class="highlight-box">
                    <h4><i class="fas fa-cogs me-2"></i>Optimizer Comparison</h4>
                    <ul>
                        <li><strong>Adam:</strong> Good default; adaptive learning rates</li>
                        <li><strong>SGD + Momentum:</strong> Stable and simple; try with proper LR schedule</li>
                        <li><strong>AdamW:</strong> Decoupled weight decay; helpful for Transformers</li>
                        <li><strong>RMSprop:</strong> Good for recurrent networks (LSTMs, GRUs)</li>
                    </ul>
                </div>
            </div>

            <!-- Related Posts -->
            <div class="related-posts">
                <h3><i class="fas fa-book me-2"></i>Related Articles in This Series</h3>
                <div class="related-post-item">
                    <h5 class="mb-2">PyTorch Deep Learning: Complete Beginner's Guide to Building Neural Networks</h5>
                    <p class="text-muted small mb-2">Master PyTorch for deep learning with this comprehensive guide. Learn tensor operations, autograd, model building, and training workflows.</p>
                    <a href="pytorch-deep-learning-guide.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                </div>
                <div class="related-post-item">
                    <h5 class="mb-2">Part 4: Machine Learning with Scikit-learn</h5>
                    <p class="text-muted small mb-2">Build and evaluate machine learning models using Scikit-learn. Learn classification, regression, clustering, and best practices for real-world projects.</p>
                    <a href="../12/python-data-science-machine-learning.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                </div>
                <div class="related-post-item">
                    <h5 class="mb-2">Part 1: NumPy Foundations for Data Science</h5>
                    <p class="text-muted small mb-2">Master NumPy arrays, vectorization, broadcasting, and linear algebra operations—the foundation of Python data science.</p>
                    <a href="../12/python-data-science-numpy-foundations.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                </div>
            </div>

                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">
                        I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.
                    </p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook">
                            <i class="fab fa-facebook-f"></i>
                        </a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter">
                            <i class="fab fa-twitter"></i>
                        </a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn">
                            <i class="fab fa-linkedin-in"></i>
                        </a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube">
                            <i class="fab fa-youtube"></i>
                        </a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram">
                            <i class="fab fa-instagram"></i>
                        </a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest">
                            <i class="fab fa-pinterest-p"></i>
                        </a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email">
                            <i class="fas fa-envelope"></i>
                        </a>
                    </div>
                </div>
            </div>

            <hr class="bg-secondary">

            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small mb-2">
                        <i class="fas fa-camera me-2"></i>Background photo by Max Andrey from <a href="https://www.pexels.com/" target="_blank" class="text-light">Pexels</a>
                    </p>
                    <p class="small">
                        <i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a>
                    </p>
                    <p class="small mt-3">
                        <a href="/" class="text-light text-decoration-none">Home</a> | 
                        <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | 
                        <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a>
                    </p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">
                        Updated by <strong>Wasil Zafar</strong> | <time>January 1, 2026</time>
                    </p>
                </div>
            </div>
        </div>
    </footer>

    <!-- Scroll-to-Top Button -->
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top">
        <i class="fas fa-arrow-up"></i>
    </button>

    <!-- Bootstrap 5 JS Bundle -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Prism.js for Syntax Highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-json.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <!-- Prism Theme Switcher -->
    <script>
        const themes = {
            'prism-theme': 'Tomorrow Night',
            'prism-default': 'Default',
            'prism-dark': 'Dark',
            'prism-twilight': 'Twilight',
            'prism-okaidia': 'Okaidia',
            'prism-solarizedlight': 'Solarized Light'
        };

        const savedTheme = localStorage.getItem('prism-theme') || 'prism-theme';

        function switchTheme(themeId) {
            Object.keys(themes).forEach(id => {
                const link = document.getElementById(id);
                if (link) {
                    link.disabled = true;
                }
            });
            
            const selectedLink = document.getElementById(themeId);
            if (selectedLink) {
                selectedLink.disabled = false;
                localStorage.setItem('prism-theme', themeId);
            }

            document.querySelectorAll('div.code-toolbar select').forEach(dropdown => {
                dropdown.value = themeId;
            });

            setTimeout(() => {
                Prism.highlightAll();
            }, 10);
        }

        document.addEventListener('DOMContentLoaded', function() {
            switchTheme(savedTheme);
        });

        Prism.plugins.toolbar.registerButton('theme-switcher', function(env) {
            const select = document.createElement('select');
            select.setAttribute('aria-label', 'Select code theme');
            select.className = 'prism-theme-selector';
            
            Object.keys(themes).forEach(themeId => {
                const option = document.createElement('option');
                option.value = themeId;
                option.textContent = themes[themeId];
                if (themeId === savedTheme) {
                    option.selected = true;
                }
                select.appendChild(option);
            });
            
            select.addEventListener('change', function(e) {
                switchTheme(e.target.value);
            });
            
            return select;
        });
    </script>

    <!-- Scroll-to-Top Script -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const scrollToTopBtn = document.getElementById('scrollToTop');
            
            window.addEventListener('scroll', function() {
                if (window.scrollY > 300) {
                    scrollToTopBtn.classList.add('show');
                } else {
                    scrollToTopBtn.classList.remove('show');
                }
            });
            
            scrollToTopBtn.addEventListener('click', function() {
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        });
    </script>

    <!-- Custom Scripts -->
    <script src="../../../js/main.js"></script>
    <script src="../../../js/cookie-consent.js"></script>
</body>
</html>
