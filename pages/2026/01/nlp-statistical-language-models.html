<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Part 5 of the Complete NLP Series: Master statistical language models, N-gram models, smoothing techniques (Laplace, Kneser-Ney), perplexity evaluation, and the foundations of sequence modeling." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="NLP, Language Models, N-grams, Statistical NLP, Smoothing, Perplexity, Markov Models, Bigrams, Trigrams, Probability" />
    <meta property="og:title" content="Statistical Language Models & N-grams - Complete NLP Series Part 5" />
    <meta property="og:description" content="Understand probabilistic models of language, N-gram modeling, smoothing techniques, and sequence prediction." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-27" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>Statistical Language Models & N-grams - Complete NLP Series Part 5 - Wasil Zafar</title>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('consent', 'default', { 'ad_storage': 'denied', 'ad_user_data': 'denied', 'ad_personalization': 'denied', 'analytics_storage': 'denied', 'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE'] });
        gtag('consent', 'default', { 'ad_storage': 'granted', 'ad_user_data': 'granted', 'ad_personalization': 'granted', 'analytics_storage': 'granted' });
        gtag('set', 'url_passthrough', true);
    </script>
    <script>
        (function(w, d, s, l, i) { w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' }); var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f); })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    <style>
        .blog-hero { background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%); color: white; padding: 80px 0; }
        .blog-meta { font-size: 0.95rem; color: var(--color-teal); margin-bottom: 1rem; display: flex; align-items: center; flex-wrap: wrap; gap: 1rem; }
        .print-btn { background: var(--color-teal); color: white; border: none; padding: 0.4rem 1rem; border-radius: 4px; font-size: 0.9rem; cursor: pointer; transition: all 0.3s ease; display: inline-flex; align-items: center; gap: 0.5rem; }
        .print-btn:hover { background: var(--color-crimson); transform: translateY(-1px); }
        @media print { .print-btn, nav, .navbar, footer, .back-link, .related-posts, .scroll-to-top, .toc-toggle-btn, .sidenav-toc, .sidenav-overlay { display: none !important; } * { -webkit-print-color-adjust: exact !important; print-color-adjust: exact !important; } }
        .blog-content { max-width: 900px; margin: 0 auto; font-size: 1.05rem; line-height: 1.8; color: #333; }
        .blog-content h2 { font-size: 1.8rem; font-weight: 700; margin-top: 2.5rem; margin-bottom: 1.5rem; color: var(--color-navy); border-bottom: 3px solid var(--color-teal); padding-bottom: 0.5rem; }
        .blog-content h3 { font-size: 1.3rem; font-weight: 600; margin-top: 2rem; margin-bottom: 1rem; color: var(--color-blue); }
        .blog-content h4 { font-size: 1.1rem; font-weight: 600; margin-top: 1.5rem; margin-bottom: 1rem; color: var(--color-teal); }
        .blog-content p { margin-bottom: 1.2rem; text-align: justify; }
        .blog-content strong { color: var(--color-crimson); }
        .blog-content pre[class*="language-"] { border-radius: 6px; margin: 1.5rem 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); }
        .blog-content p code, .blog-content li code { background: rgba(59, 151, 151, 0.1); color: var(--color-crimson); padding: 0.2rem 0.4rem; border-radius: 3px; font-family: 'Consolas', monospace; font-size: 0.9em; }
        .highlight-box { background: rgba(59, 151, 151, 0.1); border-left: 4px solid var(--color-teal); padding: 1.5rem; margin: 2rem 0; border-radius: 4px; }
        .experiment-card { background: #f8f9fa; border: 1px solid #ddd; border-radius: 8px; padding: 1.5rem; margin-bottom: 1.5rem; transition: all 0.3s ease; }
        .experiment-card:hover { box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1); transform: translateY(-2px); }
        .experiment-card h4 { color: var(--color-crimson); font-weight: 700; margin-bottom: 0.5rem; }
        .bg-teal { background-color: var(--color-teal) !important; }
        .bg-crimson { background-color: var(--color-crimson) !important; }
        .toc-toggle-btn { position: fixed; bottom: 2rem; left: 2rem; width: 50px; height: 50px; background: var(--color-teal); color: white; border: none; border-radius: 50%; font-size: 1.2rem; cursor: pointer; box-shadow: 0 4px 12px rgba(59, 151, 151, 0.4); transition: all 0.3s ease; z-index: 1049; display: flex; align-items: center; justify-content: center; }
        .toc-toggle-btn:hover { background: var(--color-crimson); transform: scale(1.1); }
        .sidenav-toc { height: calc(100% - 64px); width: 0; position: fixed; z-index: 1050; top: 64px; left: 0; background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%); overflow-x: hidden; overflow-y: auto; transition: width 0.4s ease; padding-top: 30px; box-shadow: 4px 0 15px rgba(0, 0, 0, 0.3); }
        .sidenav-toc.open { width: 350px; }
        .sidenav-toc .toc-header { display: flex; align-items: center; justify-content: space-between; padding: 20px 30px; margin-bottom: 20px; border-bottom: 2px solid var(--color-teal); opacity: 0; visibility: hidden; transition: all 0.3s ease; }
        .sidenav-toc.open .toc-header { opacity: 1; visibility: visible; }
        .sidenav-toc .closebtn { font-size: 32px; color: white; background: transparent; border: none; cursor: pointer; transition: all 0.3s ease; }
        .sidenav-toc .closebtn:hover { color: var(--color-crimson); transform: rotate(90deg); }
        .sidenav-toc h3 { color: white; margin: 0; font-weight: 700; font-size: 1.3rem; }
        .sidenav-toc ol { list-style: decimal; padding-left: 30px; margin: 0; color: rgba(255, 255, 255, 0.9); }
        .sidenav-toc ol li { margin-bottom: 8px; }
        .sidenav-toc ul { list-style-type: lower-alpha; padding-left: 30px; margin-top: 8px; }
        .sidenav-toc a { padding: 12px 30px; text-decoration: none; font-size: 0.95rem; color: rgba(255, 255, 255, 0.85); display: block; transition: all 0.3s ease; border-left: 4px solid transparent; }
        .sidenav-toc a:hover { color: white; background: rgba(59, 151, 151, 0.2); border-left-color: var(--color-teal); }
        .sidenav-toc a.active { color: white; background: rgba(191, 9, 47, 0.3); border-left-color: var(--color-crimson); font-weight: 600; }
        .sidenav-overlay { display: none; position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0, 0, 0, 0.5); z-index: 1049; }
        .sidenav-overlay.show { display: block; }
        .reading-time { display: inline-block; background: var(--color-crimson); color: white; padding: 0.3rem 0.8rem; border-radius: 4px; font-size: 0.9rem; }
        .back-link { display: inline-block; color: white; text-decoration: none; transition: all 0.3s ease; margin-bottom: 1rem; opacity: 0.9; }
        .back-link:hover { color: var(--color-teal); transform: translateX(-5px); }
        .related-posts { background: #f8f9fa; border-radius: 8px; padding: 2rem; margin-top: 3rem; }
        .related-posts h3 { color: var(--color-navy); margin-bottom: 1.5rem; }
        .related-post-item { padding: 1rem; border-left: 3px solid var(--color-teal); margin-bottom: 1rem; transition: all 0.3s ease; }
        .related-post-item:hover { background: white; border-left-color: var(--color-crimson); }
        .related-post-item a { color: var(--color-blue); text-decoration: none; font-weight: 600; }
        .related-post-item a:hover { color: var(--color-crimson); }
        div.code-toolbar > .toolbar { opacity: 1; display: flex; gap: 0.5rem; }
        div.code-toolbar > .toolbar > .toolbar-item > button { background: var(--color-teal); color: white; border: none; padding: 0.4rem 0.8rem; border-radius: 4px; font-size: 0.85rem; cursor: pointer; }
        div.code-toolbar > .toolbar > .toolbar-item > select { background: var(--color-navy); color: white; border: 1px solid var(--color-teal); padding: 0.4rem 0.8rem; border-radius: 4px; font-size: 0.85rem; }
        .scroll-to-top { position: fixed; bottom: 2rem; right: 2rem; width: 50px; height: 50px; background: var(--color-teal); color: white; border: none; border-radius: 50%; font-size: 1.2rem; cursor: pointer; display: flex; align-items: center; justify-content: center; opacity: 0; visibility: hidden; transition: all 0.3s ease; box-shadow: 0 4px 12px rgba(59, 151, 151, 0.3); z-index: 999; }
        .scroll-to-top.show { opacity: 1; visibility: visible; }
        .scroll-to-top:hover { background: var(--color-crimson); transform: translateY(-3px); }
        @media (max-width: 768px) { .sidenav-toc.open { width: 280px; } .toc-toggle-btn { left: 15px; } }
        html { scroll-behavior: smooth; }
    </style>
</head>
<body>
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/"><span class="gradient-text">Wasil Zafar</span></a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link" href="/">Home</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#about">About</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#certifications">Certifications</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#interests">Interests</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link"><i class="fas fa-arrow-left me-2"></i>Back to Technology</a>
                <h1 class="display-4 fw-bold mb-3">Statistical Language Models & N-grams</h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 27, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-1"></i>30 min read</span>
                    <button onclick="window.print()" class="print-btn" title="Print this article"><i class="fas fa-print"></i> Print</button>
                </div>
                <p class="lead">Part 5 of 16: Understand probabilistic models of language, N-gram modeling, smoothing techniques, and sequence prediction.</p>
            </div>
        </div>
    </section>

    <button class="toc-toggle-btn" onclick="openNav()" title="Table of Contents" aria-label="Open Table of Contents"><i class="fas fa-list"></i></button>

    <div id="tocSidenav" class="sidenav-toc">
        <div class="toc-header">
            <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
            <button class="closebtn" onclick="closeNav()" aria-label="Close">&times;</button>
        </div>
        <ol>
            <li><a href="#introduction" onclick="closeNav()">Introduction to Language Models</a></li>
            <li><a href="#probability" onclick="closeNav()">Probability & Language</a></li>
            <li><a href="#ngrams" onclick="closeNav()">N-gram Models</a>
                <ul>
                    <li><a href="#unigrams" onclick="closeNav()">Unigrams</a></li>
                    <li><a href="#bigrams" onclick="closeNav()">Bigrams</a></li>
                    <li><a href="#trigrams" onclick="closeNav()">Trigrams & Higher-Order</a></li>
                </ul>
            </li>
            <li><a href="#smoothing" onclick="closeNav()">Smoothing Techniques</a>
                <ul>
                    <li><a href="#laplace" onclick="closeNav()">Laplace (Add-One) Smoothing</a></li>
                    <li><a href="#kneser-ney" onclick="closeNav()">Kneser-Ney Smoothing</a></li>
                    <li><a href="#interpolation" onclick="closeNav()">Interpolation & Backoff</a></li>
                </ul>
            </li>
            <li><a href="#perplexity" onclick="closeNav()">Perplexity & Evaluation</a></li>
            <li><a href="#applications" onclick="closeNav()">Applications</a></li>
            <li><a href="#limitations" onclick="closeNav()">Limitations & Transition to Neural</a></li>
            <li><a href="#conclusion" onclick="closeNav()">Conclusion & Next Steps</a></li>
        </ol>
    </div>

    <div id="tocOverlay" class="sidenav-overlay" onclick="closeNav()"></div>

    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="blog-content">
                        
                        <h2 id="introduction"><i class="fas fa-chart-bar me-2"></i>Introduction to Language Models</h2>
                        
                        <p>Statistical language models assign probabilities to sequences of words, enabling prediction of the next word given context. This foundational concept underlies everything from autocomplete to modern transformers.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-star me-2"></i>Key Insight</h4>
                            <p><strong>Language models learn to predict P(w|context)—the probability of a word given its context. N-gram models approximate this using fixed-length history.</strong></p>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-map-signs me-2"></i>Complete NLP Series Navigation</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">16-Part Series</span>
                                <span class="badge bg-crimson">NLP Mastery</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><a href="nlp-fundamentals-linguistic-basics.html">NLP Fundamentals & Linguistic Basics</a></li>
                                    <li><a href="nlp-tokenization-text-cleaning.html">Tokenization & Text Cleaning</a></li>
                                    <li><a href="nlp-text-representation-features.html">Text Representation & Feature Engineering</a></li>
                                    <li><a href="nlp-word-embeddings.html">Word Embeddings</a></li>
                                    <li><strong>Statistical Language Models & N-grams (This Guide)</strong></li>
                                    <li><a href="nlp-neural-networks.html">Neural Networks for NLP</a></li>
                                    <li><a href="nlp-rnn-lstm-gru.html">RNNs, LSTMs & GRUs</a></li>
                                    <li><a href="nlp-transformers-attention.html">Transformers & Attention Mechanism</a></li>
                                    <li><a href="nlp-pretrained-models-transfer-learning.html">Pretrained Language Models & Transfer Learning</a></li>
                                    <li><a href="nlp-gpt-text-generation.html">GPT Models & Text Generation</a></li>
                                    <li><a href="nlp-core-tasks.html">Core NLP Tasks</a></li>
                                    <li><a href="nlp-advanced-tasks.html">Advanced NLP Tasks</a></li>
                                    <li><a href="nlp-multilingual-crosslingual.html">Multilingual & Cross-lingual NLP</a></li>
                                    <li><a href="nlp-evaluation-ethics.html">Evaluation, Ethics & Responsible NLP</a></li>
                                    <li><a href="nlp-systems-production.html">NLP Systems, Optimization & Production</a></li>
                                    <li><a href="nlp-cutting-edge-research.html">Cutting-Edge & Research Topics</a></li>
                                </ol>
                            </div>
                        </div>

                        <h2 id="probability"><i class="fas fa-percentage me-2"></i>Probability & Language</h2>
                        
                        <p>At the heart of statistical language models lies the fundamental question: <strong>"How likely is this sequence of words?"</strong> Language models assign probabilities to sequences of words, enabling us to determine that "The cat sat on the mat" is more probable than "Mat the on sat cat the." This probabilistic framework forms the foundation for countless NLP applications, from speech recognition to machine translation.</p>

                        <p>The probability of a sentence can be decomposed using the <strong>Chain Rule of Probability</strong>. For a sentence W = w₁, w₂, ..., wₙ, we calculate:</p>

                        <p style="text-align: center; font-size: 1.1rem;"><strong>P(W) = P(w₁) × P(w₂|w₁) × P(w₃|w₁,w₂) × ... × P(wₙ|w₁,...,wₙ₋₁)</strong></p>

                        <p>This decomposition means the probability of each word depends on all preceding words. While mathematically precise, estimating these conditional probabilities becomes impractical as the history grows—the number of possible histories grows exponentially with sentence length.</p>

<pre><code class="language-python">import numpy as np
from collections import defaultdict
import random

# Simple probability demonstration with toy corpus
corpus = [
    "the cat sat on the mat",
    "the dog sat on the floor",
    "the cat ate the fish",
    "the dog ate the bone",
    "the bird sat on the tree"
]

# Count word occurrences
word_counts = defaultdict(int)
total_words = 0

for sentence in corpus:
    words = sentence.split()
    for word in words:
        word_counts[word] += 1
        total_words += 1

# Calculate P(word) - unigram probabilities
print("Unigram Probabilities (Maximum Likelihood Estimation):")
print("="*50)
for word, count in sorted(word_counts.items(), key=lambda x: -x[1]):
    probability = count / total_words
    print(f"P({word:8}) = {count}/{total_words} = {probability:.4f}")

print(f"\nTotal unique words: {len(word_counts)}")
print(f"Total word tokens: {total_words}")</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>The Markov Assumption</h4>
                            <p>N-gram models make a simplifying assumption called the <strong>Markov assumption</strong>: the probability of a word depends only on the previous n-1 words, not the entire history. This makes estimation tractable while still capturing local context:</p>
                            <p style="text-align: center;"><strong>P(wₙ|w₁,...,wₙ₋₁) ≈ P(wₙ|wₙ₋ₖ,...,wₙ₋₁)</strong> for k-gram model</p>
                        </div>

<pre><code class="language-python">import numpy as np
from collections import defaultdict

# Demonstrate Chain Rule decomposition
sentence = "the cat sat on the mat"
words = sentence.split()

print("Chain Rule Decomposition:")
print("="*60)
print(f"\nSentence: '{sentence}'")
print(f"\nP(sentence) = ", end="")

for i, word in enumerate(words):
    if i == 0:
        print(f"P({word})", end="")
    else:
        history = " ".join(words[:i])
        print(f" × P({word}|{history})", end="")

print("\n\n" + "="*60)
print("\nWith Bigram (Markov) Assumption:")
print(f"\nP(sentence) ≈ ", end="")

for i, word in enumerate(words):
    if i == 0:
        print(f"P({word})", end="")
    else:
        print(f" × P({word}|{words[i-1]})", end="")

print("\n")
print("\nNotice: Each probability now depends only on the previous word!")</code></pre>

                        <h2 id="ngrams"><i class="fas fa-layer-group me-2"></i>N-gram Models</h2>

                        <p>N-gram models predict the probability of a word based on the previous (n-1) words. The "n" in n-gram refers to the total number of words in the sequence being considered. These models trade off context length against data sparsity—longer contexts capture more information but require exponentially more training data.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-info-circle me-2"></i>N-gram Terminology</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Definitions</span>
                                <span class="badge bg-crimson">Core Concepts</span>
                            </div>
                            <div class="content">
                                <ul>
                                    <li><strong>Unigram (n=1):</strong> Single words, no context - P(word)</li>
                                    <li><strong>Bigram (n=2):</strong> Word pairs - P(word|previous_word)</li>
                                    <li><strong>Trigram (n=3):</strong> Three-word sequences - P(word|prev_2_words)</li>
                                    <li><strong>4-gram, 5-gram:</strong> Longer contexts, rarer in practice due to sparsity</li>
                                </ul>
                            </div>
                        </div>

                        <h3 id="unigrams">Unigrams</h3>
                        <p>A <strong>unigram model</strong> treats each word independently, ignoring all context. While simple, it captures word frequency distributions and serves as a baseline. The probability of a sentence is simply the product of individual word probabilities:</p>

                        <p style="text-align: center;"><strong>P(w₁, w₂, ..., wₙ) = P(w₁) × P(w₂) × ... × P(wₙ)</strong></p>

<pre><code class="language-python">import numpy as np
from collections import Counter

# Training corpus
corpus = [
    "I love natural language processing",
    "Natural language processing is fascinating",
    "I love machine learning too",
    "Language models are powerful tools",
    "I am learning about language models"
]

class UnigramModel:
    def __init__(self):
        self.word_counts = Counter()
        self.total_words = 0
        
    def train(self, corpus):
        """Train unigram model on corpus"""
        for sentence in corpus:
            words = sentence.lower().split()
            self.word_counts.update(words)
            self.total_words += len(words)
    
    def probability(self, word):
        """P(word) - unigram probability"""
        word = word.lower()
        return self.word_counts[word] / self.total_words
    
    def sentence_probability(self, sentence):
        """P(sentence) using unigram assumption"""
        words = sentence.lower().split()
        prob = 1.0
        for word in words:
            prob *= self.probability(word)
        return prob

# Train and test
unigram = UnigramModel()
unigram.train(corpus)

print("Unigram Model - Word Probabilities:")
print("="*50)
for word, count in unigram.word_counts.most_common(10):
    prob = unigram.probability(word)
    print(f"P({word:15}) = {count:2}/{unigram.total_words} = {prob:.4f}")

# Test sentences
test_sentences = [
    "I love language",
    "machine language processing",
    "xyz unknown words"
]

print("\nSentence Probabilities:")
print("="*50)
for sent in test_sentences:
    prob = unigram.sentence_probability(sent)
    print(f"P('{sent}') = {prob:.10f}")</code></pre>

                        <h3 id="bigrams">Bigrams</h3>
                        <p>A <strong>bigram model</strong> conditions each word on the immediately preceding word, capturing local word patterns. This simple extension dramatically improves over unigrams by modeling word co-occurrences like "New York" or "machine learning."</p>

                        <p style="text-align: center;"><strong>P(wₙ|w₁,...,wₙ₋₁) ≈ P(wₙ|wₙ₋₁)</strong></p>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter

class BigramModel:
    def __init__(self):
        self.bigram_counts = defaultdict(Counter)
        self.unigram_counts = Counter()
        self.vocab = set()
        
    def train(self, corpus):
        """Train bigram model with start/end tokens"""
        for sentence in corpus:
            # Add start and end tokens
            words = ['<s>'] + sentence.lower().split() + ['</s>']
            self.vocab.update(words)
            
            # Count unigrams and bigrams
            for i in range(len(words) - 1):
                self.unigram_counts[words[i]] += 1
                self.bigram_counts[words[i]][words[i+1]] += 1
            self.unigram_counts[words[-1]] += 1
    
    def probability(self, word, previous):
        """P(word|previous) using MLE"""
        word = word.lower()
        previous = previous.lower()
        
        if self.unigram_counts[previous] == 0:
            return 0.0
        return self.bigram_counts[previous][word] / self.unigram_counts[previous]
    
    def sentence_probability(self, sentence):
        """Calculate sentence probability using bigram model"""
        words = ['<s>'] + sentence.lower().split() + ['</s>']
        prob = 1.0
        
        for i in range(1, len(words)):
            p = self.probability(words[i], words[i-1])
            prob *= p
        return prob

# Training corpus
corpus = [
    "I love natural language processing",
    "I love machine learning",
    "Natural language processing is fun",
    "Machine learning is powerful",
    "I enjoy learning new things"
]

# Train model
bigram = BigramModel()
bigram.train(corpus)

print("Bigram Probabilities after 'I':")
print("="*50)
for next_word, count in bigram.bigram_counts['i'].most_common():
    prob = bigram.probability(next_word, 'i')
    print(f"P({next_word:15}|I) = {count}/{bigram.unigram_counts['i']} = {prob:.4f}")

print("\nBigram Probabilities after 'natural':")
print("="*50)
for next_word, count in bigram.bigram_counts['natural'].most_common():
    prob = bigram.probability(next_word, 'natural')
    print(f"P({next_word:15}|natural) = {count}/{bigram.unigram_counts['natural']} = {prob:.4f}")

# Test sentences
print("\nSentence Probabilities:")
print("="*50)
test_sentences = ["I love learning", "I love processing", "natural language"]
for sent in test_sentences:
    prob = bigram.sentence_probability(sent)
    print(f"P('{sent}') = {prob:.6f}")</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-calculator me-2"></i>Maximum Likelihood Estimation (MLE)</h4>
                            <p>The standard method for estimating n-gram probabilities is <strong>Maximum Likelihood Estimation</strong>:</p>
                            <p style="text-align: center;"><strong>P_MLE(wₙ|wₙ₋₁) = Count(wₙ₋₁, wₙ) / Count(wₙ₋₁)</strong></p>
                            <p>MLE simply counts co-occurrences and normalizes. While intuitive, it assigns zero probability to unseen n-grams—a critical flaw we'll address with smoothing.</p>
                        </div>

                        <h3 id="trigrams">Trigrams & Higher-Order</h3>
                        <p><strong>Trigram models</strong> condition on the previous two words, capturing longer-range dependencies. They model patterns like "New York City" or "to be or." Higher-order models (4-gram, 5-gram) capture even more context but suffer from severe data sparsity—most possible sequences never appear in training data.</p>

                        <p style="text-align: center;"><strong>P(wₙ|w₁,...,wₙ₋₁) ≈ P(wₙ|wₙ₋₂, wₙ₋₁)</strong></p>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter

class TrigramModel:
    def __init__(self):
        self.trigram_counts = defaultdict(Counter)
        self.bigram_counts = defaultdict(int)
        self.vocab = set()
        
    def train(self, corpus):
        """Train trigram model"""
        for sentence in corpus:
            words = ['<s>', '<s>'] + sentence.lower().split() + ['</s>']
            self.vocab.update(words)
            
            for i in range(2, len(words)):
                context = (words[i-2], words[i-1])
                self.bigram_counts[context] += 1
                self.trigram_counts[context][words[i]] += 1
    
    def probability(self, word, context):
        """P(word|prev2, prev1) using MLE"""
        word = word.lower()
        context = tuple(w.lower() for w in context)
        
        if self.bigram_counts[context] == 0:
            return 0.0
        return self.trigram_counts[context][word] / self.bigram_counts[context]
    
    def sentence_probability(self, sentence):
        """Calculate sentence probability using trigram model"""
        words = ['<s>', '<s>'] + sentence.lower().split() + ['</s>']
        prob = 1.0
        
        for i in range(2, len(words)):
            context = (words[i-2], words[i-1])
            p = self.probability(words[i], context)
            prob *= p
        return prob
    
    def predict_next(self, prev2, prev1, top_k=5):
        """Predict next word given context"""
        context = (prev2.lower(), prev1.lower())
        if context not in self.trigram_counts:
            return []
        
        predictions = self.trigram_counts[context].most_common(top_k)
        total = self.bigram_counts[context]
        return [(word, count/total) for word, count in predictions]

# Larger training corpus
corpus = [
    "I want to learn natural language processing",
    "I want to learn machine learning",
    "I want to eat pizza today",
    "She wants to learn programming",
    "We want to build language models",
    "They want to understand NLP",
    "I need to learn more",
    "We need to process text"
]

# Train model
trigram = TrigramModel()
trigram.train(corpus)

print("Trigram Predictions after 'I want':")
print("="*50)
predictions = trigram.predict_next('I', 'want', top_k=5)
for word, prob in predictions:
    print(f"P({word:15}|I, want) = {prob:.4f}")

print("\nTrigram Predictions after 'want to':")
print("="*50)
predictions = trigram.predict_next('want', 'to', top_k=5)
for word, prob in predictions:
    print(f"P({word:15}|want, to) = {prob:.4f}")

print("\nThe Sparsity Problem:")
print("="*50)
test_context = ('want', 'more')
print(f"P(food|want, more) = {trigram.probability('food', test_context):.4f}")
print("Zero probability! This context never appeared in training.")</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-chart-line me-2"></i>N-gram Trade-offs</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Analysis</span>
                                <span class="badge bg-crimson">Key Insight</span>
                            </div>
                            <div class="content">
                                <table class="table table-bordered">
                                    <thead>
                                        <tr><th>N-gram Order</th><th>Context</th><th>Pros</th><th>Cons</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>Unigram</td><td>None</td><td>Dense estimates</td><td>No context</td></tr>
                                        <tr><td>Bigram</td><td>1 word</td><td>Captures local patterns</td><td>Limited context</td></tr>
                                        <tr><td>Trigram</td><td>2 words</td><td>Common in practice</td><td>Sparse estimates</td></tr>
                                        <tr><td>4-gram+</td><td>3+ words</td><td>Long dependencies</td><td>Very sparse, huge storage</td></tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>

                        <h2 id="smoothing"><i class="fas fa-sliders-h me-2"></i>Smoothing Techniques</h2>

                        <p>The fundamental problem with MLE is that <strong>unseen n-grams receive zero probability</strong>. Since any sentence containing an unseen n-gram gets zero probability, our model fails on new, grammatically correct sentences. Smoothing techniques redistribute probability mass from seen to unseen events, ensuring no valid sequence gets zero probability.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-exclamation-triangle me-2"></i>The Zero Probability Problem</h4>
                            <p>Consider training on "The cat sat" and "The dog ran." With MLE:</p>
                            <ul>
                                <li>P(cat|The) = 0.5 ✓</li>
                                <li>P(bird|The) = 0.0 ✗ (Never seen, but perfectly valid!)</li>
                            </ul>
                            <p>Even a single zero in the chain rule makes the entire sentence probability zero. Smoothing fixes this.</p>
                        </div>

                        <h3 id="laplace">Laplace (Add-One) Smoothing</h3>
                        <p><strong>Laplace smoothing</strong> (also called add-one smoothing) is the simplest approach: add 1 to every count, then normalize. This ensures every possible n-gram has at least some probability. While simple, it often transfers too much probability to unseen events.</p>

                        <p style="text-align: center;"><strong>P_Laplace(wₙ|wₙ₋₁) = (Count(wₙ₋₁, wₙ) + 1) / (Count(wₙ₋₁) + V)</strong></p>
                        <p style="text-align: center;"><em>where V = vocabulary size</em></p>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter

class SmoothedBigramModel:
    def __init__(self, smoothing='none', k=1.0):
        self.bigram_counts = defaultdict(Counter)
        self.unigram_counts = Counter()
        self.vocab = set()
        self.smoothing = smoothing
        self.k = k  # smoothing parameter
        
    def train(self, corpus):
        """Train model on corpus"""
        for sentence in corpus:
            words = ['<s>'] + sentence.lower().split() + ['</s>']
            self.vocab.update(words)
            
            for i in range(len(words) - 1):
                self.unigram_counts[words[i]] += 1
                self.bigram_counts[words[i]][words[i+1]] += 1
            self.unigram_counts[words[-1]] += 1
    
    def probability(self, word, previous):
        """Calculate probability with chosen smoothing"""
        word = word.lower()
        previous = previous.lower()
        V = len(self.vocab)
        
        count_bigram = self.bigram_counts[previous][word]
        count_unigram = self.unigram_counts[previous]
        
        if self.smoothing == 'none':
            # MLE - no smoothing
            if count_unigram == 0:
                return 0.0
            return count_bigram / count_unigram
        
        elif self.smoothing == 'laplace':
            # Add-one smoothing
            return (count_bigram + 1) / (count_unigram + V)
        
        elif self.smoothing == 'add-k':
            # Add-k smoothing (k < 1 usually works better)
            return (count_bigram + self.k) / (count_unigram + self.k * V)

# Training corpus
corpus = [
    "the cat sat on the mat",
    "the dog sat on the floor",
    "the cat ate the fish",
    "the dog ate the bone"
]

# Compare smoothing methods
print("Comparing Smoothing Methods:")
print("="*60)

for smoothing in ['none', 'laplace', 'add-k']:
    if smoothing == 'add-k':
        model = SmoothedBigramModel(smoothing=smoothing, k=0.1)
    else:
        model = SmoothedBigramModel(smoothing=smoothing)
    model.train(corpus)
    
    print(f"\n{smoothing.upper()} Smoothing:")
    print("-" * 40)
    
    # Seen bigram
    p_seen = model.probability('cat', 'the')
    print(f"P(cat|the) = {p_seen:.4f}  [SEEN bigram]")
    
    # Unseen bigram
    p_unseen = model.probability('elephant', 'the')
    print(f"P(elephant|the) = {p_unseen:.4f}  [UNSEEN bigram]")
    
    # Another unseen
    p_rare = model.probability('fish', 'sat')
    print(f"P(fish|sat) = {p_rare:.4f}  [UNSEEN bigram]")</code></pre>

                        <p><strong>Add-k smoothing</strong> generalizes Laplace by adding k (where k < 1) instead of 1. This transfers less probability mass to unseen events, often yielding better results:</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict, Counter

# Demonstrate how k affects probability distribution
def compute_probs_for_k(k, count_prev=100, count_bigram=20, vocab_size=1000):
    """Calculate smoothed probability for different k values"""
    p_seen = (count_bigram + k) / (count_prev + k * vocab_size)
    p_unseen = k / (count_prev + k * vocab_size)
    return p_seen, p_unseen

k_values = [0.001, 0.01, 0.1, 0.5, 1.0, 2.0]
print("Effect of k on Probability Distribution:")
print("="*60)
print(f"Context: count(prev) = 100, count(bigram) = 20, V = 1000")
print(f"{'k':>8} | {'P(seen)':>12} | {'P(unseen)':>12} | {'Ratio':>10}")
print("-" * 60)

for k in k_values:
    p_seen, p_unseen = compute_probs_for_k(k)
    ratio = p_seen / p_unseen if p_unseen > 0 else float('inf')
    print(f"{k:>8.3f} | {p_seen:>12.6f} | {p_unseen:>12.6f} | {ratio:>10.1f}")

print("\nNote: Smaller k preserves more distinction between seen/unseen events")</code></pre>

                        <h3 id="kneser-ney">Kneser-Ney Smoothing</h3>
                        <p><strong>Kneser-Ney smoothing</strong> is considered the gold standard for n-gram smoothing. It uses <strong>absolute discounting</strong> (subtracting a fixed amount from each count) combined with a clever <strong>continuation probability</strong> that considers how many different contexts a word appears in, not just raw frequency.</p>

                        <p>The key insight: a word like "Francisco" has high frequency but always follows "San"—it shouldn't get high probability in other contexts. Kneser-Ney's continuation probability captures this:</p>

                        <p style="text-align: center;"><strong>P_continuation(w) ∝ |{v : Count(v, w) > 0}|</strong></p>
                        <p style="text-align: center;"><em>Number of unique contexts word w appears in</em></p>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter

class KneserNeyBigram:
    def __init__(self, discount=0.75):
        self.bigram_counts = defaultdict(Counter)
        self.unigram_counts = Counter()
        self.continuation_counts = Counter()  # How many contexts each word follows
        self.vocab = set()
        self.d = discount
        
    def train(self, corpus):
        """Train with Kneser-Ney smoothing"""
        contexts_for_word = defaultdict(set)
        
        for sentence in corpus:
            words = ['<s>'] + sentence.lower().split() + ['</s>']
            self.vocab.update(words)
            
            for i in range(len(words) - 1):
                prev, word = words[i], words[i+1]
                self.unigram_counts[prev] += 1
                self.bigram_counts[prev][word] += 1
                contexts_for_word[word].add(prev)
        
        # Continuation count = number of unique preceding contexts
        for word, contexts in contexts_for_word.items():
            self.continuation_counts[word] = len(contexts)
        
        self.total_continuations = sum(self.continuation_counts.values())
    
    def probability(self, word, previous):
        """P(word|previous) using Kneser-Ney"""
        word = word.lower()
        previous = previous.lower()
        
        count_bigram = self.bigram_counts[previous][word]
        count_prev = self.unigram_counts[previous]
        
        if count_prev == 0:
            # Fallback to continuation probability
            return self.continuation_counts[word] / max(1, self.total_continuations)
        
        # Number of unique words following 'previous'
        num_following = len(self.bigram_counts[previous])
        
        # Discounted probability + interpolation weight × continuation prob
        first_term = max(count_bigram - self.d, 0) / count_prev
        lambda_weight = (self.d * num_following) / count_prev
        continuation = self.continuation_counts[word] / max(1, self.total_continuations)
        
        return first_term + lambda_weight * continuation

# Corpus with "San Francisco" pattern
corpus = [
    "I live in San Francisco",
    "San Francisco is beautiful",
    "Visit San Francisco today",
    "She moved to San Francisco",
    "The weather in San Francisco is nice",
    "I ate delicious food yesterday",
    "The food was delicious",
    "Delicious food is everywhere"
]

# Compare regular vs Kneser-Ney
print("Kneser-Ney vs Regular Bigram:")
print("="*60)

# Train both models
from collections import defaultdict, Counter

class SimpleBigram:
    def __init__(self):
        self.bigram_counts = defaultdict(Counter)
        self.unigram_counts = Counter()
        
    def train(self, corpus):
        for sentence in corpus:
            words = ['<s>'] + sentence.lower().split() + ['</s>']
            for i in range(len(words) - 1):
                self.unigram_counts[words[i]] += 1
                self.bigram_counts[words[i]][words[i+1]] += 1
    
    def probability(self, word, previous):
        count = self.unigram_counts[previous.lower()]
        if count == 0:
            return 0
        return self.bigram_counts[previous.lower()][word.lower()] / count

simple = SimpleBigram()
simple.train(corpus)

kn = KneserNeyBigram(discount=0.75)
kn.train(corpus)

print("\nWord 'francisco' appears often but ONLY after 'san':")
print(f"  Simple Bigram P(francisco|the) = {simple.probability('francisco', 'the'):.4f}")
print(f"  Kneser-Ney P(francisco|the) = {kn.probability('francisco', 'the'):.4f}")

print("\nWord 'delicious' appears in multiple contexts:")
print(f"  Simple Bigram P(delicious|the) = {simple.probability('delicious', 'the'):.4f}")
print(f"  Kneser-Ney P(delicious|the) = {kn.probability('delicious', 'the'):.4f}")

print("\nContinuation counts:")
print(f"  'francisco' follows {kn.continuation_counts['francisco']} unique context(s)")
print(f"  'delicious' follows {kn.continuation_counts['delicious']} unique context(s)")
print(f"  'food' follows {kn.continuation_counts['food']} unique context(s)")</code></pre>

                        <h3 id="interpolation">Interpolation & Backoff</h3>
                        <p><strong>Interpolation</strong> combines multiple n-gram orders by weighting their probabilities together. This leverages the reliability of lower-order models (which have denser estimates) while capturing the specificity of higher-order models.</p>

                        <p style="text-align: center;"><strong>P_interp(w|w₁w₂) = λ₃P(w|w₁w₂) + λ₂P(w|w₂) + λ₁P(w)</strong></p>
                        <p style="text-align: center;"><em>where λ₁ + λ₂ + λ₃ = 1</em></p>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter

class InterpolatedTrigramModel:
    def __init__(self, lambda1=0.1, lambda2=0.3, lambda3=0.6):
        # Lambdas: unigram, bigram, trigram weights
        self.l1, self.l2, self.l3 = lambda1, lambda2, lambda3
        
        self.unigram_counts = Counter()
        self.bigram_counts = defaultdict(Counter)
        self.trigram_counts = defaultdict(Counter)
        self.total_words = 0
        self.vocab = set()
        
    def train(self, corpus):
        for sentence in corpus:
            words = ['<s>', '<s>'] + sentence.lower().split() + ['</s>']
            self.vocab.update(words)
            self.total_words += len(words)
            
            for i in range(len(words)):
                self.unigram_counts[words[i]] += 1
                
                if i >= 1:
                    self.bigram_counts[words[i-1]][words[i]] += 1
                
                if i >= 2:
                    context = (words[i-2], words[i-1])
                    self.trigram_counts[context][words[i]] += 1
    
    def probability(self, word, prev1=None, prev2=None):
        """Interpolated probability combining uni/bi/trigram"""
        word = word.lower()
        
        # Unigram: P(w)
        p_uni = self.unigram_counts[word] / max(1, self.total_words)
        
        # Bigram: P(w|prev1)
        p_bi = 0.0
        if prev1:
            prev1 = prev1.lower()
            count_prev = sum(self.bigram_counts[prev1].values())
            if count_prev > 0:
                p_bi = self.bigram_counts[prev1][word] / count_prev
        
        # Trigram: P(w|prev2, prev1)
        p_tri = 0.0
        if prev1 and prev2:
            prev2 = prev2.lower()
            context = (prev2, prev1)
            count_context = sum(self.trigram_counts[context].values())
            if count_context > 0:
                p_tri = self.trigram_counts[context][word] / count_context
        
        # Interpolate
        return self.l1 * p_uni + self.l2 * p_bi + self.l3 * p_tri

# Training corpus
corpus = [
    "I want to eat Chinese food",
    "I want to eat Italian food",
    "I want to learn language models",
    "She wants to eat pizza",
    "They want to visit Paris",
    "We want to eat dinner",
    "I like Chinese food",
    "Italian food is delicious"
]

# Train interpolated model
model = InterpolatedTrigramModel(lambda1=0.1, lambda2=0.3, lambda3=0.6)
model.train(corpus)

print("Interpolated Trigram Model:")
print("="*60)
print(f"Weights: λ_unigram={model.l1}, λ_bigram={model.l2}, λ_trigram={model.l3}")

print("\nPredicting word after 'want to':")
print("-" * 40)
candidates = ['eat', 'learn', 'visit', 'sleep', 'run']
for word in candidates:
    p = model.probability(word, prev1='to', prev2='want')
    print(f"P({word:8}|want to) = {p:.4f}")

print("\nPredicting word after 'to eat':")
print("-" * 40)
candidates = ['chinese', 'italian', 'pizza', 'breakfast', 'xyz']
for word in candidates:
    p = model.probability(word, prev1='eat', prev2='to')
    print(f"P({word:10}|to eat) = {p:.4f}")</code></pre>

                        <p><strong>Backoff</strong> is an alternative approach that uses higher-order n-grams when available, "backing off" to lower-order models only when the higher-order count is zero. Unlike interpolation which always combines all orders, backoff is more selective:</p>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter

class StupidBackoffModel:
    """Simple backoff model (Stupid Backoff from Google)"""
    
    def __init__(self, backoff_weight=0.4):
        self.alpha = backoff_weight
        self.unigram_counts = Counter()
        self.bigram_counts = defaultdict(Counter)
        self.trigram_counts = defaultdict(Counter)
        self.total_words = 0
        
    def train(self, corpus):
        for sentence in corpus:
            words = ['<s>', '<s>'] + sentence.lower().split() + ['</s>']
            self.total_words += len(words)
            
            for i in range(len(words)):
                self.unigram_counts[words[i]] += 1
                if i >= 1:
                    self.bigram_counts[words[i-1]][words[i]] += 1
                if i >= 2:
                    context = (words[i-2], words[i-1])
                    self.trigram_counts[context][words[i]] += 1
    
    def score(self, word, prev1=None, prev2=None):
        """Stupid backoff scoring (not true probability)"""
        word = word.lower()
        
        # Try trigram first
        if prev1 and prev2:
            context = (prev2.lower(), prev1.lower())
            if self.trigram_counts[context][word] > 0:
                count_context = sum(self.trigram_counts[context].values())
                return self.trigram_counts[context][word] / count_context, 'trigram'
        
        # Backoff to bigram
        if prev1:
            prev1 = prev1.lower()
            if self.bigram_counts[prev1][word] > 0:
                count_prev = sum(self.bigram_counts[prev1].values())
                return self.alpha * self.bigram_counts[prev1][word] / count_prev, 'bigram'
        
        # Backoff to unigram
        return self.alpha * self.alpha * self.unigram_counts[word] / self.total_words, 'unigram'

# Training corpus
corpus = [
    "I want to eat Chinese food",
    "I want to eat Italian food",
    "She wants to visit Paris",
    "They want to learn programming",
    "The food is delicious"
]

model = StupidBackoffModel(backoff_weight=0.4)
model.train(corpus)

print("Stupid Backoff Model:")
print("="*60)
print(f"Backoff weight α = {model.alpha}")

test_cases = [
    ('chinese', 'eat', 'to'),    # Seen trigram
    ('pizza', 'eat', 'to'),      # Backoff to bigram
    ('programming', 'want', 'i'), # Seen trigram
    ('xyz', 'foo', 'bar'),       # Backoff to unigram
]

print("\nScoring different contexts:")
print("-" * 60)
for word, prev1, prev2 in test_cases:
    score, level = model.score(word, prev1, prev2)
    print(f"S({word:12}|{prev2} {prev1}) = {score:.6f}  [used {level}]")</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-balance-scale me-2"></i>Interpolation vs Backoff</h4>
                            <p><strong>Interpolation</strong> always combines all n-gram orders—useful when all provide signal. <strong>Backoff</strong> uses the best available estimate—more efficient and sometimes more accurate. Modern systems often use <strong>Modified Kneser-Ney with interpolation</strong>, combining the best of both approaches.</p>
                        </div>

                        <h2 id="perplexity"><i class="fas fa-tachometer-alt me-2"></i>Perplexity & Evaluation</h2>
                        
                        <p><strong>Perplexity</strong> is the standard metric for evaluating language models. Intuitively, it measures how "surprised" the model is by the test data—lower perplexity means the model predicts the test data better. Mathematically, perplexity is the inverse probability of the test set, normalized by the number of words:</p>

                        <p style="text-align: center;"><strong>PP(W) = P(w₁, w₂, ..., wₙ)^(-1/n) = ∏P(wᵢ|w₁,...,wᵢ₋₁)^(-1/n)</strong></p>

                        <p>Equivalently, perplexity equals 2 raised to the cross-entropy:</p>

                        <p style="text-align: center;"><strong>PP(W) = 2^H(W) = 2^(-1/n × Σlog₂P(wᵢ|context))</strong></p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>Interpreting Perplexity</h4>
                            <p>Perplexity can be interpreted as the <strong>weighted average branching factor</strong>—the effective number of equally likely words that could follow at each position. A perplexity of 100 means the model is as uncertain as if choosing uniformly among 100 words at each step.</p>
                            <ul>
                                <li><strong>Lower is better:</strong> PP = 50 beats PP = 200</li>
                                <li><strong>Baseline:</strong> Random uniform over V words gives PP = V</li>
                                <li><strong>State-of-art:</strong> Neural LMs achieve PP < 20 on benchmarks</li>
                            </ul>
                        </div>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter
import math

class PerplexityEvaluator:
    def __init__(self):
        self.bigram_counts = defaultdict(Counter)
        self.unigram_counts = Counter()
        self.vocab = set()
        self.vocab_size = 0
        
    def train(self, corpus):
        """Train bigram model with add-k smoothing"""
        for sentence in corpus:
            words = ['<s>'] + sentence.lower().split() + ['</s>']
            self.vocab.update(words)
            
            for i in range(len(words) - 1):
                self.unigram_counts[words[i]] += 1
                self.bigram_counts[words[i]][words[i+1]] += 1
        
        self.vocab_size = len(self.vocab)
    
    def probability(self, word, previous, k=0.01):
        """Add-k smoothed bigram probability"""
        count_bigram = self.bigram_counts[previous][word]
        count_unigram = self.unigram_counts[previous]
        return (count_bigram + k) / (count_unigram + k * self.vocab_size)
    
    def sentence_log_prob(self, sentence, k=0.01):
        """Calculate log probability of sentence"""
        words = ['<s>'] + sentence.lower().split() + ['</s>']
        log_prob = 0.0
        
        for i in range(1, len(words)):
            p = self.probability(words[i], words[i-1], k)
            log_prob += math.log2(p)
        
        return log_prob, len(words) - 1  # -1 for <s>
    
    def perplexity(self, test_corpus, k=0.01):
        """Calculate perplexity on test corpus"""
        total_log_prob = 0.0
        total_words = 0
        
        for sentence in test_corpus:
            log_prob, num_words = self.sentence_log_prob(sentence, k)
            total_log_prob += log_prob
            total_words += num_words
        
        # Perplexity = 2^(-average log prob)
        avg_log_prob = total_log_prob / total_words
        perplexity = 2 ** (-avg_log_prob)
        
        return perplexity, total_words

# Training corpus
train_corpus = [
    "the cat sat on the mat",
    "the dog sat on the floor",
    "the cat ate the fish",
    "the dog ate the bone",
    "the bird sat on the tree",
    "the cat chased the mouse",
    "the dog chased the cat"
]

# Test corpus
test_corpus = [
    "the cat sat on the floor",
    "the dog ate the fish",
    "the bird ate the worm"
]

# Train and evaluate
evaluator = PerplexityEvaluator()
evaluator.train(train_corpus)

print("Perplexity Evaluation:")
print("="*60)
print(f"Vocabulary size: {evaluator.vocab_size}")
print(f"Training sentences: {len(train_corpus)}")
print(f"Test sentences: {len(test_corpus)}")

# Calculate perplexity for different smoothing values
print("\nPerplexity with different smoothing:")
print("-" * 40)
for k in [0.001, 0.01, 0.1, 0.5, 1.0]:
    pp, num_words = evaluator.perplexity(test_corpus, k=k)
    print(f"k = {k:5.3f}: Perplexity = {pp:8.2f} ({num_words} words)")

# Show per-sentence perplexity
print("\nPer-sentence analysis (k=0.01):")
print("-" * 60)
for sentence in test_corpus:
    log_prob, n = evaluator.sentence_log_prob(sentence, k=0.01)
    pp = 2 ** (-log_prob / n)
    print(f"'{sentence}'")
    print(f"  Log prob: {log_prob:.2f}, Words: {n}, Perplexity: {pp:.2f}\n")</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-chart-bar me-2"></i>Comparing N-gram Orders by Perplexity</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Experiment</span>
                                <span class="badge bg-crimson">Evaluation</span>
                            </div>
                            <div class="content">
                                <p>Higher-order n-grams typically achieve lower perplexity on sufficient data:</p>
                                <table class="table table-bordered">
                                    <thead>
                                        <tr><th>Model</th><th>Perplexity (Penn Treebank)</th><th>Parameters</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>Unigram</td><td>≈1000</td><td>≈50K</td></tr>
                                        <tr><td>Bigram</td><td>≈150</td><td>≈2M</td></tr>
                                        <tr><td>Trigram</td><td>≈80</td><td>≈20M</td></tr>
                                        <tr><td>KN-smoothed Trigram</td><td>≈60</td><td>≈20M</td></tr>
                                        <tr><td>Neural LM (LSTM)</td><td>≈40</td><td>≈20M</td></tr>
                                        <tr><td>GPT-2 (1.5B)</td><td>≈18</td><td>1.5B</td></tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>

<pre><code class="language-python">import numpy as np
import math
from collections import defaultdict, Counter

def cross_entropy_to_perplexity(cross_entropy):
    """Convert cross-entropy (in bits) to perplexity"""
    return 2 ** cross_entropy

def perplexity_to_cross_entropy(perplexity):
    """Convert perplexity to cross-entropy"""
    return math.log2(perplexity)

# Demonstration
print("Cross-Entropy and Perplexity Relationship:")
print("="*50)
print(f"{'Cross-Entropy (bits)':>20} | {'Perplexity':>15}")
print("-" * 50)

for ce in [2, 4, 6, 8, 10]:
    pp = cross_entropy_to_perplexity(ce)
    print(f"{ce:>20} | {pp:>15.0f}")

print("\n" + "="*50)
print(f"{'Perplexity':>20} | {'Cross-Entropy (bits)':>20}")
print("-" * 50)

for pp in [10, 50, 100, 500, 1000]:
    ce = perplexity_to_cross_entropy(pp)
    print(f"{pp:>20} | {ce:>20.2f}")

# What perplexity means in practice
print("\n" + "="*50)
print("What Perplexity Means Intuitively:")
print("-" * 50)
print("PP = 10:   Like choosing from 10 equally likely words")
print("PP = 100:  Like choosing from 100 equally likely words")
print("PP = V:    Random guessing (uniform distribution)")
print("PP = 1:    Perfect prediction (impossible in practice)")</code></pre>

                        <h2 id="applications"><i class="fas fa-rocket me-2"></i>Applications</h2>
                        
                        <p>Statistical language models power numerous NLP applications. While neural models have largely superseded n-grams for complex tasks, understanding these applications provides foundation for modern techniques. Let's explore key applications: <strong>text generation</strong>, <strong>spelling correction</strong>, <strong>speech recognition</strong>, and <strong>machine translation decoding</strong>.</p>

                        <h3>Text Generation with N-grams</h3>
                        
                        <p>N-gram models can generate text by sampling words according to their conditional probabilities. Starting from a seed, we repeatedly sample the next word given the context. The quality depends on n-gram order and corpus—higher orders produce more coherent but less diverse text.</p>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter
import random

class TextGenerator:
    def __init__(self, n=3):
        self.n = n  # n-gram order
        self.ngram_counts = defaultdict(Counter)
        self.context_totals = defaultdict(int)
        
    def train(self, corpus):
        """Train n-gram model for generation"""
        for sentence in corpus:
            # Add start tokens
            tokens = ['<s>'] * (self.n - 1) + sentence.lower().split() + ['</s>']
            
            for i in range(self.n - 1, len(tokens)):
                context = tuple(tokens[i - self.n + 1:i])
                word = tokens[i]
                self.ngram_counts[context][word] += 1
                self.context_totals[context] += 1
    
    def generate(self, max_words=20, temperature=1.0):
        """Generate text using the trained model"""
        # Start with beginning tokens
        context = tuple(['<s>'] * (self.n - 1))
        generated = []
        
        for _ in range(max_words):
            if context not in self.ngram_counts:
                break
            
            # Get probability distribution
            candidates = self.ngram_counts[context]
            words = list(candidates.keys())
            counts = np.array(list(candidates.values()), dtype=float)
            
            # Apply temperature
            if temperature != 1.0:
                counts = counts ** (1.0 / temperature)
            
            probs = counts / counts.sum()
            
            # Sample next word
            next_word = random.choices(words, weights=probs)[0]
            
            if next_word == '</s>':
                break
            
            generated.append(next_word)
            
            # Update context
            context = tuple(list(context)[1:] + [next_word])
        
        return ' '.join(generated)

# Training corpus
corpus = [
    "I love to learn about natural language processing",
    "Natural language processing is a fascinating field",
    "I love to study machine learning algorithms",
    "Machine learning algorithms are powerful tools",
    "I want to build intelligent systems",
    "Intelligent systems can understand language",
    "Language is the foundation of communication",
    "Communication is essential for collaboration",
    "I love to read books about artificial intelligence",
    "Artificial intelligence will transform the world"
]

# Train and generate with different n-gram orders
print("Text Generation with N-gram Models:")
print("="*60)

for n in [2, 3, 4]:
    print(f"\n{n}-gram Model:")
    print("-" * 40)
    
    gen = TextGenerator(n=n)
    gen.train(corpus)
    
    # Generate multiple samples
    for i in range(3):
        text = gen.generate(max_words=15, temperature=0.8)
        print(f"  Sample {i+1}: {text}")</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-temperature-high me-2"></i>Temperature in Text Generation</h4>
                            <p><strong>Temperature</strong> controls randomness in generation:</p>
                            <ul>
                                <li><strong>T = 1.0:</strong> Sample from true probability distribution</li>
                                <li><strong>T < 1.0:</strong> Sharper distribution, more deterministic (favors high-probability words)</li>
                                <li><strong>T > 1.0:</strong> Flatter distribution, more random/creative</li>
                                <li><strong>T → 0:</strong> Greedy decoding (always pick most likely word)</li>
                            </ul>
                        </div>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter
import random

# Demonstrate temperature effects
def apply_temperature(probs, temperature):
    """Apply temperature to probability distribution"""
    probs = np.array(probs, dtype=float)
    # Raise to power of 1/T, then normalize
    adjusted = probs ** (1.0 / temperature)
    return adjusted / adjusted.sum()

# Original distribution
original_probs = np.array([0.5, 0.3, 0.15, 0.05])
words = ['the', 'a', 'some', 'xyz']

print("Temperature Effect on Word Selection:")
print("="*60)
print(f"Words: {words}")
print(f"Original probs: {original_probs}")
print()

for temp in [0.5, 1.0, 1.5, 2.0]:
    adjusted = apply_temperature(original_probs, temp)
    print(f"T = {temp}:  {adjusted.round(3)}")
    print(f"         Most likely: {words[adjusted.argmax()]} ({adjusted.max():.1%})")

print("\n" + "="*60)
print("Observation:")
print("- Lower T: Probabilities become more peaked")
print("- Higher T: Probabilities become more uniform")
print("- T=1: Original distribution preserved")</code></pre>

                        <h3>Spelling Correction</h3>
                        
                        <p>Language models enable <strong>context-sensitive spelling correction</strong>. Given a potentially misspelled word, we generate candidates and rank them using P(correction) × P(context|correction). The Noisy Channel Model formalizes this:</p>

                        <p style="text-align: center;"><strong>P(correct|observed) ∝ P(observed|correct) × P(correct)</strong></p>
                        <p style="text-align: center;"><em>Error Model × Language Model</em></p>

<pre><code class="language-python">import numpy as np
from collections import Counter
import re

class SpellingCorrector:
    def __init__(self):
        self.word_counts = Counter()
        self.total_words = 0
        
    def train(self, text):
        """Train language model from text"""
        words = re.findall(r'\w+', text.lower())
        self.word_counts.update(words)
        self.total_words = sum(self.word_counts.values())
    
    def P(self, word):
        """Unigram probability P(word)"""
        return self.word_counts[word] / self.total_words
    
    def edits1(self, word):
        """Generate all strings 1 edit away"""
        letters = 'abcdefghijklmnopqrstuvwxyz'
        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
        
        deletes = [L + R[1:] for L, R in splits if R]
        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]
        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]
        inserts = [L + c + R for L, R in splits for c in letters]
        
        return set(deletes + transposes + replaces + inserts)
    
    def edits2(self, word):
        """Generate all strings 2 edits away"""
        return set(e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))
    
    def candidates(self, word):
        """Generate correction candidates"""
        return (
            self.known([word]) or           # If word is known, return it
            self.known(self.edits1(word)) or  # Known words 1 edit away
            self.known(self.edits2(word)) or  # Known words 2 edits away
            [word]                            # Return original if nothing found
        )
    
    def known(self, words):
        """Return words that exist in vocabulary"""
        return set(w for w in words if w in self.word_counts)
    
    def correct(self, word):
        """Return most probable correction"""
        return max(self.candidates(word), key=self.P)
    
    def correct_with_scores(self, word, top_k=5):
        """Return top k corrections with probabilities"""
        cands = self.candidates(word)
        scored = [(c, self.P(c)) for c in cands]
        scored.sort(key=lambda x: -x[1])
        return scored[:top_k]

# Training text
training_text = """
The quick brown fox jumps over the lazy dog.
Natural language processing is a field of artificial intelligence.
Machine learning algorithms learn patterns from data.
The weather today is beautiful and sunny.
I love to learn about programming and technology.
The cat sat on the mat near the window.
Programming languages include Python Java and JavaScript.
Data science combines statistics and computer science.
"""

# Train corrector
corrector = SpellingCorrector()
corrector.train(training_text * 10)  # Repeat for more counts

print("Spelling Correction with Language Model:")
print("="*60)

# Test corrections
misspellings = [
    ('teh', 'the'),
    ('langauge', 'language'),
    ('computr', 'computer'),
    ('learnng', 'learning'),
    ('machne', 'machine'),
    ('progrm', 'program')
]

for misspelled, expected in misspellings:
    correction = corrector.correct(misspelled)
    status = '✓' if correction == expected else '✗'
    print(f"'{misspelled}' → '{correction}' (expected: '{expected}') {status}")

print("\nTop candidates for 'teh':")
print("-" * 40)
for word, prob in corrector.correct_with_scores('teh'):
    print(f"  {word:15} P = {prob:.6f}")</code></pre>

                        <h3>Speech Recognition Decoding</h3>
                        
                        <p>In automatic speech recognition (ASR), the language model combines with the acoustic model to find the most likely transcription. Given acoustic features A, we seek:</p>

                        <p style="text-align: center;"><strong>W* = argmax P(W|A) = argmax P(A|W) × P(W)</strong></p>
                        <p style="text-align: center;"><em>Acoustic Model × Language Model</em></p>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter

# Simulated speech recognition scoring
class SimpleSpeechDecoder:
    """Simplified demonstration of LM in speech recognition"""
    
    def __init__(self):
        self.bigram_counts = defaultdict(Counter)
        self.unigram_counts = Counter()
        
    def train(self, corpus):
        for sentence in corpus:
            words = ['<s>'] + sentence.lower().split() + ['</s>']
            for i in range(len(words) - 1):
                self.unigram_counts[words[i]] += 1
                self.bigram_counts[words[i]][words[i+1]] += 1
    
    def lm_score(self, sentence):
        """Log probability from language model"""
        words = ['<s>'] + sentence.lower().split() + ['</s>']
        score = 0.0
        for i in range(1, len(words)):
            count_bi = self.bigram_counts[words[i-1]][words[i]] + 0.1
            count_uni = self.unigram_counts[words[i-1]] + 10
            score += np.log(count_bi / count_uni)
        return score
    
    def decode(self, acoustic_hypotheses, lm_weight=0.5):
        """
        Re-rank ASR hypotheses using LM
        acoustic_hypotheses: list of (text, acoustic_score) tuples
        """
        results = []
        for text, acoustic in acoustic_hypotheses:
            lm = self.lm_score(text)
            combined = (1 - lm_weight) * acoustic + lm_weight * lm
            results.append((text, acoustic, lm, combined))
        
        results.sort(key=lambda x: -x[3])  # Sort by combined score
        return results

# Training corpus for LM
corpus = [
    "I want to recognize speech",
    "I want to wreck a nice beach",  # Famous ASR joke!
    "speech recognition is difficult",
    "recognize speech accurately",
    "the speech was excellent"
]

decoder = SimpleSpeechDecoder()
decoder.train(corpus)

print("Speech Recognition with Language Model:")
print("="*60)

# Simulated ASR output (ambiguous acoustically)
acoustic_hypotheses = [
    ("I want to recognize speech", -5.0),
    ("I want to wreck a nice beach", -4.8),  # Slightly better acoustic
    ("I wand to recognize speach", -5.2),
]

print("\nHypotheses and Scores:")
print("-" * 60)
print(f"{'Text':^35} | {'Acoustic':^10} | {'LM':^10} | {'Combined':^10}")
print("-" * 60)

results = decoder.decode(acoustic_hypotheses, lm_weight=0.5)
for text, acoustic, lm, combined in results:
    print(f"{text:35} | {acoustic:10.2f} | {lm:10.2f} | {combined:10.2f}")

print("\nWinner:", results[0][0])
print("\nNote: LM helps distinguish acoustically similar phrases!")</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-cogs me-2"></i>N-gram Models in Modern Systems</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Applications</span>
                                <span class="badge bg-crimson">Industry Use</span>
                            </div>
                            <div class="content">
                                <p>While neural models dominate, n-grams remain useful:</p>
                                <ul>
                                    <li><strong>Keyboard Autocomplete:</strong> N-grams power fast, on-device text prediction</li>
                                    <li><strong>Spam Filtering:</strong> Character n-grams detect obfuscated spam</li>
                                    <li><strong>ASR Rescoring:</strong> N-gram LMs refine neural ASR output</li>
                                    <li><strong>Language Identification:</strong> Character n-gram profiles identify languages</li>
                                    <li><strong>Cache/Hybrid Models:</strong> N-grams adapt to recent context in neural systems</li>
                                </ul>
                            </div>
                        </div>

                        <h2 id="limitations"><i class="fas fa-exclamation-circle me-2"></i>Limitations & Transition to Neural</h2>
                        
                        <p>While n-gram models provide an elegant foundation for language modeling, they suffer from fundamental limitations that motivated the development of neural language models. Understanding these limitations explains why modern NLP has shifted to neural approaches.</p>

                        <h3>The Sparsity Problem</h3>
                        
                        <p>The most severe limitation is <strong>data sparsity</strong>. The number of possible n-grams grows exponentially with n and vocabulary size (V^n). Even with billions of words of training data, most valid n-grams never appear. Smoothing helps but cannot solve the fundamental problem:</p>

<pre><code class="language-python">import numpy as np

def calculate_possible_ngrams(vocab_size, n):
    """Calculate number of possible n-grams"""
    return vocab_size ** n

def estimate_coverage(vocab_size, n, corpus_size):
    """Estimate fraction of possible n-grams seen in corpus"""
    possible = vocab_size ** n
    # Rough estimate: unique n-grams ~ corpus_size for large corpora
    estimated_seen = min(corpus_size * 0.7, possible)  # Diminishing returns
    return estimated_seen / possible

vocab_size = 50000  # Typical vocabulary
corpus_size = 1e9   # 1 billion words

print("N-gram Sparsity Analysis:")
print("="*60)
print(f"Vocabulary size: {vocab_size:,}")
print(f"Corpus size: {corpus_size/1e9:.1f} billion words")
print()
print(f"{'N':>5} | {'Possible N-grams':>20} | {'Coverage':>15}")
print("-" * 60)

for n in range(1, 6):
    possible = calculate_possible_ngrams(vocab_size, n)
    coverage = estimate_coverage(vocab_size, n, corpus_size)
    
    if possible > 1e15:
        possible_str = f"{possible:.2e}"
    else:
        possible_str = f"{possible:,.0f}"
    
    print(f"{n:>5} | {possible_str:>20} | {coverage:>14.8%}")

print("\nKey insight: Coverage drops exponentially with n!")
print("Even 1 billion words see <1% of possible trigrams.")</code></pre>

                        <h3>Limited Context Window</h3>
                        
                        <p>N-gram models have a <strong>fixed, short context window</strong>. They cannot capture dependencies that span more than n-1 words. Consider: "The author who wrote the book that influenced the movement that changed history <strong>was</strong>..." — predicting "was" requires understanding the distant subject "author," impossible for any reasonable n-gram order.</p>

<pre><code class="language-python">import numpy as np

# Demonstrate long-range dependency problem
sentences = [
    # Subject-verb agreement across distance
    ("The cat that sat on the mat was happy", "was"),
    ("The cats that sat on the mat were happy", "were"),
    
    # Context needed from far away
    ("The researcher who published the paper in the prestigious journal received an award", "received"),
]

print("Long-Range Dependency Problem:")
print("="*60)

for sentence, target in sentences:
    words = sentence.split()
    target_idx = words.index(target)
    
    print(f"\nSentence: '{sentence}'")
    print(f"Target word: '{target}' at position {target_idx}")
    print(f"")
    
    for n in [2, 3, 4, 5]:
        context_start = max(0, target_idx - n + 1)
        context = words[context_start:target_idx]
        print(f"  {n}-gram context: '{' '.join(context)}' → {target}")
    
    # Show what context would be needed
    subject = words[0:2]  # "The cat" or "The cats"
    print(f"  Actual needed context: '{' '.join(subject)}' (distance: {target_idx})")

print("\n" + "="*60)
print("N-grams cannot capture agreement when subject is far from verb!")</code></pre>

                        <h3>No Generalization Across Similar Words</h3>
                        
                        <p>N-gram models treat each word as an atomic symbol with no similarity to other words. Seeing "dog chased cat" tells the model nothing about "puppy chased kitten"—these are completely unrelated n-grams. Neural models with word embeddings share statistical strength across similar words:</p>

<pre><code class="language-python">import numpy as np
from collections import defaultdict, Counter

# Demonstrate lack of generalization
class NgramModel:
    def __init__(self):
        self.bigram_counts = defaultdict(Counter)
        self.unigram_counts = Counter()
        
    def train(self, corpus):
        for sentence in corpus:
            words = sentence.lower().split()
            for i in range(len(words) - 1):
                self.unigram_counts[words[i]] += 1
                self.bigram_counts[words[i]][words[i+1]] += 1
    
    def probability(self, word, previous):
        count = self.unigram_counts[previous.lower()]
        if count == 0:
            return 0.0
        return self.bigram_counts[previous.lower()][word.lower()] / count

# Training: only see "dog" and "cat" examples
corpus = [
    "the dog chased the cat",
    "a dog barked loudly",
    "the cat meowed softly",
    "my dog is friendly"
]

model = NgramModel()
model.train(corpus)

print("N-gram Generalization Problem:")
print("="*60)

# Words we've seen
print("\nWords we trained on:")
print("  'dog chased' → seen")
print("  'cat meowed' → seen")

print("\nTrying to predict similar but unseen patterns:")
print("-" * 50)

test_cases = [
    ('chased', 'dog'),     # Seen
    ('chased', 'puppy'),   # Similar but unseen  
    ('chased', 'hound'),   # Similar but unseen
    ('meowed', 'cat'),     # Seen
    ('meowed', 'kitten'),  # Similar but unseen
]

for word, prev in test_cases:
    prob = model.probability(word, prev)
    status = "✓ seen" if prob > 0 else "✗ unseen (0 probability!)"
    print(f"P({word}|{prev:8}) = {prob:.4f}  {status}")

print("\n" + "="*60)
print("N-grams can't generalize: 'puppy' ≠ 'dog' in this model.")
print("Neural models share weights across similar words via embeddings.")</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-arrow-right me-2"></i>Why Neural Language Models?</h4>
                            <p>Neural language models address all these limitations:</p>
                            <ul>
                                <li><strong>Dense Representations:</strong> Words become vectors, enabling smooth probability estimates</li>
                                <li><strong>Generalization:</strong> Similar words have similar vectors, share statistical strength</li>
                                <li><strong>Long Context:</strong> RNNs/Transformers capture dependencies across hundreds/thousands of tokens</li>
                                <li><strong>Compositional:</strong> Learn to compose meaning from parts, not memorize n-grams</li>
                            </ul>
                        </div>

                        <h3>Storage and Computational Costs</h3>
                        
                        <p>Storing n-gram counts requires massive memory for large vocabularies and corpora. Google's n-gram corpus (released 2006) contains 1 trillion tokens but requires careful pruning to be usable. Neural models can be more parameter-efficient while capturing richer patterns:</p>

<pre><code class="language-python">import numpy as np

def estimate_ngram_storage(vocab_size, max_n, avg_count_bytes=4):
    """Estimate storage for n-gram model"""
    total_bytes = 0
    
    print(f"N-gram Storage Estimation (V={vocab_size:,}):")
    print("="*50)
    
    for n in range(1, max_n + 1):
        # Theoretical maximum (sparse in practice)
        max_entries = vocab_size ** n
        
        # Realistic: assume 1% coverage for bigrams, decreasing
        coverage = 0.01 * (0.1 ** (n - 2)) if n > 1 else 1.0
        estimated_entries = int(max_entries * min(coverage, 1.0))
        estimated_entries = min(estimated_entries, 1e10)  # Cap at 10B
        
        # Storage: (n words × 4 bytes each) + count (4 bytes)
        bytes_per_entry = n * avg_count_bytes + avg_count_bytes
        storage = estimated_entries * bytes_per_entry
        
        print(f"  {n}-gram: ~{estimated_entries:,.0f} entries = {storage/1e9:.2f} GB")
        total_bytes += storage
    
    return total_bytes

print("Small Vocabulary (V=10,000):")
small = estimate_ngram_storage(10000, 4)
print(f"  Total: {small/1e9:.2f} GB\n")

print("Large Vocabulary (V=100,000):")
large = estimate_ngram_storage(100000, 4)
print(f"  Total: {large/1e9:.2f} GB\n")

print("Comparison with Neural Models:")
print("="*50)
print("  GPT-2 Small (117M params):  ~0.5 GB")
print("  GPT-2 Large (774M params):  ~3.0 GB")
print("  BERT Base (110M params):    ~0.4 GB")
print("\nNeural models pack more knowledge into fewer bytes!")</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-balance-scale me-2"></i>N-grams vs Neural: When to Use Each</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Decision Guide</span>
                                <span class="badge bg-crimson">Best Practices</span>
                            </div>
                            <div class="content">
                                <table class="table table-bordered">
                                    <thead>
                                        <tr><th>Use N-grams When...</th><th>Use Neural When...</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>Interpretability is critical</td><td>Best quality is required</td></tr>
                                        <tr><td>Low latency needed (mobile)</td><td>Long context matters</td></tr>
                                        <tr><td>Limited compute resources</td><td>Generalization is important</td></tr>
                                        <tr><td>Domain-specific, small vocab</td><td>Large, diverse vocabulary</td></tr>
                                        <tr><td>Baseline/hybrid systems</td><td>State-of-the-art performance</td></tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>

                        <h2 id="conclusion"><i class="fas fa-flag-checkered me-2"></i>Conclusion & Next Steps</h2>
                        
                        <p>Statistical language models and n-grams represent a foundational pillar of NLP that, despite their limitations, continue to inform modern approaches. We've explored how these models assign probabilities to word sequences, the trade-offs between different n-gram orders, and the critical role of smoothing techniques in handling unseen data.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-graduation-cap me-2"></i>Key Takeaways</h4>
                            <ul>
                                <li><strong>Language Models:</strong> Assign P(sequence) using the chain rule of probability</li>
                                <li><strong>Markov Assumption:</strong> Approximate full history with fixed n-1 word context</li>
                                <li><strong>MLE:</strong> Count-based estimation, simple but assigns zero to unseen events</li>
                                <li><strong>Smoothing:</strong> Essential for handling sparsity (Laplace, Add-k, Kneser-Ney)</li>
                                <li><strong>Interpolation/Backoff:</strong> Combine n-gram orders for robustness</li>
                                <li><strong>Perplexity:</strong> Standard evaluation metric (lower = better)</li>
                                <li><strong>Limitations:</strong> Sparsity, fixed context, no similarity generalization</li>
                            </ul>
                        </div>

                        <p>The concepts learned here—probability estimation, smoothing, evaluation metrics, and the tension between model capacity and data requirements—carry forward to neural language models. Understanding why n-grams struggle provides intuition for why architectures like RNNs and Transformers were developed.</p>

<pre><code class="language-python"># Summary: Complete N-gram Pipeline
import numpy as np
from collections import defaultdict, Counter
import math

class CompleteNgramModel:
    """Production-ready n-gram model with all techniques"""
    
    def __init__(self, n=3, smoothing='kneser-ney', discount=0.75):
        self.n = n
        self.smoothing = smoothing
        self.discount = discount
        
        # Storage for counts
        self.ngram_counts = [defaultdict(Counter) for _ in range(n)]
        self.context_totals = [defaultdict(int) for _ in range(n)]
        self.continuation_counts = Counter()
        self.vocab = set()
        
    def train(self, corpus):
        """Train on corpus with proper tokenization"""
        continuation_contexts = defaultdict(set)
        
        for sentence in corpus:
            tokens = ['<s>'] * (self.n - 1) + sentence.lower().split() + ['</s>']
            self.vocab.update(tokens)
            
            for i in range(len(tokens)):
                # Count n-grams of all orders
                for order in range(1, min(i + 2, self.n + 1)):
                    context = tuple(tokens[i - order + 1:i])
                    word = tokens[i]
                    self.ngram_counts[order - 1][context][word] += 1
                    self.context_totals[order - 1][context] += 1
                    
                    if order == 2:  # Track continuation counts for Kneser-Ney
                        continuation_contexts[word].add(context)
        
        for word, contexts in continuation_contexts.items():
            self.continuation_counts[word] = len(contexts)
        
        self.total_continuations = sum(self.continuation_counts.values())
        print(f"Trained {self.n}-gram model on {len(corpus)} sentences")
        print(f"Vocabulary size: {len(self.vocab)}")
    
    def probability(self, word, context):
        """Calculate smoothed probability"""
        word = word.lower()
        context = tuple(w.lower() for w in context)
        
        if self.smoothing == 'kneser-ney':
            return self._kneser_ney_prob(word, context)
        else:
            return self._additive_prob(word, context)
    
    def _kneser_ney_prob(self, word, context):
        """Interpolated Kneser-Ney probability"""
        if len(context) == 0:
            # Base case: continuation probability
            return self.continuation_counts[word] / max(1, self.total_continuations)
        
        order = len(context)
        count = self.ngram_counts[order][context][word]
        total = self.context_totals[order][context]
        
        if total == 0:
            return self._kneser_ney_prob(word, context[1:])
        
        # Discounted probability + interpolation weight × lower-order
        num_types = len(self.ngram_counts[order][context])
        lambda_weight = self.discount * num_types / total
        
        first_term = max(count - self.discount, 0) / total
        lower_order = self._kneser_ney_prob(word, context[1:])
        
        return first_term + lambda_weight * lower_order
    
    def _additive_prob(self, word, context, k=0.01):
        """Add-k smoothed probability"""
        order = len(context)
        count = self.ngram_counts[order][context][word]
        total = self.context_totals[order][context]
        V = len(self.vocab)
        return (count + k) / (total + k * V)
    
    def perplexity(self, test_corpus):
        """Calculate perplexity on test set"""
        total_log_prob = 0
        total_words = 0
        
        for sentence in test_corpus:
            tokens = ['<s>'] * (self.n - 1) + sentence.lower().split() + ['</s>']
            
            for i in range(self.n - 1, len(tokens)):
                context = tuple(tokens[i - self.n + 1:i])
                word = tokens[i]
                p = self.probability(word, context)
                total_log_prob += math.log2(max(p, 1e-10))
                total_words += 1
        
        return 2 ** (-total_log_prob / total_words)

# Quick demonstration
corpus = [
    "I love natural language processing",
    "Language models are fascinating",
    "I want to learn more about NLP",
    "Statistical models provide foundations"
]

model = CompleteNgramModel(n=3, smoothing='kneser-ney')
model.train(corpus)

test = ["I love language models"]
pp = model.perplexity(test)
print(f"Test perplexity: {pp:.2f}")</code></pre>

                        <h3>What's Next: Neural Language Models</h3>
                        
                        <p>In the next part of our series, we'll explore how <strong>neural networks overcome n-gram limitations</strong>. You'll learn how word embeddings enable generalization, how RNNs/LSTMs capture long-range dependencies, and how attention mechanisms (leading to Transformers) revolutionized language modeling.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-road me-2"></i>Your Learning Path</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Next Steps</span>
                                <span class="badge bg-crimson">Practice</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><strong>Implement from scratch:</strong> Build your own n-gram model with smoothing</li>
                                    <li><strong>Experiment with NLTK:</strong> Use `nltk.lm` for production n-gram models</li>
                                    <li><strong>Compare orders:</strong> Train 2/3/4-gram models, compare perplexity</li>
                                    <li><strong>Build an application:</strong> Create a simple autocomplete or text generator</li>
                                    <li><strong>Move to neural:</strong> Continue to Part 6 for neural language models</li>
                                </ol>
                            </div>
                        </div>

                        <p>Statistical language models may seem simple compared to modern transformers, but they embody timeless principles: probabilistic modeling, handling uncertainty, trading off bias and variance, and the fundamental goal of predicting human language. These foundations will serve you well as you advance through the rest of this NLP series.</p>

                        <div class="related-posts">
                            <h3><i class="fas fa-book-reader me-2"></i>Continue the NLP Series</h3>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 4: Word Embeddings</h5>
                                <p class="text-muted small mb-2">Capture meaning and similarity with Word2Vec, GloVe, and FastText.</p>
                                <a href="nlp-word-embeddings.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 6: Neural Networks for NLP</h5>
                                <p class="text-muted small mb-2">Apply deep learning fundamentals to natural language processing.</p>
                                <a href="nlp-neural-networks.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 7: RNNs, LSTMs & GRUs</h5>
                                <p class="text-muted small mb-2">Master sequential neural networks for language modeling.</p>
                                <a href="nlp-rnn-lstm-gru.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.</p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook"><i class="fab fa-facebook-f"></i></a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter"><i class="fab fa-twitter"></i></a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn"><i class="fab fa-linkedin-in"></i></a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube"><i class="fab fa-youtube"></i></a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram"><i class="fab fa-instagram"></i></a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest"><i class="fab fa-pinterest-p"></i></a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email"><i class="fas fa-envelope"></i></a>
                    </div>
                </div>
            </div>
            <hr class="bg-secondary">
            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small"><i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a></p>
                    <p class="small mt-3"><a href="/" class="text-light text-decoration-none">Home</a> | <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a></p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">Enjoying this content? ☕ <a href="https://buymeacoffee.com/itswzee" target="_blank" class="text-light" style="text-decoration: underline;">Keep me caffeinated</a> to keep the pixels flowing!</p>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top"><i class="fas fa-arrow-up"></i></button>
    <script src="../../../js/cookie-consent.js"></script>
    <script src="../../../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <script>
        const themes = { 'prism-theme': 'Tomorrow Night', 'prism-default': 'Default', 'prism-dark': 'Dark', 'prism-twilight': 'Twilight', 'prism-okaidia': 'Okaidia', 'prism-solarizedlight': 'Solarized Light' };
        const savedTheme = localStorage.getItem('prism-theme') || 'prism-theme';
        function switchTheme(themeId) { Object.keys(themes).forEach(id => { const link = document.getElementById(id); if (link) link.disabled = true; }); const selectedLink = document.getElementById(themeId); if (selectedLink) { selectedLink.disabled = false; localStorage.setItem('prism-theme', themeId); } document.querySelectorAll('div.code-toolbar select').forEach(dropdown => { dropdown.value = themeId; }); setTimeout(() => Prism.highlightAll(), 10); }
        document.addEventListener('DOMContentLoaded', function() { switchTheme(savedTheme); });
        Prism.plugins.toolbar.registerButton('theme-switcher', function(env) { const select = document.createElement('select'); select.setAttribute('aria-label', 'Select code theme'); Object.keys(themes).forEach(themeId => { const option = document.createElement('option'); option.value = themeId; option.textContent = themes[themeId]; if (themeId === savedTheme) option.selected = true; select.appendChild(option); }); select.addEventListener('change', function(e) { switchTheme(e.target.value); }); return select; });
    </script>

    <script>
        document.addEventListener('DOMContentLoaded', function() { const scrollToTopBtn = document.getElementById('scrollToTop'); window.addEventListener('scroll', function() { if (window.scrollY > 300) { scrollToTopBtn.classList.add('show'); } else { scrollToTopBtn.classList.remove('show'); } }); scrollToTopBtn.addEventListener('click', function() { window.scrollTo({ top: 0, behavior: 'smooth' }); }); });
        function openNav() { document.getElementById('tocSidenav').classList.add('open'); document.getElementById('tocOverlay').classList.add('show'); document.body.style.overflow = 'hidden'; }
        function closeNav() { document.getElementById('tocSidenav').classList.remove('open'); document.getElementById('tocOverlay').classList.remove('show'); document.body.style.overflow = 'auto'; }
        document.addEventListener('keydown', function(e) { if (e.key === 'Escape') closeNav(); });
    </script>
</body>
</html>
