<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Part 9 of the Complete NLP Series: Master pretrained language models—BERT, RoBERTa, ALBERT, ELECTRA, pretraining objectives (MLM, NSP), fine-tuning strategies, and transfer learning for NLP." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="NLP, BERT, Transfer Learning, Pretrained Models, Fine-tuning, MLM, Masked Language Model, RoBERTa, ALBERT, ELECTRA, Hugging Face" />
    <meta property="og:title" content="Pretrained Language Models & Transfer Learning - Complete NLP Series Part 9" />
    <meta property="og:description" content="Learn about BERT, pretraining objectives, fine-tuning strategies, and transfer learning." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-27" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>Pretrained Language Models & Transfer Learning - Complete NLP Series Part 9 - Wasil Zafar</title>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('consent', 'default', { 'ad_storage': 'denied', 'ad_user_data': 'denied', 'ad_personalization': 'denied', 'analytics_storage': 'denied', 'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE'] });
        gtag('consent', 'default', { 'ad_storage': 'granted', 'ad_user_data': 'granted', 'ad_personalization': 'granted', 'analytics_storage': 'granted' });
        gtag('set', 'url_passthrough', true);
    </script>
    <script>
        (function(w, d, s, l, i) { w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' }); var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f); })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    <style>
        .blog-hero { background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%); color: white; padding: 80px 0; }
        .blog-meta { font-size: 0.95rem; color: var(--color-teal); margin-bottom: 1rem; display: flex; align-items: center; flex-wrap: wrap; gap: 1rem; }
        .print-btn { background: var(--color-teal); color: white; border: none; padding: 0.4rem 1rem; border-radius: 4px; font-size: 0.9rem; cursor: pointer; transition: all 0.3s ease; display: inline-flex; align-items: center; gap: 0.5rem; }
        .print-btn:hover { background: var(--color-crimson); transform: translateY(-1px); }
        @media print { .print-btn, nav, .navbar, footer, .back-link, .related-posts, .scroll-to-top, .toc-toggle-btn, .sidenav-toc, .sidenav-overlay { display: none !important; } * { -webkit-print-color-adjust: exact !important; print-color-adjust: exact !important; } }
        .blog-content { max-width: 900px; margin: 0 auto; font-size: 1.05rem; line-height: 1.8; color: #333; }
        .blog-content h2 { font-size: 1.8rem; font-weight: 700; margin-top: 2.5rem; margin-bottom: 1.5rem; color: var(--color-navy); border-bottom: 3px solid var(--color-teal); padding-bottom: 0.5rem; }
        .blog-content h3 { font-size: 1.3rem; font-weight: 600; margin-top: 2rem; margin-bottom: 1rem; color: var(--color-blue); }
        .blog-content h4 { font-size: 1.1rem; font-weight: 600; margin-top: 1.5rem; margin-bottom: 1rem; color: var(--color-teal); }
        .blog-content p { margin-bottom: 1.2rem; text-align: justify; }
        .blog-content strong { color: var(--color-crimson); }
        .blog-content pre[class*="language-"] { border-radius: 6px; margin: 1.5rem 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); }
        .blog-content p code, .blog-content li code { background: rgba(59, 151, 151, 0.1); color: var(--color-crimson); padding: 0.2rem 0.4rem; border-radius: 3px; font-family: 'Consolas', monospace; font-size: 0.9em; }
        .highlight-box { background: rgba(59, 151, 151, 0.1); border-left: 4px solid var(--color-teal); padding: 1.5rem; margin: 2rem 0; border-radius: 4px; }
        .experiment-card { background: #f8f9fa; border: 1px solid #ddd; border-radius: 8px; padding: 1.5rem; margin-bottom: 1.5rem; transition: all 0.3s ease; }
        .experiment-card:hover { box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1); transform: translateY(-2px); }
        .experiment-card h4 { color: var(--color-crimson); font-weight: 700; margin-bottom: 0.5rem; }
        .bg-teal { background-color: var(--color-teal) !important; }
        .bg-crimson { background-color: var(--color-crimson) !important; }
        .toc-toggle-btn { position: fixed; bottom: 2rem; left: 2rem; width: 50px; height: 50px; background: var(--color-teal); color: white; border: none; border-radius: 50%; font-size: 1.2rem; cursor: pointer; box-shadow: 0 4px 12px rgba(59, 151, 151, 0.4); transition: all 0.3s ease; z-index: 1049; display: flex; align-items: center; justify-content: center; }
        .toc-toggle-btn:hover { background: var(--color-crimson); transform: scale(1.1); }
        .sidenav-toc { height: calc(100% - 64px); width: 0; position: fixed; z-index: 1050; top: 64px; left: 0; background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%); overflow-x: hidden; overflow-y: auto; transition: width 0.4s ease; padding-top: 30px; box-shadow: 4px 0 15px rgba(0, 0, 0, 0.3); }
        .sidenav-toc.open { width: 350px; }
        .sidenav-toc .toc-header { display: flex; align-items: center; justify-content: space-between; padding: 20px 30px; margin-bottom: 20px; border-bottom: 2px solid var(--color-teal); opacity: 0; visibility: hidden; transition: all 0.3s ease; }
        .sidenav-toc.open .toc-header { opacity: 1; visibility: visible; }
        .sidenav-toc .closebtn { font-size: 32px; color: white; background: transparent; border: none; cursor: pointer; transition: all 0.3s ease; }
        .sidenav-toc .closebtn:hover { color: var(--color-crimson); transform: rotate(90deg); }
        .sidenav-toc h3 { color: white; margin: 0; font-weight: 700; font-size: 1.3rem; }
        .sidenav-toc ol { list-style: decimal; padding-left: 30px; margin: 0; color: rgba(255, 255, 255, 0.9); }
        .sidenav-toc ol li { margin-bottom: 8px; }
        .sidenav-toc ul { list-style-type: lower-alpha; padding-left: 30px; margin-top: 8px; }
        .sidenav-toc a { padding: 12px 30px; text-decoration: none; font-size: 0.95rem; color: rgba(255, 255, 255, 0.85); display: block; transition: all 0.3s ease; border-left: 4px solid transparent; }
        .sidenav-toc a:hover { color: white; background: rgba(59, 151, 151, 0.2); border-left-color: var(--color-teal); }
        .sidenav-toc a.active { color: white; background: rgba(191, 9, 47, 0.3); border-left-color: var(--color-crimson); font-weight: 600; }
        .sidenav-overlay { display: none; position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0, 0, 0, 0.5); z-index: 1049; }
        .sidenav-overlay.show { display: block; }
        .reading-time { display: inline-block; background: var(--color-crimson); color: white; padding: 0.3rem 0.8rem; border-radius: 4px; font-size: 0.9rem; }
        .back-link { display: inline-block; color: white; text-decoration: none; transition: all 0.3s ease; margin-bottom: 1rem; opacity: 0.9; }
        .back-link:hover { color: var(--color-teal); transform: translateX(-5px); }
        .related-posts { background: #f8f9fa; border-radius: 8px; padding: 2rem; margin-top: 3rem; }
        .related-posts h3 { color: var(--color-navy); margin-bottom: 1.5rem; }
        .related-post-item { padding: 1rem; border-left: 3px solid var(--color-teal); margin-bottom: 1rem; transition: all 0.3s ease; }
        .related-post-item:hover { background: white; border-left-color: var(--color-crimson); }
        .related-post-item a { color: var(--color-blue); text-decoration: none; font-weight: 600; }
        .related-post-item a:hover { color: var(--color-crimson); }
        div.code-toolbar > .toolbar { opacity: 1; display: flex; gap: 0.5rem; }
        div.code-toolbar > .toolbar > .toolbar-item > button { background: var(--color-teal); color: white; border: none; padding: 0.4rem 0.8rem; border-radius: 4px; font-size: 0.85rem; cursor: pointer; }
        div.code-toolbar > .toolbar > .toolbar-item > select { background: var(--color-navy); color: white; border: 1px solid var(--color-teal); padding: 0.4rem 0.8rem; border-radius: 4px; font-size: 0.85rem; }
        .scroll-to-top { position: fixed; bottom: 2rem; right: 2rem; width: 50px; height: 50px; background: var(--color-teal); color: white; border: none; border-radius: 50%; font-size: 1.2rem; cursor: pointer; display: flex; align-items: center; justify-content: center; opacity: 0; visibility: hidden; transition: all 0.3s ease; box-shadow: 0 4px 12px rgba(59, 151, 151, 0.3); z-index: 999; }
        .scroll-to-top.show { opacity: 1; visibility: visible; }
        .scroll-to-top:hover { background: var(--color-crimson); transform: translateY(-3px); }
        @media (max-width: 768px) { .sidenav-toc.open { width: 280px; } .toc-toggle-btn { left: 15px; } }
        html { scroll-behavior: smooth; }
    </style>
</head>
<body>
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/"><span class="gradient-text">Wasil Zafar</span></a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link" href="/">Home</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#about">About</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#certifications">Certifications</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#interests">Interests</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link"><i class="fas fa-arrow-left me-2"></i>Back to Technology</a>
                <h1 class="display-4 fw-bold mb-3">Pretrained Language Models & Transfer Learning</h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 27, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-1"></i>40 min read</span>
                    <button onclick="window.print()" class="print-btn" title="Print this article"><i class="fas fa-print"></i> Print</button>
                </div>
                <p class="lead">Part 9 of 16: Master pretrained language models—BERT, RoBERTa, ALBERT, pretraining objectives, and fine-tuning strategies for transfer learning.</p>
            </div>
        </div>
    </section>

    <button class="toc-toggle-btn" onclick="openNav()" title="Table of Contents" aria-label="Open Table of Contents"><i class="fas fa-list"></i></button>

    <div id="tocSidenav" class="sidenav-toc">
        <div class="toc-header">
            <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
            <button class="closebtn" onclick="closeNav()" aria-label="Close">&times;</button>
        </div>
        <ol>
            <li><a href="#introduction" onclick="closeNav()">Introduction to Transfer Learning</a></li>
            <li><a href="#pretraining" onclick="closeNav()">Pretraining Objectives</a>
                <ul>
                    <li><a href="#mlm" onclick="closeNav()">Masked Language Modeling (MLM)</a></li>
                    <li><a href="#nsp" onclick="closeNav()">Next Sentence Prediction (NSP)</a></li>
                </ul>
            </li>
            <li><a href="#bert" onclick="closeNav()">BERT Architecture</a></li>
            <li><a href="#variants" onclick="closeNav()">BERT Variants</a>
                <ul>
                    <li><a href="#roberta" onclick="closeNav()">RoBERTa</a></li>
                    <li><a href="#albert" onclick="closeNav()">ALBERT</a></li>
                    <li><a href="#electra" onclick="closeNav()">ELECTRA</a></li>
                </ul>
            </li>
            <li><a href="#fine-tuning" onclick="closeNav()">Fine-Tuning Strategies</a></li>
            <li><a href="#huggingface" onclick="closeNav()">Hugging Face Transformers</a></li>
            <li><a href="#practical" onclick="closeNav()">Practical Implementation</a></li>
            <li><a href="#conclusion" onclick="closeNav()">Conclusion & Next Steps</a></li>
        </ol>
    </div>

    <div id="tocOverlay" class="sidenav-overlay" onclick="closeNav()"></div>

    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="blog-content">
                        
                        <h2 id="introduction"><i class="fas fa-share-alt me-2"></i>Introduction to Transfer Learning</h2>
                        
                        <p>Transfer learning revolutionized NLP by enabling models pretrained on massive text corpora to be adapted for specific tasks with minimal labeled data. This "pretrain-then-finetune" paradigm dramatically improved performance across nearly all NLP benchmarks.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-star me-2"></i>Key Insight</h4>
                            <p><strong>Pretrained language models learn general linguistic knowledge (syntax, semantics, world knowledge) that transfers effectively to downstream tasks, reducing the need for task-specific labeled data.</strong></p>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-map-signs me-2"></i>Complete NLP Series Navigation</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">16-Part Series</span>
                                <span class="badge bg-crimson">NLP Mastery</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><a href="nlp-fundamentals-linguistic-basics.html">NLP Fundamentals & Linguistic Basics</a></li>
                                    <li><a href="nlp-tokenization-text-cleaning.html">Tokenization & Text Cleaning</a></li>
                                    <li><a href="nlp-text-representation-features.html">Text Representation & Feature Engineering</a></li>
                                    <li><a href="nlp-word-embeddings.html">Word Embeddings</a></li>
                                    <li><a href="nlp-statistical-language-models.html">Statistical Language Models & N-grams</a></li>
                                    <li><a href="nlp-neural-networks.html">Neural Networks for NLP</a></li>
                                    <li><a href="nlp-rnn-lstm-gru.html">RNNs, LSTMs & GRUs</a></li>
                                    <li><a href="nlp-transformers-attention.html">Transformers & Attention Mechanism</a></li>
                                    <li><strong>Pretrained Language Models & Transfer Learning (This Guide)</strong></li>
                                    <li><a href="nlp-gpt-text-generation.html">GPT Models & Text Generation</a></li>
                                    <li><a href="nlp-core-tasks.html">Core NLP Tasks</a></li>
                                    <li><a href="nlp-advanced-tasks.html">Advanced NLP Tasks</a></li>
                                    <li><a href="nlp-multilingual-crosslingual.html">Multilingual & Cross-lingual NLP</a></li>
                                    <li><a href="nlp-evaluation-ethics.html">Evaluation, Ethics & Responsible NLP</a></li>
                                    <li><a href="nlp-systems-production.html">NLP Systems, Optimization & Production</a></li>
                                    <li><a href="nlp-cutting-edge-research.html">Cutting-Edge & Research Topics</a></li>
                                </ol>
                            </div>
                        </div>

                        <h2 id="pretraining"><i class="fas fa-dumbbell me-2"></i>Pretraining Objectives</h2>

                        <p>Pretraining objectives define how models learn from unlabeled text. These self-supervised tasks allow models to capture linguistic patterns, semantic relationships, and world knowledge from massive corpora without requiring expensive manual annotations. The choice of pretraining objective significantly impacts what the model learns and how well it transfers to downstream tasks.</p>

                        <p>Different objectives encourage different representations: some focus on understanding context bidirectionally, others on predicting future tokens, and some combine multiple objectives to capture diverse linguistic phenomena. Understanding these objectives helps practitioners choose the right pretrained model for their specific use case.</p>

                        <h3 id="mlm">Masked Language Modeling (MLM)</h3>
                        
                        <p>Masked Language Modeling is BERT's core pretraining objective. During training, 15% of tokens are randomly selected for prediction: 80% are replaced with [MASK], 10% with random tokens, and 10% remain unchanged. The model learns to predict the original tokens using bidirectional context, enabling deep understanding of word relationships and sentence structure.</p>

                        <p>This approach differs fundamentally from traditional left-to-right language models by allowing the model to attend to both preceding and following context simultaneously. The masking strategy prevents the model from simply copying the input and forces it to develop robust representations of linguistic patterns.</p>

<pre><code class="language-python">from transformers import BertTokenizer, BertForMaskedLM
import torch

# Load BERT for masked language modeling
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')

# Create input with [MASK] token
text = "The capital of France is [MASK]."
inputs = tokenizer(text, return_tensors='pt')

# Get predictions
with torch.no_grad():
    outputs = model(**inputs)
    predictions = outputs.logits

# Find the masked position
mask_idx = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]

# Get top 5 predictions
top_tokens = predictions[0, mask_idx, :].topk(5)
print("Top 5 predictions for [MASK]:")
for score, token_id in zip(top_tokens.values[0], top_tokens.indices[0]):
    print(f"  {tokenizer.decode([token_id])}: {score.item():.3f}")</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>MLM Training Details</h4>
                            <p><strong>The 15% masking rate balances two factors:</strong> too little masking means inefficient learning from each example, while too much masking removes too much context for accurate prediction. The 80-10-10 split (MASK/random/unchanged) helps the model handle real text at inference time where there are no [MASK] tokens.</p>
                        </div>

                        <h3 id="nsp">Next Sentence Prediction (NSP)</h3>
                        
                        <p>Next Sentence Prediction is BERT's secondary pretraining objective, designed to help the model understand relationships between sentences. During training, the model receives pairs of sentences and must predict whether the second sentence actually follows the first in the original document or is a randomly sampled sentence from elsewhere in the corpus.</p>

                        <p>While NSP was intended to improve performance on tasks requiring sentence-pair understanding (like question answering and natural language inference), later research showed it may not be essential. RoBERTa demonstrated that removing NSP and focusing solely on MLM with longer sequences can achieve better results on many benchmarks.</p>

<pre><code class="language-python">from transformers import BertTokenizer, BertForNextSentencePrediction
import torch

# Load BERT for NSP
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')

# Example: Related sentences (should predict "is next")
sentence_a = "The weather is beautiful today."
sentence_b = "I think I'll go for a walk in the park."

# Tokenize the sentence pair
encoding = tokenizer(
    sentence_a, 
    sentence_b, 
    return_tensors='pt',
    padding=True,
    truncation=True
)

# Get prediction
with torch.no_grad():
    outputs = model(**encoding)
    logits = outputs.logits

# Interpret results (0 = is_next, 1 = not_next)
probs = torch.softmax(logits, dim=1)
is_next_prob = probs[0, 0].item()
not_next_prob = probs[0, 1].item()

print(f"Sentence A: {sentence_a}")
print(f"Sentence B: {sentence_b}")
print(f"Is Next probability: {is_next_prob:.3f}")
print(f"Not Next probability: {not_next_prob:.3f}")</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-flask me-2"></i>Other Pretraining Objectives</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Beyond MLM</span>
                                <span class="badge bg-crimson">Alternative Approaches</span>
                            </div>
                            <div class="content">
                                <ul>
                                    <li><strong>Sentence Order Prediction (SOP):</strong> ALBERT uses this instead of NSP—predicts if two consecutive sentences are in correct order</li>
                                    <li><strong>Replaced Token Detection:</strong> ELECTRA's approach—detect which tokens were replaced by a generator model</li>
                                    <li><strong>Causal Language Modeling:</strong> GPT's approach—predict next token given all previous tokens</li>
                                    <li><strong>Span Masking:</strong> SpanBERT masks contiguous spans rather than random tokens</li>
                                    <li><strong>Translation Language Modeling:</strong> XLM uses parallel corpora to learn cross-lingual representations</li>
                                </ul>
                            </div>
                        </div>

                        <h2 id="bert"><i class="fas fa-brain me-2"></i>BERT Architecture</h2>
                        
                        <p><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong> is a transformer encoder stack that processes input text bidirectionally. Unlike previous models that read text left-to-right or combined left-to-right and right-to-left representations, BERT uses self-attention to consider all positions simultaneously, enabling each token to attend to every other token regardless of position.</p>

                        <p>BERT comes in two main sizes: BERT-Base (12 layers, 768 hidden dimensions, 12 attention heads, 110M parameters) and BERT-Large (24 layers, 1024 hidden dimensions, 16 attention heads, 340M parameters). The model uses WordPiece tokenization with a vocabulary of 30,000 tokens and includes special tokens like [CLS] for classification and [SEP] to separate sentence pairs.</p>

                        <p>The [CLS] token's final hidden state serves as the aggregate sequence representation for classification tasks. Each input sequence begins with [CLS], and for sentence-pair tasks, [SEP] tokens separate the two sentences. Learned position embeddings and segment embeddings are added to token embeddings before processing through the transformer layers.</p>

<pre><code class="language-python">from transformers import BertModel, BertTokenizer
import torch

# Load BERT model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Print model architecture summary
print("BERT-Base Architecture:")
print(f"  Vocabulary size: {model.config.vocab_size}")
print(f"  Hidden size: {model.config.hidden_size}")
print(f"  Num layers: {model.config.num_hidden_layers}")
print(f"  Num attention heads: {model.config.num_attention_heads}")
print(f"  Max position embeddings: {model.config.max_position_embeddings}")
print(f"  Total parameters: {sum(p.numel() for p in model.parameters()):,}")

# Example encoding
text = "BERT learns bidirectional representations."
inputs = tokenizer(text, return_tensors='pt')

# Get outputs
with torch.no_grad():
    outputs = model(**inputs)

# Examine output shapes
print(f"\nOutput shapes:")
print(f"  last_hidden_state: {outputs.last_hidden_state.shape}")  # [batch, seq_len, hidden]
print(f"  pooler_output (CLS): {outputs.pooler_output.shape}")     # [batch, hidden]</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-cube me-2"></i>BERT Input Representation</h4>
                            <p><strong>BERT's input embedding is the sum of three components:</strong></p>
                            <ul>
                                <li><strong>Token Embeddings:</strong> WordPiece token representations from the vocabulary</li>
                                <li><strong>Position Embeddings:</strong> Learned embeddings for positions 0 to 511</li>
                                <li><strong>Segment Embeddings:</strong> Distinguish first sentence (A) from second sentence (B)</li>
                            </ul>
                        </div>

<pre><code class="language-python">from transformers import BertTokenizer, BertModel
import torch

# Load model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Sentence pair example
sentence_a = "How old are you?"
sentence_b = "I am 25 years old."

# Tokenize with special tokens
encoding = tokenizer(
    sentence_a,
    sentence_b,
    return_tensors='pt',
    padding=True,
    truncation=True,
    max_length=128
)

# Examine tokenization
tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])
print("Tokens:", tokens)
print("\nSpecial tokens:")
print(f"  [CLS] id: {tokenizer.cls_token_id}")
print(f"  [SEP] id: {tokenizer.sep_token_id}")
print(f"  [MASK] id: {tokenizer.mask_token_id}")
print(f"  [PAD] id: {tokenizer.pad_token_id}")

# Token type IDs show sentence segments
print(f"\nToken type IDs: {encoding['token_type_ids'][0].tolist()}")
print("  (0 = sentence A, 1 = sentence B)")</code></pre>

                        <h2 id="variants"><i class="fas fa-code-branch me-2"></i>BERT Variants</h2>

                        <p>Following BERT's success, researchers developed numerous variants that improved upon the original architecture through better pretraining strategies, more efficient architectures, or different training objectives. These variants offer trade-offs between performance, efficiency, and computational requirements.</p>

                        <h3 id="roberta">RoBERTa</h3>
                        
                        <p><strong>RoBERTa (Robustly Optimized BERT Approach)</strong> by Facebook AI demonstrated that BERT was significantly undertrained. By training longer on more data, using dynamic masking, removing NSP, and training with larger batches, RoBERTa achieved state-of-the-art results on most NLP benchmarks while using the same architecture as BERT.</p>

                        <p>Key improvements include: training on 10x more data (160GB vs 16GB), removing the problematic NSP objective, using dynamic masking (different masks each epoch rather than static), larger mini-batches (8K sequences), and training for longer (500K steps vs 1M steps but with larger batches). These seemingly simple changes led to substantial performance gains.</p>

<pre><code class="language-python">from transformers import RobertaTokenizer, RobertaModel
import torch

# Load RoBERTa
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

# RoBERTa uses byte-level BPE, slightly different from BERT
text = "RoBERTa uses byte-level BPE tokenization."
inputs = tokenizer(text, return_tensors='pt')

print("RoBERTa-Base Configuration:")
print(f"  Hidden size: {model.config.hidden_size}")
print(f"  Vocabulary size: {model.config.vocab_size}")
print(f"  Parameters: {sum(p.numel() for p in model.parameters()):,}")

# Get embeddings
with torch.no_grad():
    outputs = model(**inputs)

# RoBERTa uses &lt;s&gt; and &lt;/s&gt; instead of [CLS] and [SEP]
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
print(f"\nTokens: {tokens}")
print(f"CLS embedding shape: {outputs.last_hidden_state[:, 0, :].shape}")</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-chart-bar me-2"></i>BERT vs RoBERTa Key Differences</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Training Improvements</span>
                            </div>
                            <div class="content">
                                <table class="table table-bordered">
                                    <thead>
                                        <tr><th>Aspect</th><th>BERT</th><th>RoBERTa</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>Training Data</td><td>16GB (BooksCorpus + Wikipedia)</td><td>160GB (+ CC-News, OpenWebText, Stories)</td></tr>
                                        <tr><td>NSP Objective</td><td>Yes</td><td>No (removed)</td></tr>
                                        <tr><td>Masking Strategy</td><td>Static (same masks)</td><td>Dynamic (different each epoch)</td></tr>
                                        <tr><td>Batch Size</td><td>256</td><td>8,192</td></tr>
                                        <tr><td>Training Steps</td><td>1M</td><td>500K (but with larger batches)</td></tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>

                        <h3 id="albert">ALBERT</h3>
                        
                        <p><strong>ALBERT (A Lite BERT)</strong> by Google Research addresses BERT's memory limitations and training time through two parameter reduction techniques: factorized embedding parameterization and cross-layer parameter sharing. These changes dramatically reduce the number of parameters while maintaining or improving performance.</p>

                        <p>Factorized embedding decomposes the embedding matrix into two smaller matrices, separating the vocabulary embedding dimension (E) from the hidden layer dimension (H). Cross-layer parameter sharing means all transformer layers share the same weights, reducing parameters proportionally to the number of layers. ALBERT-xxlarge achieves state-of-the-art with only 235M parameters compared to BERT-large's 340M.</p>

<pre><code class="language-python">from transformers import AlbertTokenizer, AlbertModel
import torch

# Load ALBERT
tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')
model = AlbertModel.from_pretrained('albert-base-v2')

print("ALBERT-Base-v2 Configuration:")
print(f"  Hidden size: {model.config.hidden_size}")
print(f"  Embedding size: {model.config.embedding_size}")  # Factorized!
print(f"  Num hidden groups: {model.config.num_hidden_groups}")  # Shared layers
print(f"  Parameters: {sum(p.numel() for p in model.parameters()):,}")

# Compare with BERT parameter count
# BERT-base: ~110M, ALBERT-base: ~12M (9x smaller!)

text = "ALBERT uses parameter sharing to reduce model size."
inputs = tokenizer(text, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)

print(f"\nOutput shape: {outputs.last_hidden_state.shape}")
print(f"Tokens: {tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}")</code></pre>

                        <h3 id="electra">ELECTRA</h3>
                        
                        <p><strong>ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)</strong> uses a novel "replaced token detection" pretraining task. Instead of predicting masked tokens, ELECTRA learns to distinguish real input tokens from plausible fakes generated by a small generator network. This approach is more sample-efficient because the model learns from all tokens, not just the 15% that are masked.</p>

                        <p>The architecture consists of a generator (small MLM model) that proposes replacements for masked tokens, and a discriminator (the main model) that learns to detect which tokens were replaced. After pretraining, only the discriminator is used for downstream tasks. ELECTRA-small matches BERT-base performance while training with 1/4 the compute, making it ideal for resource-constrained settings.</p>

<pre><code class="language-python">from transformers import ElectraTokenizer, ElectraForPreTraining
import torch

# Load ELECTRA
tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')
model = ElectraForPreTraining.from_pretrained('google/electra-small-discriminator')

print("ELECTRA-Small Configuration:")
print(f"  Hidden size: {model.config.hidden_size}")
print(f"  Num layers: {model.config.num_hidden_layers}")
print(f"  Parameters: {sum(p.numel() for p in model.parameters()):,}")

# ELECTRA discriminator predicts if each token is original or replaced
sentence = "The chef cooked a delicious dinner"
fake_sentence = "The chef cooked a delicious laptop"  # "laptop" is fake

inputs = tokenizer(fake_sentence, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)

# Each position gets a score (higher = more likely replaced)
is_replaced_scores = outputs.logits.squeeze()
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

print("\nReplacement detection scores:")
for token, score in zip(tokens, is_replaced_scores.tolist()):
    status = "FAKE" if score > 0 else "real"
    print(f"  {token:12s}: {score:+.3f} ({status})")</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-tachometer-alt me-2"></i>DistilBERT: Knowledge Distillation</h4>
                            <p><strong>DistilBERT</strong> uses knowledge distillation to create a smaller, faster model that retains 97% of BERT's performance while being 60% smaller and 60% faster. The student model learns to match the teacher's output distributions, attention patterns, and hidden representations. This makes DistilBERT ideal for production deployment where inference speed matters.</p>
                        </div>

<pre><code class="language-python">from transformers import DistilBertTokenizer, DistilBertModel
import torch

# Load DistilBERT - 40% smaller, 60% faster than BERT
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased')

print("DistilBERT Configuration:")
print(f"  Hidden size: {model.config.hidden_size}")
print(f"  Num layers: {model.config.num_hidden_layers}")  # 6 vs BERT's 12
print(f"  Parameters: {sum(p.numel() for p in model.parameters()):,}")  # ~66M vs 110M

# DistilBERT has no token_type_ids (no segment embeddings)
text = "DistilBERT is fast and efficient."
inputs = tokenizer(text, return_tensors='pt')

with torch.no_grad():
    outputs = model(**inputs)

print(f"\nOutput shape: {outputs.last_hidden_state.shape}")</code></pre>

                        <h2 id="fine-tuning"><i class="fas fa-sliders-h me-2"></i>Fine-Tuning Strategies</h2>
                        
                        <p>Fine-tuning adapts a pretrained model to a specific downstream task by continuing training on task-specific labeled data. The key decisions involve: which layers to fine-tune, what learning rate to use, how long to train, and how to prevent catastrophic forgetting of the pretrained knowledge. Different strategies suit different scenarios based on data availability and task similarity to pretraining.</p>

                        <p><strong>Full fine-tuning</strong> updates all model parameters, typically with a small learning rate (2e-5 to 5e-5) to avoid overwriting pretrained knowledge. This works best when you have moderate amounts of labeled data (10K+ examples). Training typically converges in 3-4 epochs, and it's important to monitor validation performance to avoid overfitting.</p>

                        <p><strong>Feature extraction</strong> freezes pretrained weights and only trains the task-specific head. This is faster and prevents overfitting when labeled data is scarce, but may underperform full fine-tuning. A middle ground is <strong>gradual unfreezing</strong>, which starts with frozen layers and progressively unfreezes from top to bottom during training.</p>

<pre><code class="language-python">from transformers import BertForSequenceClassification, BertTokenizer
import torch

# Load pretrained BERT with classification head
model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=2  # Binary classification
)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Strategy 1: Full fine-tuning (all parameters trainable)
for param in model.parameters():
    param.requires_grad = True

trainable_full = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Full fine-tuning: {trainable_full:,} trainable parameters")

# Strategy 2: Feature extraction (freeze encoder, train classifier only)
for param in model.bert.parameters():
    param.requires_grad = False

# Only classifier is trainable
trainable_head = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Feature extraction: {trainable_head:,} trainable parameters")

# Strategy 3: Freeze embeddings and early layers only
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Freeze embeddings
for param in model.bert.embeddings.parameters():
    param.requires_grad = False

# Freeze first 6 layers
for layer in model.bert.encoder.layer[:6]:
    for param in layer.parameters():
        param.requires_grad = False

trainable_partial = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Partial fine-tuning (last 6 layers + head): {trainable_partial:,} parameters")</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-graduation-cap me-2"></i>Learning Rate Strategies</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Optimization</span>
                                <span class="badge bg-crimson">Best Practices</span>
                            </div>
                            <div class="content">
                                <ul>
                                    <li><strong>Discriminative Learning Rates:</strong> Lower LR for bottom layers (2e-5), higher for top layers (5e-5)</li>
                                    <li><strong>Linear Warmup:</strong> Start at 0, increase linearly for first 10% of steps, then decay</li>
                                    <li><strong>Layer-wise Decay:</strong> Multiply LR by 0.95 for each layer from top to bottom</li>
                                    <li><strong>Typical Range:</strong> 1e-5 to 5e-5 for fine-tuning (much smaller than pretraining)</li>
                                </ul>
                            </div>
                        </div>

<pre><code class="language-python">from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW
import torch

# Load model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Discriminative learning rates: lower for encoder, higher for classifier
optimizer_grouped_parameters = [
    {
        'params': model.bert.embeddings.parameters(),
        'lr': 1e-5  # Lowest LR for embeddings
    },
    {
        'params': model.bert.encoder.parameters(),
        'lr': 2e-5  # Medium LR for encoder layers
    },
    {
        'params': model.classifier.parameters(),
        'lr': 5e-5  # Higher LR for task head
    }
]

optimizer = AdamW(optimizer_grouped_parameters, weight_decay=0.01)

# Linear warmup + decay schedule
num_training_steps = 1000
num_warmup_steps = 100  # 10% warmup

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)

print("Learning rate schedule:")
for i, step in enumerate([0, 50, 100, 500, 1000]):
    # Simulate steps
    for _ in range(step - (i > 0 and [0, 50, 100, 500, 1000][i-1] or 0)):
        scheduler.step()
    print(f"  Step {step}: LR = {scheduler.get_last_lr()[0]:.2e}")</code></pre>

                        <h2 id="huggingface"><i class="fas fa-face-smile me-2"></i>Hugging Face Transformers</h2>
                        
                        <p>The <strong>Hugging Face Transformers</strong> library has become the de facto standard for working with pretrained language models. It provides a unified API for loading, using, and fine-tuning hundreds of models across different architectures. The library abstracts away implementation details while providing flexibility for advanced users.</p>

                        <p>Key components include: <strong>AutoClasses</strong> that automatically infer the correct model/tokenizer class, <strong>Trainer API</strong> for simplified training loops, <strong>Pipelines</strong> for inference on common tasks, and integration with the <strong>Hugging Face Hub</strong> for sharing models. The ecosystem also includes Datasets for efficient data loading and Evaluate for metrics computation.</p>

<pre><code class="language-python">from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
from transformers import pipeline
import torch

# AutoClasses automatically detect the right class
model_name = 'bert-base-uncased'

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

print(f"Loaded {model.__class__.__name__}")
print(f"Tokenizer: {tokenizer.__class__.__name__}")

# Pipelines provide one-line inference
sentiment_pipeline = pipeline(
    'sentiment-analysis',
    model='distilbert-base-uncased-finetuned-sst-2-english'
)

texts = [
    "This movie was absolutely fantastic!",
    "I really didn't enjoy this book at all.",
    "The weather is okay today."
]

results = sentiment_pipeline(texts)
for text, result in zip(texts, results):
    print(f"\n'{text[:40]}...'")
    print(f"  Label: {result['label']}, Score: {result['score']:.3f}")</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-cubes me-2"></i>Hugging Face Ecosystem</h4>
                            <p><strong>Key Libraries:</strong></p>
                            <ul>
                                <li><strong>transformers:</strong> Core library for models and tokenizers</li>
                                <li><strong>datasets:</strong> Efficient data loading with memory mapping and streaming</li>
                                <li><strong>evaluate:</strong> Metrics for model evaluation</li>
                                <li><strong>accelerate:</strong> Distributed training and mixed precision</li>
                                <li><strong>peft:</strong> Parameter-efficient fine-tuning (LoRA, adapters)</li>
                            </ul>
                        </div>

<pre><code class="language-python">from transformers import AutoTokenizer, DataCollatorWithPadding
from datasets import load_dataset
import torch

# Load a dataset from Hugging Face Hub
dataset = load_dataset('imdb', split='train[:1000]')  # First 1000 examples

print(f"Dataset features: {dataset.features}")
print(f"Number of examples: {len(dataset)}")
print(f"\nFirst example:")
print(f"  Text: {dataset[0]['text'][:100]}...")
print(f"  Label: {dataset[0]['label']}")

# Tokenize the dataset
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        truncation=True,
        max_length=512
    )

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=['text']
)

print(f"\nTokenized features: {tokenized_dataset.features}")
print(f"First example tokens: {tokenized_dataset[0]['input_ids'][:10]}...")</code></pre>

                        <h2 id="practical"><i class="fas fa-code me-2"></i>Practical Implementation</h2>
                        
                        <p>Let's implement end-to-end fine-tuning for three common NLP tasks: text classification, named entity recognition (NER), and question answering. Each example demonstrates the complete workflow from data loading to evaluation.</p>

                        <h3>Text Classification Fine-tuning</h3>
                        
                        <p>Text classification assigns labels to documents. For sentiment analysis, we fine-tune BERT to predict positive/negative sentiment from movie reviews. The model adds a linear classification head on top of the [CLS] token representation.</p>

<pre><code class="language-python">from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset
import numpy as np

# Load IMDB dataset
dataset = load_dataset('imdb')

# Load tokenizer and model
model_name = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2,
    id2label={0: 'NEGATIVE', 1: 'POSITIVE'},
    label2id={'NEGATIVE': 0, 'POSITIVE': 1}
)

# Tokenization function
def tokenize(examples):
    return tokenizer(
        examples['text'],
        truncation=True,
        max_length=256,
        padding='max_length'
    )

# Tokenize dataset
tokenized_train = dataset['train'].shuffle(seed=42).select(range(2000)).map(
    tokenize, batched=True, remove_columns=['text']
)
tokenized_test = dataset['test'].shuffle(seed=42).select(range(500)).map(
    tokenize, batched=True, remove_columns=['text']
)

print(f"Training examples: {len(tokenized_train)}")
print(f"Test examples: {len(tokenized_test)}")
print(f"Features: {tokenized_train.features}")</code></pre>

<pre><code class="language-python">from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
from datasets import load_dataset
import numpy as np

# Setup (abbreviated - uses tokenized data from above)
model_name = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Define compute_metrics function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = (predictions == labels).mean()
    return {'accuracy': accuracy}

# Training arguments
training_args = TrainingArguments(
    output_dir='./sentiment_model',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=2e-5,
    weight_decay=0.01,
    warmup_ratio=0.1,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    logging_steps=50,
    report_to='none'  # Disable wandb/tensorboard
)

print("Training Configuration:")
print(f"  Epochs: {training_args.num_train_epochs}")
print(f"  Batch size: {training_args.per_device_train_batch_size}")
print(f"  Learning rate: {training_args.learning_rate}")
print(f"  Warmup ratio: {training_args.warmup_ratio}")</code></pre>

                        <h3>Named Entity Recognition (NER)</h3>
                        
                        <p>NER identifies and classifies named entities (persons, organizations, locations, etc.) in text. This is a token classification task where each token receives a label. We use the BIO tagging scheme: B-PER (beginning of person), I-PER (inside person), O (outside any entity).</p>

<pre><code class="language-python">from transformers import (
    AutoTokenizer,
    AutoModelForTokenClassification,
    pipeline
)
import torch

# Load pre-trained NER model
model_name = 'dslim/bert-base-NER'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

# Check label mapping
print("NER Labels:")
for id, label in model.config.id2label.items():
    print(f"  {id}: {label}")

# Use pipeline for easy inference
ner_pipeline = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy='simple')

# Test on sample text
text = "Apple Inc. was founded by Steve Jobs in Cupertino, California."
entities = ner_pipeline(text)

print(f"\nText: {text}")
print("\nDetected Entities:")
for entity in entities:
    print(f"  {entity['word']:20s} | {entity['entity_group']:5s} | Score: {entity['score']:.3f}")</code></pre>

<pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

# Manual token classification (understanding the internals)
model_name = 'dslim/bert-base-NER'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

text = "Barack Obama was the 44th President of the United States."
inputs = tokenizer(text, return_tensors='pt', return_offsets_mapping=True)

# Get predictions
with torch.no_grad():
    outputs = model(
        input_ids=inputs['input_ids'],
        attention_mask=inputs['attention_mask']
    )

# Convert logits to predictions
predictions = torch.argmax(outputs.logits, dim=-1).squeeze().tolist()
tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze())

print("Token-level NER predictions:")
print("-" * 50)
for token, pred_id in zip(tokens, predictions):
    label = model.config.id2label[pred_id]
    if label != 'O':  # Only show entities
        print(f"  {token:15s} -> {label}")</code></pre>

                        <h3>Question Answering</h3>
                        
                        <p>Extractive question answering finds answer spans within a given context. The model learns to predict start and end positions of the answer in the context. This requires handling long contexts with sliding window approaches when context exceeds the maximum sequence length.</p>

<pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline
import torch

# Load QA model
model_name = 'distilbert-base-cased-distilled-squad'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

# Use pipeline for easy inference
qa_pipeline = pipeline('question-answering', model=model, tokenizer=tokenizer)

# Example context and questions
context = """
The Transformer architecture was introduced in the paper "Attention Is All You Need" 
by Vaswani et al. in 2017. It revolutionized natural language processing by replacing 
recurrent neural networks with self-attention mechanisms. The original Transformer 
was designed for machine translation but has since been adapted for virtually all NLP tasks.
BERT, introduced by Google in 2018, is an encoder-only Transformer pretrained using 
masked language modeling. GPT, from OpenAI, is a decoder-only Transformer using causal 
language modeling.
"""

questions = [
    "When was the Transformer introduced?",
    "What did the Transformer replace?",
    "Who introduced BERT?",
    "What type of Transformer is GPT?"
]

print("Question Answering Demo")
print("=" * 60)
for question in questions:
    result = qa_pipeline(question=question, context=context)
    print(f"\nQ: {question}")
    print(f"A: {result['answer']} (confidence: {result['score']:.3f})")</code></pre>

<pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

# Manual QA (understanding start/end logits)
model_name = 'distilbert-base-cased-distilled-squad'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)

question = "What is BERT pretrained with?"
context = "BERT is pretrained using masked language modeling and next sentence prediction."

# Tokenize
inputs = tokenizer(
    question,
    context,
    return_tensors='pt',
    truncation=True,
    max_length=512
)

# Get start/end logits
with torch.no_grad():
    outputs = model(**inputs)

# Find best start and end positions
start_idx = torch.argmax(outputs.start_logits)
end_idx = torch.argmax(outputs.end_logits)

# Decode answer
input_ids = inputs['input_ids'].squeeze()
answer_tokens = input_ids[start_idx:end_idx + 1]
answer = tokenizer.decode(answer_tokens)

print(f"Question: {question}")
print(f"Context: {context}")
print(f"\nStart position: {start_idx.item()}, End position: {end_idx.item()}")
print(f"Answer: {answer}")

# Show confidence scores
start_prob = torch.softmax(outputs.start_logits, dim=-1)[0, start_idx].item()
end_prob = torch.softmax(outputs.end_logits, dim=-1)[0, end_idx].item()
print(f"Start confidence: {start_prob:.3f}, End confidence: {end_prob:.3f}")</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-rocket me-2"></i>Complete Fine-tuning Example with Trainer</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Full Pipeline</span>
                                <span class="badge bg-crimson">Production Ready</span>
                            </div>
                            <div class="content">
                                <p>Here's a complete, runnable fine-tuning script combining all components:</p>
                            </div>
                        </div>

<pre><code class="language-python">from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from datasets import load_dataset
import numpy as np
import torch

# Configuration
MODEL_NAME = 'distilbert-base-uncased'
NUM_LABELS = 2
MAX_LENGTH = 256
TRAIN_SIZE = 5000
TEST_SIZE = 1000

# Load and prepare data
print("Loading dataset...")
dataset = load_dataset('imdb')
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def preprocess(examples):
    return tokenizer(
        examples['text'],
        truncation=True,
        max_length=MAX_LENGTH,
        padding='max_length'
    )

train_data = dataset['train'].shuffle(seed=42).select(range(TRAIN_SIZE))
test_data = dataset['test'].shuffle(seed=42).select(range(TEST_SIZE))

train_tokenized = train_data.map(preprocess, batched=True, remove_columns=['text'])
test_tokenized = test_data.map(preprocess, batched=True, remove_columns=['text'])

# Load model
print("Loading model...")
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=NUM_LABELS
)

# Metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = (predictions == labels).mean()
    return {'accuracy': accuracy}

# Training setup
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    learning_rate=2e-5,
    warmup_steps=500,
    weight_decay=0.01,
    evaluation_strategy='steps',
    eval_steps=500,
    save_steps=500,
    logging_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    report_to='none'
)

# Create Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=test_tokenized,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

print("Ready to train! Call trainer.train() to start.")
print(f"Training samples: {len(train_tokenized)}")
print(f"Evaluation samples: {len(test_tokenized)}")</code></pre>

                        <h3>Feature Extraction vs Fine-tuning Comparison</h3>
                        
                        <p>Let's compare the two approaches on the same task to understand the trade-offs. Feature extraction is faster but may underperform; fine-tuning is slower but usually achieves better results.</p>

<pre><code class="language-python">from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
import torch
import torch.nn as nn

# Approach 1: Feature Extraction (frozen encoder)
class FeatureExtractor(nn.Module):
    def __init__(self, model_name, num_labels):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        # Freeze encoder
        for param in self.encoder.parameters():
            param.requires_grad = False
        # Trainable classifier
        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)
    
    def forward(self, input_ids, attention_mask):
        with torch.no_grad():  # No gradients through encoder
            outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = outputs.last_hidden_state[:, 0, :]
        return self.classifier(cls_embedding)

# Approach 2: Full Fine-tuning
class FullFineTuner(nn.Module):
    def __init__(self, model_name, num_labels):
        super().__init__()
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, num_labels=num_labels
        )
    
    def forward(self, input_ids, attention_mask):
        return self.model(input_ids=input_ids, attention_mask=attention_mask).logits

# Compare parameter counts
model_name = 'distilbert-base-uncased'
num_labels = 2

fe_model = FeatureExtractor(model_name, num_labels)
ff_model = FullFineTuner(model_name, num_labels)

fe_trainable = sum(p.numel() for p in fe_model.parameters() if p.requires_grad)
ff_trainable = sum(p.numel() for p in ff_model.parameters() if p.requires_grad)

print("Parameter Comparison:")
print(f"  Feature Extraction: {fe_trainable:,} trainable params")
print(f"  Full Fine-tuning:   {ff_trainable:,} trainable params")
print(f"  Ratio: {ff_trainable / fe_trainable:.0f}x more parameters in fine-tuning")</code></pre>

                        <h2 id="conclusion"><i class="fas fa-flag-checkered me-2"></i>Conclusion & Next Steps</h2>
                        
                        <p>Pretrained language models have fundamentally transformed NLP by enabling transfer learning at scale. BERT introduced bidirectional pretraining through masked language modeling, while variants like RoBERTa, ALBERT, and ELECTRA offered improvements in training efficiency, model compression, and sample efficiency. The Hugging Face ecosystem has made these powerful models accessible to practitioners worldwide.</p>

                        <p>Key takeaways from this guide include understanding the pretraining-finetuning paradigm, choosing appropriate models based on your constraints (compute, data, latency), and implementing effective fine-tuning strategies. For most practitioners, starting with DistilBERT or BERT-base provides a good balance of performance and efficiency, while RoBERTa offers state-of-the-art quality when resources permit.</p>

                        <p>In the next part of our series, we'll explore <strong>GPT Models & Text Generation</strong>, diving into autoregressive language models, the decoder-only architecture, and techniques for controlled text generation including sampling strategies, prompt engineering, and instruction following.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-tasks me-2"></i>Practice Exercises</h4>
                            <ol>
                                <li>Fine-tune BERT on a custom text classification dataset using the Trainer API</li>
                                <li>Compare RoBERTa and DistilBERT on the same task—measure accuracy vs. inference speed</li>
                                <li>Implement gradual unfreezing: freeze all layers initially, unfreeze one layer per epoch</li>
                                <li>Use ELECTRA for NER and compare performance with BERT-based models</li>
                                <li>Build a question-answering system that handles long documents with sliding windows</li>
                            </ol>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-book-open me-2"></i>Further Reading</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Papers</span>
                                <span class="badge bg-crimson">Resources</span>
                            </div>
                            <div class="content">
                                <ul>
                                    <li><strong>BERT Paper:</strong> "BERT: Pre-training of Deep Bidirectional Transformers" (Devlin et al., 2019)</li>
                                    <li><strong>RoBERTa Paper:</strong> "RoBERTa: A Robustly Optimized BERT Pretraining Approach" (Liu et al., 2019)</li>
                                    <li><strong>ALBERT Paper:</strong> "ALBERT: A Lite BERT for Self-supervised Learning" (Lan et al., 2020)</li>
                                    <li><strong>ELECTRA Paper:</strong> "ELECTRA: Pre-training Text Encoders as Discriminators" (Clark et al., 2020)</li>
                                    <li><strong>Hugging Face Course:</strong> <a href="https://huggingface.co/course" target="_blank">huggingface.co/course</a></li>
                                </ul>
                            </div>
                        </div>

                        <div class="related-posts">
                            <h3><i class="fas fa-book-reader me-2"></i>Continue the NLP Series</h3>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 8: Transformers & Attention Mechanism</h5>
                                <p class="text-muted small mb-2">Understand the architecture that revolutionized modern NLP.</p>
                                <a href="nlp-transformers-attention.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 10: GPT Models & Text Generation</h5>
                                <p class="text-muted small mb-2">Explore autoregressive language models and generation techniques.</p>
                                <a href="nlp-gpt-text-generation.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 11: Core NLP Tasks</h5>
                                <p class="text-muted small mb-2">Apply NLP to classification, NER, POS tagging, and parsing.</p>
                                <a href="nlp-core-tasks.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.</p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook"><i class="fab fa-facebook-f"></i></a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter"><i class="fab fa-twitter"></i></a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn"><i class="fab fa-linkedin-in"></i></a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube"><i class="fab fa-youtube"></i></a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram"><i class="fab fa-instagram"></i></a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest"><i class="fab fa-pinterest-p"></i></a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email"><i class="fas fa-envelope"></i></a>
                    </div>
                </div>
            </div>
            <hr class="bg-secondary">
            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small"><i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a></p>
                    <p class="small mt-3"><a href="/" class="text-light text-decoration-none">Home</a> | <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a></p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">Enjoying this content? ☕ <a href="https://buymeacoffee.com/itswzee" target="_blank" class="text-light" style="text-decoration: underline;">Keep me caffeinated</a> to keep the pixels flowing!</p>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top"><i class="fas fa-arrow-up"></i></button>
    <script src="../../../js/cookie-consent.js"></script>
    <script src="../../../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <script>
        const themes = { 'prism-theme': 'Tomorrow Night', 'prism-default': 'Default', 'prism-dark': 'Dark', 'prism-twilight': 'Twilight', 'prism-okaidia': 'Okaidia', 'prism-solarizedlight': 'Solarized Light' };
        const savedTheme = localStorage.getItem('prism-theme') || 'prism-theme';
        function switchTheme(themeId) { Object.keys(themes).forEach(id => { const link = document.getElementById(id); if (link) link.disabled = true; }); const selectedLink = document.getElementById(themeId); if (selectedLink) { selectedLink.disabled = false; localStorage.setItem('prism-theme', themeId); } document.querySelectorAll('div.code-toolbar select').forEach(dropdown => { dropdown.value = themeId; }); setTimeout(() => Prism.highlightAll(), 10); }
        document.addEventListener('DOMContentLoaded', function() { switchTheme(savedTheme); });
        Prism.plugins.toolbar.registerButton('theme-switcher', function(env) { const select = document.createElement('select'); select.setAttribute('aria-label', 'Select code theme'); Object.keys(themes).forEach(themeId => { const option = document.createElement('option'); option.value = themeId; option.textContent = themes[themeId]; if (themeId === savedTheme) option.selected = true; select.appendChild(option); }); select.addEventListener('change', function(e) { switchTheme(e.target.value); }); return select; });
    </script>

    <script>
        document.addEventListener('DOMContentLoaded', function() { const scrollToTopBtn = document.getElementById('scrollToTop'); window.addEventListener('scroll', function() { if (window.scrollY > 300) { scrollToTopBtn.classList.add('show'); } else { scrollToTopBtn.classList.remove('show'); } }); scrollToTopBtn.addEventListener('click', function() { window.scrollTo({ top: 0, behavior: 'smooth' }); }); });
        function openNav() { document.getElementById('tocSidenav').classList.add('open'); document.getElementById('tocOverlay').classList.add('show'); document.body.style.overflow = 'hidden'; }
        function closeNav() { document.getElementById('tocSidenav').classList.remove('open'); document.getElementById('tocOverlay').classList.remove('show'); document.body.style.overflow = 'auto'; }
        document.addEventListener('keydown', function(e) { if (e.key === 'Escape') closeNav(); });
    </script>
</body>
</html>
