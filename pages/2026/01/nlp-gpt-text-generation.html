<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Part 10 of the Complete NLP Series: Master GPT models and text generation—autoregressive language modeling, GPT architecture evolution (GPT-1 to GPT-4), decoding strategies (greedy, beam search, sampling, nucleus), and prompt engineering." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="NLP, GPT, Text Generation, Language Models, Autoregressive, GPT-3, GPT-4, Decoding, Sampling, Prompt Engineering, OpenAI" />
    <meta property="og:title" content="GPT Models & Text Generation - Complete NLP Series Part 10" />
    <meta property="og:description" content="Explore autoregressive language models, GPT architecture, and text generation techniques." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-27" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>GPT Models & Text Generation - Complete NLP Series Part 10 - Wasil Zafar</title>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('consent', 'default', { 'ad_storage': 'denied', 'ad_user_data': 'denied', 'ad_personalization': 'denied', 'analytics_storage': 'denied', 'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE'] });
        gtag('consent', 'default', { 'ad_storage': 'granted', 'ad_user_data': 'granted', 'ad_personalization': 'granted', 'analytics_storage': 'granted' });
        gtag('set', 'url_passthrough', true);
    </script>
    <script>
        (function(w, d, s, l, i) { w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' }); var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f); })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    <style>
        .blog-hero { background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%); color: white; padding: 80px 0; }
        .blog-meta { font-size: 0.95rem; color: var(--color-teal); margin-bottom: 1rem; display: flex; align-items: center; flex-wrap: wrap; gap: 1rem; }
        .print-btn { background: var(--color-teal); color: white; border: none; padding: 0.4rem 1rem; border-radius: 4px; font-size: 0.9rem; cursor: pointer; transition: all 0.3s ease; display: inline-flex; align-items: center; gap: 0.5rem; }
        .print-btn:hover { background: var(--color-crimson); transform: translateY(-1px); }
        @media print { .print-btn, nav, .navbar, footer, .back-link, .related-posts, .scroll-to-top, .toc-toggle-btn, .sidenav-toc, .sidenav-overlay { display: none !important; } * { -webkit-print-color-adjust: exact !important; print-color-adjust: exact !important; } }
        .blog-content { max-width: 900px; margin: 0 auto; font-size: 1.05rem; line-height: 1.8; color: #333; }
        .blog-content h2 { font-size: 1.8rem; font-weight: 700; margin-top: 2.5rem; margin-bottom: 1.5rem; color: var(--color-navy); border-bottom: 3px solid var(--color-teal); padding-bottom: 0.5rem; }
        .blog-content h3 { font-size: 1.3rem; font-weight: 600; margin-top: 2rem; margin-bottom: 1rem; color: var(--color-blue); }
        .blog-content h4 { font-size: 1.1rem; font-weight: 600; margin-top: 1.5rem; margin-bottom: 1rem; color: var(--color-teal); }
        .blog-content p { margin-bottom: 1.2rem; text-align: justify; }
        .blog-content strong { color: var(--color-crimson); }
        .blog-content pre[class*="language-"] { border-radius: 6px; margin: 1.5rem 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); }
        .blog-content p code, .blog-content li code { background: rgba(59, 151, 151, 0.1); color: var(--color-crimson); padding: 0.2rem 0.4rem; border-radius: 3px; font-family: 'Consolas', monospace; font-size: 0.9em; }
        .highlight-box { background: rgba(59, 151, 151, 0.1); border-left: 4px solid var(--color-teal); padding: 1.5rem; margin: 2rem 0; border-radius: 4px; }
        .experiment-card { background: #f8f9fa; border: 1px solid #ddd; border-radius: 8px; padding: 1.5rem; margin-bottom: 1.5rem; transition: all 0.3s ease; }
        .experiment-card:hover { box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1); transform: translateY(-2px); }
        .experiment-card h4 { color: var(--color-crimson); font-weight: 700; margin-bottom: 0.5rem; }
        .bg-teal { background-color: var(--color-teal) !important; }
        .bg-crimson { background-color: var(--color-crimson) !important; }
        .toc-toggle-btn { position: fixed; bottom: 2rem; left: 2rem; width: 50px; height: 50px; background: var(--color-teal); color: white; border: none; border-radius: 50%; font-size: 1.2rem; cursor: pointer; box-shadow: 0 4px 12px rgba(59, 151, 151, 0.4); transition: all 0.3s ease; z-index: 1049; display: flex; align-items: center; justify-content: center; }
        .toc-toggle-btn:hover { background: var(--color-crimson); transform: scale(1.1); }
        .sidenav-toc { height: calc(100% - 64px); width: 0; position: fixed; z-index: 1050; top: 64px; left: 0; background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%); overflow-x: hidden; overflow-y: auto; transition: width 0.4s ease; padding-top: 30px; box-shadow: 4px 0 15px rgba(0, 0, 0, 0.3); }
        .sidenav-toc.open { width: 350px; }
        .sidenav-toc .toc-header { display: flex; align-items: center; justify-content: space-between; padding: 20px 30px; margin-bottom: 20px; border-bottom: 2px solid var(--color-teal); opacity: 0; visibility: hidden; transition: all 0.3s ease; }
        .sidenav-toc.open .toc-header { opacity: 1; visibility: visible; }
        .sidenav-toc .closebtn { font-size: 32px; color: white; background: transparent; border: none; cursor: pointer; transition: all 0.3s ease; }
        .sidenav-toc .closebtn:hover { color: var(--color-crimson); transform: rotate(90deg); }
        .sidenav-toc h3 { color: white; margin: 0; font-weight: 700; font-size: 1.3rem; }
        .sidenav-toc ol { list-style: decimal; padding-left: 30px; margin: 0; color: rgba(255, 255, 255, 0.9); }
        .sidenav-toc ol li { margin-bottom: 8px; }
        .sidenav-toc ul { list-style-type: lower-alpha; padding-left: 30px; margin-top: 8px; }
        .sidenav-toc a { padding: 12px 30px; text-decoration: none; font-size: 0.95rem; color: rgba(255, 255, 255, 0.85); display: block; transition: all 0.3s ease; border-left: 4px solid transparent; }
        .sidenav-toc a:hover { color: white; background: rgba(59, 151, 151, 0.2); border-left-color: var(--color-teal); }
        .sidenav-toc a.active { color: white; background: rgba(191, 9, 47, 0.3); border-left-color: var(--color-crimson); font-weight: 600; }
        .sidenav-overlay { display: none; position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0, 0, 0, 0.5); z-index: 1049; }
        .sidenav-overlay.show { display: block; }
        .reading-time { display: inline-block; background: var(--color-crimson); color: white; padding: 0.3rem 0.8rem; border-radius: 4px; font-size: 0.9rem; }
        .back-link { display: inline-block; color: white; text-decoration: none; transition: all 0.3s ease; margin-bottom: 1rem; opacity: 0.9; }
        .back-link:hover { color: var(--color-teal); transform: translateX(-5px); }
        .related-posts { background: #f8f9fa; border-radius: 8px; padding: 2rem; margin-top: 3rem; }
        .related-posts h3 { color: var(--color-navy); margin-bottom: 1.5rem; }
        .related-post-item { padding: 1rem; border-left: 3px solid var(--color-teal); margin-bottom: 1rem; transition: all 0.3s ease; }
        .related-post-item:hover { background: white; border-left-color: var(--color-crimson); }
        .related-post-item a { color: var(--color-blue); text-decoration: none; font-weight: 600; }
        .related-post-item a:hover { color: var(--color-crimson); }
        div.code-toolbar > .toolbar { opacity: 1; display: flex; gap: 0.5rem; }
        div.code-toolbar > .toolbar > .toolbar-item > button { background: var(--color-teal); color: white; border: none; padding: 0.4rem 0.8rem; border-radius: 4px; font-size: 0.85rem; cursor: pointer; }
        div.code-toolbar > .toolbar > .toolbar-item > select { background: var(--color-navy); color: white; border: 1px solid var(--color-teal); padding: 0.4rem 0.8rem; border-radius: 4px; font-size: 0.85rem; }
        .scroll-to-top { position: fixed; bottom: 2rem; right: 2rem; width: 50px; height: 50px; background: var(--color-teal); color: white; border: none; border-radius: 50%; font-size: 1.2rem; cursor: pointer; display: flex; align-items: center; justify-content: center; opacity: 0; visibility: hidden; transition: all 0.3s ease; box-shadow: 0 4px 12px rgba(59, 151, 151, 0.3); z-index: 999; }
        .scroll-to-top.show { opacity: 1; visibility: visible; }
        .scroll-to-top:hover { background: var(--color-crimson); transform: translateY(-3px); }
        @media (max-width: 768px) { .sidenav-toc.open { width: 280px; } .toc-toggle-btn { left: 15px; } }
        html { scroll-behavior: smooth; }
    </style>
</head>
<body>
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/"><span class="gradient-text">Wasil Zafar</span></a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link" href="/">Home</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#about">About</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#certifications">Certifications</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#interests">Interests</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link"><i class="fas fa-arrow-left me-2"></i>Back to Technology</a>
                <h1 class="display-4 fw-bold mb-3">GPT Models & Text Generation</h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 27, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-1"></i>45 min read</span>
                    <button onclick="window.print()" class="print-btn" title="Print this article"><i class="fas fa-print"></i> Print</button>
                </div>
                <p class="lead">Part 10 of 16: Explore autoregressive language models, GPT architecture evolution, decoding strategies, and prompt engineering techniques.</p>
            </div>
        </div>
    </section>

    <button class="toc-toggle-btn" onclick="openNav()" title="Table of Contents" aria-label="Open Table of Contents"><i class="fas fa-list"></i></button>

    <div id="tocSidenav" class="sidenav-toc">
        <div class="toc-header">
            <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
            <button class="closebtn" onclick="closeNav()" aria-label="Close">&times;</button>
        </div>
        <ol>
            <li><a href="#introduction" onclick="closeNav()">Introduction to Generative Models</a></li>
            <li><a href="#autoregressive" onclick="closeNav()">Autoregressive Language Modeling</a></li>
            <li><a href="#gpt-evolution" onclick="closeNav()">GPT Architecture Evolution</a>
                <ul>
                    <li><a href="#gpt1" onclick="closeNav()">GPT-1</a></li>
                    <li><a href="#gpt2" onclick="closeNav()">GPT-2</a></li>
                    <li><a href="#gpt3" onclick="closeNav()">GPT-3 & In-Context Learning</a></li>
                    <li><a href="#gpt4" onclick="closeNav()">GPT-4 & Beyond</a></li>
                </ul>
            </li>
            <li><a href="#decoding" onclick="closeNav()">Decoding Strategies</a>
                <ul>
                    <li><a href="#greedy" onclick="closeNav()">Greedy & Beam Search</a></li>
                    <li><a href="#sampling" onclick="closeNav()">Temperature & Top-k/Top-p Sampling</a></li>
                </ul>
            </li>
            <li><a href="#prompt-engineering" onclick="closeNav()">Prompt Engineering</a></li>
            <li><a href="#rlhf" onclick="closeNav()">RLHF & Alignment</a></li>
            <li><a href="#practical" onclick="closeNav()">Practical Implementation</a></li>
            <li><a href="#conclusion" onclick="closeNav()">Conclusion & Next Steps</a></li>
        </ol>
    </div>

    <div id="tocOverlay" class="sidenav-overlay" onclick="closeNav()"></div>

    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="blog-content">
                        
                        <h2 id="introduction"><i class="fas fa-magic me-2"></i>Introduction to Generative Models</h2>
                        
                        <p>GPT (Generative Pre-trained Transformer) models represent a paradigm shift in NLP, demonstrating that large-scale autoregressive language models can learn to perform diverse tasks through in-context learning without task-specific fine-tuning.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-star me-2"></i>Key Insight</h4>
                            <p><strong>GPT models predict the next token autoregressively, learning rich representations of language that enable few-shot and zero-shot learning across virtually any text-based task.</strong></p>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-map-signs me-2"></i>Complete NLP Series Navigation</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">16-Part Series</span>
                                <span class="badge bg-crimson">NLP Mastery</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><a href="nlp-fundamentals-linguistic-basics.html">NLP Fundamentals & Linguistic Basics</a></li>
                                    <li><a href="nlp-tokenization-text-cleaning.html">Tokenization & Text Cleaning</a></li>
                                    <li><a href="nlp-text-representation-features.html">Text Representation & Feature Engineering</a></li>
                                    <li><a href="nlp-word-embeddings.html">Word Embeddings</a></li>
                                    <li><a href="nlp-statistical-language-models.html">Statistical Language Models & N-grams</a></li>
                                    <li><a href="nlp-neural-networks.html">Neural Networks for NLP</a></li>
                                    <li><a href="nlp-rnn-lstm-gru.html">RNNs, LSTMs & GRUs</a></li>
                                    <li><a href="nlp-transformers-attention.html">Transformers & Attention Mechanism</a></li>
                                    <li><a href="nlp-pretrained-models-transfer-learning.html">Pretrained Language Models & Transfer Learning</a></li>
                                    <li><strong>GPT Models & Text Generation (This Guide)</strong></li>
                                    <li><a href="nlp-core-tasks.html">Core NLP Tasks</a></li>
                                    <li><a href="nlp-advanced-tasks.html">Advanced NLP Tasks</a></li>
                                    <li><a href="nlp-multilingual-crosslingual.html">Multilingual & Cross-lingual NLP</a></li>
                                    <li><a href="nlp-evaluation-ethics.html">Evaluation, Ethics & Responsible NLP</a></li>
                                    <li><a href="nlp-systems-production.html">NLP Systems, Optimization & Production</a></li>
                                    <li><a href="nlp-cutting-edge-research.html">Cutting-Edge & Research Topics</a></li>
                                </ol>
                            </div>
                        </div>

                        <h2 id="autoregressive"><i class="fas fa-arrow-right me-2"></i>Autoregressive Language Modeling</h2>
                        
                        <p><strong>Autoregressive language modeling</strong> is the foundational approach behind GPT models, where text is generated one token at a time by predicting the next token based on all previous tokens. Unlike masked language models (BERT) that can see context from both directions, autoregressive models maintain a strict left-to-right generation process, making them naturally suited for text generation tasks.</p>

                        <p>The core mathematical formulation models the probability of a sequence as a product of conditional probabilities: P(x₁, x₂, ..., xₙ) = ∏ P(xᵢ | x₁, ..., xᵢ₋₁). During training, the model learns to maximize the likelihood of predicting each token given its preceding context. This causal (unidirectional) attention mechanism ensures that predictions for position i can only attend to positions less than i, preventing "peeking" at future tokens.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>Autoregressive vs. Masked Language Models</h4>
                            <p><strong>Autoregressive (GPT):</strong> Predicts next token using only left context → Natural for generation<br>
                            <strong>Masked (BERT):</strong> Predicts masked tokens using bidirectional context → Better for understanding</p>
                            <p>The autoregressive approach trades off bidirectional context for the ability to generate coherent, long-form text sequences naturally.</p>
                        </div>

<pre><code class="language-python">import torch
import torch.nn.functional as F
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load a pre-trained GPT-2 model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Set model to evaluation mode
model.eval()

# Example: Compute next-token probabilities
input_text = "The future of artificial intelligence"
input_ids = tokenizer.encode(input_text, return_tensors="pt")

# Get logits for next token prediction
with torch.no_grad():
    outputs = model(input_ids)
    next_token_logits = outputs.logits[:, -1, :]  # Logits for last position

# Convert to probabilities
probs = F.softmax(next_token_logits, dim=-1)

# Get top 10 predicted tokens
top_k = 10
top_probs, top_indices = torch.topk(probs, top_k)

print(f"Input: '{input_text}'")
print(f"\nTop {top_k} predictions for next token:")
for i in range(top_k):
    token = tokenizer.decode(top_indices[0][i])
    prob = top_probs[0][i].item()
    print(f"  '{token}' - probability: {prob:.4f}")</code></pre>

                        <p>The autoregressive approach enables several key capabilities that make GPT models so powerful. First, it allows for <strong>arbitrary-length generation</strong>—the model can continue generating tokens indefinitely until a stop condition is met. Second, the training objective is simple and scalable: predict the next word given all previous words. Third, this formulation naturally handles variable-length inputs and outputs without requiring fixed sequence lengths during generation.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-flask me-2"></i>Understanding Causal Self-Attention</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Core Concept</span>
                                <span class="badge bg-crimson">Attention Masking</span>
                            </div>
                            <div class="content">
                                <p>GPT uses <strong>causal (masked) self-attention</strong> where each position can only attend to earlier positions. This is implemented using a triangular attention mask:</p>
<pre><code class="language-python">import torch
import torch.nn.functional as F

def causal_self_attention(query, key, value, mask=None):
    """
    Implements causal self-attention for autoregressive models.
    
    Args:
        query, key, value: Tensors of shape (batch, seq_len, d_model)
        mask: Optional attention mask
    """
    d_k = query.size(-1)
    seq_len = query.size(1)
    
    # Compute attention scores
    scores = torch.matmul(query, key.transpose(-2, -1)) / (d_k ** 0.5)
    
    # Create causal mask (lower triangular)
    # This prevents attending to future positions
    causal_mask = torch.tril(torch.ones(seq_len, seq_len))
    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dims
    
    # Apply mask: set future positions to -infinity
    scores = scores.masked_fill(causal_mask == 0, float('-inf'))
    
    # Apply softmax to get attention weights
    attention_weights = F.softmax(scores, dim=-1)
    
    # Compute weighted sum of values
    output = torch.matmul(attention_weights, value)
    
    return output, attention_weights

# Example usage
batch_size, seq_len, d_model = 1, 5, 64
q = k = v = torch.randn(batch_size, seq_len, d_model)

output, weights = causal_self_attention(q, k, v)

print("Attention weights (causal mask applied):")
print(weights[0, 0].detach().numpy().round(3))
print("\nNote: Each row shows attention to previous positions only")
print("Position 0 attends only to itself, position 4 attends to all positions")</code></pre>
                            </div>
                        </div>

                        <h2 id="gpt-evolution"><i class="fas fa-chart-line me-2"></i>GPT Architecture Evolution</h2>

                        <p>The GPT family represents a remarkable progression in language model capabilities, driven primarily by scaling model size, training data, and compute. Each generation introduced architectural refinements and demonstrated emergent capabilities that weren't present in smaller models. Understanding this evolution provides insight into how modern large language models achieve their impressive performance.</p>

                        <h3 id="gpt1">GPT-1: Generative Pre-Training</h3>
                        
                        <p>Released by OpenAI in 2018, <strong>GPT-1</strong> introduced the paradigm of generative pre-training followed by discriminative fine-tuning. With 117 million parameters and 12 transformer layers, GPT-1 demonstrated that unsupervised pre-training on large text corpora (BooksCorpus, ~7,000 books) could produce representations useful for downstream tasks. The key innovation was showing that a single architecture could be fine-tuned for multiple tasks including natural language inference, question answering, and semantic similarity.</p>

                        <p>The architecture used a decoder-only transformer with learned positional embeddings, GELU activation functions, and byte-pair encoding (BPE) tokenization. GPT-1 established the "pre-train then fine-tune" paradigm that would dominate NLP for years, though it still required task-specific fine-tuning for each downstream application.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-info-circle me-2"></i>GPT-1 Key Specifications</h4>
                            <p><strong>Parameters:</strong> 117 million | <strong>Layers:</strong> 12 | <strong>Hidden Size:</strong> 768 | <strong>Attention Heads:</strong> 12<br>
                            <strong>Training Data:</strong> BooksCorpus (~5GB) | <strong>Context Length:</strong> 512 tokens<br>
                            <strong>Innovation:</strong> Demonstrated transfer learning from unsupervised pre-training to supervised tasks</p>
                        </div>

                        <h3 id="gpt2">GPT-2: Zero-Shot Task Transfer</h3>
                        
                        <p><strong>GPT-2</strong> (2019) scaled up to 1.5 billion parameters and introduced a crucial insight: with sufficient scale and diverse training data, language models could perform tasks <strong>zero-shot</strong>—without any fine-tuning or examples. Trained on WebText (40GB of web pages filtered for quality), GPT-2 demonstrated that tasks like summarization, translation, and question answering emerged naturally from the language modeling objective.</p>

                        <p>The paper "Language Models are Unsupervised Multitask Learners" showed that framing tasks as text completion allowed zero-shot transfer. For example, for translation: "Translate English to French: [English text] =" would generate the French translation. This discovery foreshadowed the prompt engineering techniques that would become central to using larger models.</p>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load GPT-2 (multiple sizes available: gpt2, gpt2-medium, gpt2-large, gpt2-xl)
model_name = "gpt2-medium"  # 355M parameters
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)
model.eval()

# Zero-shot task examples with GPT-2
tasks = [
    # Translation (zero-shot)
    "Translate English to French:\nEnglish: Hello, how are you?\nFrench:",
    
    # Summarization (zero-shot)
    "Summarize the following text:\nText: The quick brown fox jumps over the lazy dog. This sentence contains every letter of the alphabet and is often used for typing practice.\nSummary:",
    
    # Question answering (zero-shot)
    "Answer the question based on context.\nContext: Paris is the capital of France. It is known for the Eiffel Tower.\nQuestion: What is Paris known for?\nAnswer:",
]

def generate_completion(prompt, max_new_tokens=50):
    """Generate text completion for a prompt."""
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            num_return_sequences=1,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )
    
    generated = tokenizer.decode(output[0], skip_special_tokens=True)
    return generated[len(prompt):]  # Return only generated part

print("GPT-2 Zero-Shot Task Examples")
print("=" * 60)

for i, task in enumerate(tasks, 1):
    print(f"\nTask {i}:")
    print(f"Prompt: {task}")
    print(f"Generated: {generate_completion(task, max_new_tokens=30)}")</code></pre>

                        <h3 id="gpt3">GPT-3 & In-Context Learning</h3>
                        
                        <p><strong>GPT-3</strong> (2020) marked a watershed moment with 175 billion parameters—over 100x larger than GPT-2. The landmark paper "Language Models are Few-Shot Learners" demonstrated <strong>in-context learning</strong>: the ability to learn new tasks from just a few examples provided in the prompt, without any gradient updates. This "few-shot" capability emerged purely from scale and training data diversity (300 billion tokens from filtered Common Crawl, books, and Wikipedia).</p>

                        <p>GPT-3 introduced the formal distinction between zero-shot, one-shot, and few-shot prompting. With few-shot examples, GPT-3 approached or exceeded fine-tuned model performance on many benchmarks. The model also exhibited surprising emergent capabilities including basic arithmetic, code generation, and complex reasoning—abilities not explicitly trained but arising from the language modeling objective at scale.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-brain me-2"></i>In-Context Learning: Few-Shot Prompting</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">GPT-3+ Capability</span>
                                <span class="badge bg-crimson">No Fine-Tuning</span>
                            </div>
                            <div class="content">
                                <p>In-context learning allows the model to learn task patterns from examples in the prompt:</p>
<pre><code class="language-python">from transformers import pipeline
import torch

# Using GPT-2 to demonstrate the concept (GPT-3 API works similarly)
# For actual GPT-3, use OpenAI API
generator = pipeline('text-generation', model='gpt2-large')

# Few-shot prompt structure
few_shot_prompt = """Classify the sentiment of the review.

Review: This movie was absolutely fantastic! Best film I've seen all year.
Sentiment: Positive

Review: Terrible waste of time. The acting was wooden and the plot made no sense.
Sentiment: Negative

Review: It was okay, nothing special but not bad either.
Sentiment: Neutral

Review: I loved every minute of this brilliant masterpiece!
Sentiment:"""

# Generate completion
result = generator(
    few_shot_prompt,
    max_new_tokens=5,
    num_return_sequences=1,
    temperature=0.3,
    do_sample=True,
    pad_token_id=50256  # GPT-2 EOS token
)

print("Few-Shot Sentiment Classification")
print("=" * 50)
print(result[0]['generated_text'])

# Zero-shot vs One-shot vs Few-shot comparison
print("\n" + "=" * 50)
print("Prompting Strategies Comparison:")
print("=" * 50)

strategies = {
    "Zero-shot": "Classify sentiment: 'Great product!'\nSentiment:",
    "One-shot": """Classify sentiment.
Example: 'Terrible!' -> Negative
Classify: 'Great product!' -> """,
    "Few-shot": """Classify sentiment.
'Terrible!' -> Negative
'Amazing!' -> Positive  
'It's fine' -> Neutral
'Great product!' ->"""
}

for strategy, prompt in strategies.items():
    print(f"\n{strategy}:")
    print(f"Prompt structure: {prompt[:60]}...")</code></pre>
                            </div>
                        </div>

                        <h3 id="gpt4">GPT-4 & Beyond</h3>
                        
                        <p><strong>GPT-4</strong> (2023) represented another significant leap, though OpenAI disclosed fewer technical details. GPT-4 demonstrated substantially improved reasoning, reduced hallucination, and notably introduced <strong>multimodal capabilities</strong>—accepting both text and images as input. On professional benchmarks like the bar exam and medical licensing tests, GPT-4 scored in the top percentiles, suggesting human-level performance on many complex reasoning tasks.</p>

                        <p>The GPT-4 era also brought important developments in <strong>safety and alignment</strong>. Extensive red-teaming, RLHF (Reinforcement Learning from Human Feedback), and constitutional AI techniques made GPT-4 more helpful, harmless, and honest than predecessors. The model showed improved instruction-following, better calibration of uncertainty, and reduced tendency to produce harmful or false content.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-rocket me-2"></i>GPT Model Evolution Summary</h4>
                            <table class="table table-bordered mt-3">
                                <thead class="table-dark">
                                    <tr>
                                        <th>Model</th>
                                        <th>Year</th>
                                        <th>Parameters</th>
                                        <th>Key Innovation</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>GPT-1</td>
                                        <td>2018</td>
                                        <td>117M</td>
                                        <td>Pre-train → Fine-tune paradigm</td>
                                    </tr>
                                    <tr>
                                        <td>GPT-2</td>
                                        <td>2019</td>
                                        <td>1.5B</td>
                                        <td>Zero-shot task transfer</td>
                                    </tr>
                                    <tr>
                                        <td>GPT-3</td>
                                        <td>2020</td>
                                        <td>175B</td>
                                        <td>In-context learning, few-shot</td>
                                    </tr>
                                    <tr>
                                        <td>GPT-4</td>
                                        <td>2023</td>
                                        <td>~1.7T*</td>
                                        <td>Multimodal, improved reasoning</td>
                                    </tr>
                                </tbody>
                            </table>
                            <p class="small text-muted">*GPT-4 size is estimated; OpenAI hasn't confirmed exact parameters</p>
                        </div>

<pre><code class="language-python"># Working with OpenAI API (GPT-3.5/GPT-4)
# pip install openai

from openai import OpenAI
import os

# Initialize client (set OPENAI_API_KEY environment variable)
# client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# Example API call structure for GPT-4
def call_gpt4(prompt, system_message="You are a helpful assistant."):
    """
    Example function to call GPT-4 API.
    Requires valid OpenAI API key.
    """
    # Uncomment to use with valid API key:
    # response = client.chat.completions.create(
    #     model="gpt-4",
    #     messages=[
    #         {"role": "system", "content": system_message},
    #         {"role": "user", "content": prompt}
    #     ],
    #     temperature=0.7,
    #     max_tokens=500
    # )
    # return response.choices[0].message.content
    
    # Placeholder for demonstration
    return f"[GPT-4 response to: {prompt[:50]}...]"

# Example prompts showcasing GPT-4 capabilities
examples = {
    "Complex Reasoning": """
        If all bloops are razzies and all razzies are lazzies, 
        are all bloops definitely lazzies? Explain your reasoning step by step.
    """,
    
    "Code Generation": """
        Write a Python function that finds the longest palindromic 
        substring in a given string using dynamic programming.
    """,
    
    "Creative Writing": """
        Write a haiku about machine learning that captures both 
        its technical nature and philosophical implications.
    """
}

print("GPT-4 Capability Examples")
print("=" * 60)
for capability, prompt in examples.items():
    print(f"\n{capability}:")
    print(f"Prompt: {prompt.strip()[:60]}...")
    print(f"Response: {call_gpt4(prompt)}")</code></pre>

                        <h2 id="decoding"><i class="fas fa-random me-2"></i>Decoding Strategies</h2>

                        <p><strong>Decoding strategies</strong> determine how we select tokens from the model's predicted probability distribution during text generation. The choice of decoding method dramatically affects output quality—whether it's coherent, diverse, creative, or deterministic. Understanding these strategies is crucial for getting optimal results from any language model.</p>

                        <h3 id="greedy">Greedy & Beam Search</h3>
                        
                        <p><strong>Greedy decoding</strong> is the simplest strategy: at each step, select the token with the highest probability. While fast and deterministic, greedy decoding often produces repetitive, generic text because it always chooses the "safe" option. It can also lead to suboptimal global solutions—a locally optimal choice at each step doesn't guarantee the best overall sequence.</p>

                        <p><strong>Beam search</strong> addresses greedy decoding's limitations by maintaining multiple candidate sequences (beams) at each step. Instead of committing to one token, beam search tracks the top-k most probable sequences and selects the best complete sequence at the end. While beam search produces more coherent text than greedy decoding, it can still suffer from repetition and lack of diversity, particularly for open-ended generation tasks.</p>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load model and tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")
model.eval()

prompt = "The secret to happiness is"
input_ids = tokenizer.encode(prompt, return_tensors="pt")

print("Decoding Strategy Comparison")
print("=" * 60)
print(f"Prompt: '{prompt}'")
print()

# 1. Greedy Decoding
print("1. GREEDY DECODING")
print("-" * 40)
greedy_output = model.generate(
    input_ids,
    max_new_tokens=40,
    do_sample=False,  # Greedy: no sampling
    pad_token_id=tokenizer.eos_token_id
)
print(f"Output: {tokenizer.decode(greedy_output[0], skip_special_tokens=True)}")
print()

# 2. Beam Search
print("2. BEAM SEARCH (num_beams=5)")
print("-" * 40)
beam_output = model.generate(
    input_ids,
    max_new_tokens=40,
    num_beams=5,  # Consider top 5 sequences at each step
    early_stopping=True,
    no_repeat_ngram_size=2,  # Prevent repetition
    pad_token_id=tokenizer.eos_token_id
)
print(f"Output: {tokenizer.decode(beam_output[0], skip_special_tokens=True)}")
print()

# 3. Beam Search with multiple outputs
print("3. BEAM SEARCH (multiple sequences)")
print("-" * 40)
beam_outputs = model.generate(
    input_ids,
    max_new_tokens=30,
    num_beams=5,
    num_return_sequences=3,  # Return top 3 beams
    early_stopping=True,
    no_repeat_ngram_size=2,
    pad_token_id=tokenizer.eos_token_id
)
for i, output in enumerate(beam_outputs):
    print(f"Beam {i+1}: {tokenizer.decode(output, skip_special_tokens=True)}")</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-exclamation-triangle me-2"></i>The Repetition Problem</h4>
                            <p>Both greedy decoding and beam search often produce repetitive text. Common solutions include:</p>
                            <ul>
                                <li><strong>no_repeat_ngram_size:</strong> Prevent repeating n-grams</li>
                                <li><strong>repetition_penalty:</strong> Penalize tokens that have already appeared</li>
                                <li><strong>Sampling methods:</strong> Add randomness to token selection</li>
                            </ul>
                        </div>

                        <h3 id="sampling">Temperature & Top-k/Top-p Sampling</h3>
                        
                        <p><strong>Sampling-based decoding</strong> introduces controlled randomness to generate more diverse and creative text. Instead of always picking the most likely token, we sample from the probability distribution. The key parameters that control this randomness are temperature, top-k, and top-p (nucleus sampling).</p>

                        <p><strong>Temperature</strong> controls the "sharpness" of the probability distribution. Lower temperatures (e.g., 0.3) make the distribution peakier, favoring high-probability tokens and producing more focused, deterministic output. Higher temperatures (e.g., 1.5) flatten the distribution, giving lower-probability tokens more chance and producing more creative, sometimes chaotic output. Temperature of 1.0 leaves probabilities unchanged.</p>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
import torch.nn.functional as F

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")
model.eval()

def visualize_temperature_effect(logits, temperatures=[0.3, 0.7, 1.0, 1.5, 2.0]):
    """Show how temperature affects probability distribution."""
    print("Temperature Effect on Probability Distribution")
    print("=" * 60)
    
    for temp in temperatures:
        # Apply temperature scaling
        scaled_logits = logits / temp
        probs = F.softmax(scaled_logits, dim=-1)
        
        # Get top 5 tokens
        top_probs, top_indices = torch.topk(probs, 5)
        
        print(f"\nTemperature = {temp}:")
        print(f"  Top token prob: {top_probs[0][0].item():.4f}")
        print(f"  Distribution entropy: {(-probs * torch.log(probs + 1e-10)).sum().item():.4f}")
        
        tokens_probs = [(tokenizer.decode(idx), p.item()) 
                        for idx, p in zip(top_indices[0], top_probs[0])]
        print(f"  Top 5: {tokens_probs}")

# Get logits for a prompt
prompt = "The meaning of life is"
input_ids = tokenizer.encode(prompt, return_tensors="pt")

with torch.no_grad():
    outputs = model(input_ids)
    logits = outputs.logits[:, -1, :]  # Last position logits

visualize_temperature_effect(logits)</code></pre>

                        <p><strong>Top-k sampling</strong> restricts sampling to the k most likely tokens, setting all other probabilities to zero before renormalizing. This prevents sampling very unlikely tokens while maintaining diversity. <strong>Top-p (nucleus) sampling</strong> is more adaptive: it selects the smallest set of tokens whose cumulative probability exceeds p (e.g., 0.9). This means the number of considered tokens varies—fewer for confident predictions, more when the model is uncertain.</p>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")
model.eval()

prompt = "In a world where robots dream,"
input_ids = tokenizer.encode(prompt, return_tensors="pt")

print("Sampling Strategy Comparison")
print("=" * 60)
print(f"Prompt: '{prompt}'")
print()

# Pure sampling with different temperatures
print("1. TEMPERATURE SAMPLING")
print("-" * 40)
for temp in [0.3, 0.7, 1.0, 1.5]:
    output = model.generate(
        input_ids,
        max_new_tokens=30,
        do_sample=True,
        temperature=temp,
        pad_token_id=tokenizer.eos_token_id
    )
    print(f"T={temp}: {tokenizer.decode(output[0], skip_special_tokens=True)}")
print()

# Top-k sampling
print("2. TOP-K SAMPLING")
print("-" * 40)
for k in [10, 50, 100]:
    output = model.generate(
        input_ids,
        max_new_tokens=30,
        do_sample=True,
        top_k=k,
        temperature=1.0,
        pad_token_id=tokenizer.eos_token_id
    )
    print(f"k={k}: {tokenizer.decode(output[0], skip_special_tokens=True)}")
print()

# Top-p (nucleus) sampling
print("3. TOP-P (NUCLEUS) SAMPLING")
print("-" * 40)
for p in [0.5, 0.9, 0.95]:
    output = model.generate(
        input_ids,
        max_new_tokens=30,
        do_sample=True,
        top_p=p,
        temperature=1.0,
        pad_token_id=tokenizer.eos_token_id
    )
    print(f"p={p}: {tokenizer.decode(output[0], skip_special_tokens=True)}")</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-cogs me-2"></i>Recommended Sampling Configurations</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Best Practices</span>
                                <span class="badge bg-crimson">Production Settings</span>
                            </div>
                            <div class="content">
                                <table class="table table-bordered">
                                    <thead class="table-light">
                                        <tr>
                                            <th>Use Case</th>
                                            <th>Temperature</th>
                                            <th>Top-p</th>
                                            <th>Top-k</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td>Factual/Deterministic (Q&A, code)</td>
                                            <td>0.1-0.3</td>
                                            <td>0.9</td>
                                            <td>-</td>
                                        </tr>
                                        <tr>
                                            <td>Balanced (summaries, analysis)</td>
                                            <td>0.5-0.7</td>
                                            <td>0.9</td>
                                            <td>-</td>
                                        </tr>
                                        <tr>
                                            <td>Creative (stories, poetry)</td>
                                            <td>0.8-1.0</td>
                                            <td>0.95</td>
                                            <td>50</td>
                                        </tr>
                                        <tr>
                                            <td>Brainstorming (diverse ideas)</td>
                                            <td>1.0-1.2</td>
                                            <td>0.95</td>
                                            <td>100</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>

<pre><code class="language-python">from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

tokenizer = GPT2Tokenizer.from_pretrained("gpt2-medium")
model = GPT2LMHeadModel.from_pretrained("gpt2-medium")
model.eval()

def generate_with_config(prompt, config_name, **kwargs):
    """Generate text with specific configuration."""
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    
    output = model.generate(
        input_ids,
        max_new_tokens=50,
        pad_token_id=tokenizer.eos_token_id,
        no_repeat_ngram_size=3,
        **kwargs
    )
    
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Production-ready configurations
configs = {
    "Factual/Precise": {
        "do_sample": True,
        "temperature": 0.2,
        "top_p": 0.9,
    },
    "Balanced": {
        "do_sample": True,
        "temperature": 0.7,
        "top_p": 0.9,
    },
    "Creative": {
        "do_sample": True,
        "temperature": 0.9,
        "top_p": 0.95,
        "top_k": 50,
    },
    "Highly Creative": {
        "do_sample": True,
        "temperature": 1.2,
        "top_p": 0.95,
        "top_k": 100,
    }
}

prompt = "The future of artificial intelligence will"

print("Configuration Comparison")
print("=" * 60)
print(f"Prompt: '{prompt}'")
print()

for config_name, params in configs.items():
    print(f"{config_name}:")
    print(f"  Settings: T={params.get('temperature')}, p={params.get('top_p')}, k={params.get('top_k', '-')}")
    result = generate_with_config(prompt, config_name, **params)
    print(f"  Output: {result}")
    print()</code></pre>

                        <h2 id="prompt-engineering"><i class="fas fa-terminal me-2"></i>Prompt Engineering</h2>

                        <p><strong>Prompt engineering</strong> is the art and science of crafting inputs that elicit desired outputs from language models. As models have grown larger and more capable, prompt engineering has become a crucial skill—often the difference between mediocre and exceptional results lies entirely in how you frame the request. Effective prompts leverage the model's in-context learning abilities to guide it toward accurate, relevant, and well-formatted responses.</p>

                        <p>The fundamental insight behind prompt engineering is that GPT models are <strong>completion engines</strong>: they predict what text should follow a given input. By carefully structuring that input—providing context, examples, constraints, and role definitions—we can steer the model toward specific behaviors without any retraining. This makes prompt engineering both powerful and accessible to non-ML practitioners.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-key me-2"></i>Core Prompt Engineering Principles</h4>
                            <ul>
                                <li><strong>Be specific:</strong> Vague prompts yield vague responses</li>
                                <li><strong>Provide context:</strong> Give the model relevant background information</li>
                                <li><strong>Use examples:</strong> Few-shot prompting dramatically improves accuracy</li>
                                <li><strong>Define the role:</strong> "You are an expert X" primes appropriate behavior</li>
                                <li><strong>Specify format:</strong> Explicitly request JSON, bullet points, etc.</li>
                                <li><strong>Think step-by-step:</strong> Chain-of-thought prompting improves reasoning</li>
                            </ul>
                        </div>

<pre><code class="language-python"># Prompt Engineering Techniques
# These examples demonstrate patterns applicable to any GPT-family model

# 1. BASIC PROMPTING PATTERNS
basic_prompts = {
    # Bad: vague, no context
    "vague": "Write about AI.",
    
    # Good: specific, contextual, structured
    "specific": """Write a 200-word blog post introduction about how AI 
is transforming healthcare diagnostics. Target audience: healthcare 
professionals with limited technical background. Include one specific 
example of an AI diagnostic tool.""",
}

# 2. ROLE-BASED PROMPTING
role_prompts = {
    "teacher": """You are an experienced computer science professor 
known for clear, intuitive explanations.

Explain recursion to a student who understands loops but hasn't 
seen recursion before. Use a real-world analogy.""",

    "code_reviewer": """You are a senior software engineer performing 
a code review. Be constructive but thorough.

Review this code for bugs, style issues, and potential improvements:
```python
def fib(n):
    if n <= 1: return n
    return fib(n-1) + fib(n-2)
```""",
}

# 3. FEW-SHOT PROMPTING (with examples)
few_shot_prompt = """Convert natural language to SQL queries.

Example 1:
Natural: Show all users older than 25
SQL: SELECT * FROM users WHERE age > 25;

Example 2:
Natural: Count products in electronics category
SQL: SELECT COUNT(*) FROM products WHERE category = 'electronics';

Example 3:
Natural: Get the top 5 highest-paid employees
SQL: SELECT * FROM employees ORDER BY salary DESC LIMIT 5;

Convert:
Natural: Find users who registered in 2024
SQL:"""

# 4. CHAIN-OF-THOUGHT (CoT) PROMPTING
cot_prompt = """Solve this step by step:

Question: A store sells apples for $2 each and oranges for $3 each. 
If John buys 5 apples and 3 oranges, and pays with a $20 bill, 
how much change does he receive?

Let me solve this step by step:
1. First, calculate the cost of apples: 5 apples × $2 = $10
2. Then, calculate the cost of oranges: 3 oranges × $3 = $9
3. Add up the total cost: $10 + $9 = $19
4. Calculate the change: $20 - $19 = $1

Answer: John receives $1 in change.

Now solve this problem step by step:

Question: A bookstore has a "buy 2 get 1 free" deal on novels. 
Each novel costs $15. If Sarah wants 7 novels, how much will she pay?

Let me solve this step by step:"""

print("Prompt Engineering Examples")
print("=" * 60)
print("\n1. Specific vs Vague Prompts:")
print(f"   Vague: {basic_prompts['vague']}")
print(f"   Specific: {basic_prompts['specific'][:80]}...")
print("\n2. Few-Shot Pattern (SQL generation):")
print(f"   {few_shot_prompt[:200]}...")
print("\n3. Chain-of-Thought Pattern:")
print(f"   Structured reasoning encourages the model to show work")</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-flask me-2"></i>Advanced Prompting Techniques</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Advanced</span>
                                <span class="badge bg-crimson">High Performance</span>
                            </div>
                            <div class="content">
<pre><code class="language-python"># ADVANCED PROMPTING TECHNIQUES

# 1. SELF-CONSISTENCY: Generate multiple responses, pick majority
self_consistency_prompt = """Answer the following question. Show your reasoning.

Question: If a train travels at 60 mph for 2.5 hours, then at 80 mph 
for 1.5 hours, what is the total distance traveled?

[Generate this prompt multiple times with temperature > 0, 
then select the most common answer]"""

# 2. TREE OF THOUGHTS: Explore multiple reasoning paths
tot_prompt = """For the following problem, consider 3 different 
approaches before selecting the best one.

Problem: How can we reduce plastic waste in oceans?

Approach 1: [Think about technological solutions]
Approach 2: [Think about policy/regulation solutions]
Approach 3: [Think about behavioral/educational solutions]

Now evaluate each approach and synthesize the best elements."""

# 3. STRUCTURED OUTPUT PROMPTS
structured_prompt = """Analyze the following product review and return 
a JSON object with the following structure:
{
    "sentiment": "positive" | "negative" | "neutral",
    "confidence": 0.0-1.0,
    "key_points": ["point1", "point2"],
    "product_aspects": {
        "quality": "mentioned positively/negatively/not mentioned",
        "price": "mentioned positively/negatively/not mentioned",
        "service": "mentioned positively/negatively/not mentioned"
    }
}

Review: "Great laptop! Fast performance and beautiful screen. 
A bit pricey but worth every penny. Shipping was delayed though."

JSON:"""

# 4. CONTRASTIVE PROMPTING
contrastive_prompt = """I will show you examples of formal and informal 
email responses. Learn the pattern, then write a formal response.

INFORMAL: "Hey! Sure thing, I'll get that report to you by Friday. 
No worries! Catch you later!"

FORMAL: "Dear Mr. Johnson, Thank you for your inquiry. I will ensure 
the report is delivered by Friday, January 24th. Please do not 
hesitate to reach out if you require any further assistance. 
Best regards, [Name]"

Now write a FORMAL response to this message:
"yo, can u send me the meeting notes from yesterday? thx"

FORMAL:"""

# 5. PERSONA + CONSTRAINT COMBINATION
complex_prompt = """You are a children's book author who specializes 
in making complex topics accessible to 8-year-olds.

CONSTRAINTS:
- Use simple vocabulary (grade 3 reading level)
- Include one fun analogy
- Keep the explanation under 100 words
- End with an engaging question for the child

TOPIC: Explain how the internet works.

RESPONSE:"""

print("Advanced Prompting Techniques")
print("=" * 60)
print("\n1. Self-Consistency: Multiple samples → majority vote")
print("2. Tree of Thoughts: Explore multiple reasoning paths")
print("3. Structured Output: Enforce JSON/specific format")
print("4. Contrastive Prompting: Show what TO do and NOT to do")
print("5. Persona + Constraints: Role + specific requirements")</code></pre>
                            </div>
                        </div>

<pre><code class="language-python">from transformers import pipeline
import torch

# Practical prompt engineering with Hugging Face
generator = pipeline('text-generation', model='gpt2-large')

def generate_with_prompt(prompt, max_length=200):
    """Generate completion with formatting."""
    result = generator(
        prompt,
        max_new_tokens=100,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        num_return_sequences=1,
        pad_token_id=50256
    )
    return result[0]['generated_text']

# PROMPT TEMPLATE LIBRARY
class PromptTemplate:
    """Reusable prompt templates for common tasks."""
    
    @staticmethod
    def summarization(text, style="concise"):
        styles = {
            "concise": "Summarize in 1-2 sentences:",
            "detailed": "Provide a detailed summary covering all key points:",
            "bullet": "Summarize as bullet points:"
        }
        return f"{styles[style]}\n\nText: {text}\n\nSummary:"
    
    @staticmethod
    def classification(text, categories):
        category_list = ", ".join(categories)
        return f"""Classify the following text into one of these categories: {category_list}

Text: {text}

Category:"""
    
    @staticmethod
    def extraction(text, fields):
        field_list = ", ".join(fields)
        return f"""Extract the following information from the text: {field_list}

Text: {text}

Extracted information:"""
    
    @staticmethod
    def code_generation(description, language="Python"):
        return f"""Write {language} code for the following task:

Task: {description}

Requirements:
- Include comments explaining the code
- Handle edge cases
- Follow best practices

{language} code:
```{language.lower()}
"""

# Example usage
text = "Apple announced a new iPhone today with improved camera and battery."

print("Prompt Template Examples")
print("=" * 60)

# Summarization
print("\n1. SUMMARIZATION:")
prompt = PromptTemplate.summarization(text)
print(f"Prompt: {prompt}")

# Classification
print("\n2. CLASSIFICATION:")
prompt = PromptTemplate.classification(text, ["Technology", "Sports", "Politics"])
print(f"Prompt: {prompt}")

# Extraction
print("\n3. EXTRACTION:")
prompt = PromptTemplate.extraction(text, ["Company", "Product", "Features"])
print(f"Prompt: {prompt}")

# Code generation
print("\n4. CODE GENERATION:")
prompt = PromptTemplate.code_generation("Calculate the factorial of a number")
print(f"Prompt: {prompt}")</code></pre>

                        <h2 id="rlhf"><i class="fas fa-user-check me-2"></i>RLHF & Alignment</h2>

                        <p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong> is the key technique that transforms raw language models into helpful, harmless, and honest assistants. While pre-training teaches models to predict text, RLHF aligns model behavior with human preferences and values. This process was crucial in developing ChatGPT, GPT-4, and Claude—making them safe and useful for real-world applications.</p>

                        <p>The RLHF pipeline consists of three main stages: (1) <strong>Supervised Fine-Tuning (SFT)</strong> on human demonstrations of desired behavior, (2) <strong>Reward Model Training</strong> to learn human preferences from comparisons, and (3) <strong>Reinforcement Learning</strong> (typically PPO) to optimize the language model against the reward model while maintaining similarity to the original model.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-shield-alt me-2"></i>Why RLHF Matters</h4>
                            <p>Pre-trained language models optimize for <strong>predicting likely text</strong>, not for being helpful or truthful. RLHF teaches models to:</p>
                            <ul>
                                <li>Follow instructions accurately</li>
                                <li>Refuse harmful requests appropriately</li>
                                <li>Acknowledge uncertainty instead of fabricating answers</li>
                                <li>Provide balanced, thoughtful responses</li>
                            </ul>
                        </div>

<pre><code class="language-python"># RLHF Conceptual Implementation
# This demonstrates the RLHF training loop structure

import torch
import torch.nn as nn
import torch.nn.functional as F

class RewardModel(nn.Module):
    """
    Reward model that scores responses based on human preferences.
    In practice, this is trained on human comparison data.
    """
    def __init__(self, base_model_hidden_size=768):
        super().__init__()
        self.score_head = nn.Linear(base_model_hidden_size, 1)
    
    def forward(self, hidden_states):
        """
        Args:
            hidden_states: Final hidden states from language model
        Returns:
            Scalar reward score
        """
        # Use the last token's hidden state for scoring
        last_hidden = hidden_states[:, -1, :]
        reward = self.score_head(last_hidden)
        return reward

def compute_reward_model_loss(reward_model, preferred_hidden, rejected_hidden):
    """
    Train reward model using human preference pairs.
    
    The model learns to assign higher scores to preferred responses.
    Loss: -log(sigmoid(r_preferred - r_rejected))
    """
    preferred_reward = reward_model(preferred_hidden)
    rejected_reward = reward_model(rejected_hidden)
    
    # Bradley-Terry pairwise ranking loss
    loss = -F.logsigmoid(preferred_reward - rejected_reward).mean()
    return loss

# PPO Training Loop Structure (Conceptual)
def rlhf_training_step(policy_model, reward_model, reference_model, 
                       prompt_batch, kl_coeff=0.1):
    """
    Single RLHF training step using PPO.
    
    Key components:
    1. Generate responses using current policy
    2. Score responses with reward model
    3. Compute KL penalty to prevent divergence from reference
    4. Update policy using PPO objective
    """
    # Generate responses from current policy
    # responses = policy_model.generate(prompt_batch)
    
    # Get rewards from reward model
    # rewards = reward_model(responses)
    
    # Compute KL divergence from reference model (regularization)
    # policy_logprobs = policy_model.log_probs(responses)
    # reference_logprobs = reference_model.log_probs(responses)
    # kl_penalty = policy_logprobs - reference_logprobs
    
    # Final reward = reward - kl_coeff * kl_penalty
    # This prevents the policy from diverging too far from the original model
    
    # PPO update using advantage estimation
    # ...
    
    print("RLHF Training Step Components:")
    print("1. Generate responses from policy")
    print("2. Score with reward model")
    print("3. Compute KL penalty for regularization")
    print("4. Update policy with PPO")
    
    return None  # Placeholder

print("RLHF Pipeline Overview")
print("=" * 60)
print("\nStage 1: Supervised Fine-Tuning (SFT)")
print("  - Train on human demonstrations")
print("  - Learn basic instruction-following")
print("\nStage 2: Reward Model Training")
print("  - Collect human comparisons (A vs B)")
print("  - Train model to predict human preference")
print("\nStage 3: PPO Optimization")
print("  - Generate responses")
print("  - Score with reward model")
print("  - Update policy with KL constraint")

rlhf_training_step(None, None, None, None)</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-balance-scale me-2"></i>Direct Preference Optimization (DPO)</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Alternative to RLHF</span>
                                <span class="badge bg-crimson">Simpler Training</span>
                            </div>
                            <div class="content">
                                <p><strong>DPO</strong> is a newer technique that achieves similar alignment results without training a separate reward model or using RL. It directly optimizes the language model on preference data.</p>
<pre><code class="language-python">import torch
import torch.nn.functional as F

def dpo_loss(policy_model, reference_model, 
             preferred_responses, rejected_responses, beta=0.1):
    """
    Direct Preference Optimization loss.
    
    DPO bypasses reward modeling by implicitly defining the reward
    as the log probability ratio between policy and reference models.
    
    Args:
        policy_model: Model being trained
        reference_model: Frozen reference model
        preferred_responses: Human-preferred responses
        rejected_responses: Human-rejected responses
        beta: Temperature parameter controlling strength of preference
    """
    # Get log probabilities from policy model
    policy_preferred_logps = get_log_probs(policy_model, preferred_responses)
    policy_rejected_logps = get_log_probs(policy_model, rejected_responses)
    
    # Get log probabilities from reference model (frozen)
    with torch.no_grad():
        ref_preferred_logps = get_log_probs(reference_model, preferred_responses)
        ref_rejected_logps = get_log_probs(reference_model, rejected_responses)
    
    # Compute log ratios
    preferred_ratio = policy_preferred_logps - ref_preferred_logps
    rejected_ratio = policy_rejected_logps - ref_rejected_logps
    
    # DPO loss: -log(sigmoid(beta * (preferred_ratio - rejected_ratio)))
    loss = -F.logsigmoid(beta * (preferred_ratio - rejected_ratio)).mean()
    
    return loss

def get_log_probs(model, sequences):
    """Placeholder: compute log probabilities of sequences."""
    # In practice: forward pass + gather log probs at token positions
    return torch.zeros(len(sequences))

print("DPO Advantages over RLHF:")
print("=" * 50)
print("✓ No separate reward model training")
print("✓ No RL (PPO) instability issues")
print("✓ Simpler implementation")
print("✓ Similar performance to RLHF")
print("✓ More computationally efficient")</code></pre>
                            </div>
                        </div>

                        <h2 id="practical"><i class="fas fa-code me-2"></i>Practical Implementation</h2>

                        <p>This section provides complete, production-ready code for text generation using various models and configurations. All examples are designed to be copy-paste executable and demonstrate best practices for working with GPT models in real applications.</p>

<pre><code class="language-python"># Complete Text Generation Pipeline
# pip install transformers torch accelerate

from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    pipeline,
    set_seed
)
import torch

class TextGenerator:
    """
    Production-ready text generation class with multiple 
    decoding strategies and configurations.
    """
    
    def __init__(self, model_name="gpt2-medium", device=None):
        """
        Initialize the text generator.
        
        Args:
            model_name: HuggingFace model identifier
            device: 'cuda', 'cpu', or None for auto-detect
        """
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Loading model {model_name} on {self.device}...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
        ).to(self.device)
        
        # Set pad token if not present (common for GPT models)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model.eval()
        print("Model loaded successfully!")
    
    def generate(self, prompt, max_new_tokens=100, 
                 strategy="balanced", **kwargs):
        """
        Generate text with predefined strategies.
        
        Args:
            prompt: Input text to continue
            max_new_tokens: Maximum tokens to generate
            strategy: 'greedy', 'beam', 'balanced', 'creative', 'precise'
        """
        # Predefined generation configurations
        strategies = {
            "greedy": {
                "do_sample": False,
            },
            "beam": {
                "do_sample": False,
                "num_beams": 5,
                "early_stopping": True,
            },
            "precise": {
                "do_sample": True,
                "temperature": 0.3,
                "top_p": 0.9,
            },
            "balanced": {
                "do_sample": True,
                "temperature": 0.7,
                "top_p": 0.9,
            },
            "creative": {
                "do_sample": True,
                "temperature": 1.0,
                "top_p": 0.95,
                "top_k": 50,
            }
        }
        
        config = strategies.get(strategy, strategies["balanced"])
        config.update(kwargs)  # Allow overrides
        
        # Encode input
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=self.tokenizer.eos_token_id,
                no_repeat_ngram_size=3,
                **config
            )
        
        # Decode and return only the generated part
        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = full_text[len(prompt):]
        
        return generated_text.strip()
    
    def generate_multiple(self, prompt, n=3, **kwargs):
        """Generate multiple diverse completions."""
        kwargs['num_return_sequences'] = n
        kwargs['do_sample'] = True  # Required for multiple sequences
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=kwargs.pop('max_new_tokens', 50),
                pad_token_id=self.tokenizer.eos_token_id,
                **kwargs
            )
        
        completions = []
        for output in outputs:
            text = self.tokenizer.decode(output, skip_special_tokens=True)
            completions.append(text[len(prompt):].strip())
        
        return completions

# Usage example
print("TextGenerator Example")
print("=" * 60)

# Initialize (using smaller model for demo)
generator = TextGenerator("gpt2")

# Test different strategies
prompt = "The most important lesson I learned in life is"

print(f"\nPrompt: '{prompt}'")
print()

for strategy in ["greedy", "precise", "balanced", "creative"]:
    result = generator.generate(prompt, max_new_tokens=40, strategy=strategy)
    print(f"{strategy.upper():12}: {result[:80]}...")
print()

# Generate multiple completions
print("Multiple Completions (3):")
completions = generator.generate_multiple(
    prompt, n=3, temperature=0.8, max_new_tokens=30
)
for i, comp in enumerate(completions, 1):
    print(f"  {i}. {comp[:60]}...")</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-stream me-2"></i>Streaming Generation</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Real-time Output</span>
                                <span class="badge bg-crimson">User Experience</span>
                            </div>
                            <div class="content">
                                <p>For interactive applications, streaming generation provides real-time token-by-token output:</p>
<pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
import torch

# Load model
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Create streamer for real-time output
streamer = TextStreamer(tokenizer, skip_special_tokens=True)

# Generate with streaming
prompt = "Once upon a time in a distant galaxy,"
inputs = tokenizer(prompt, return_tensors="pt")

print("Streaming Generation:")
print(f"Prompt: {prompt}")
print("Output: ", end="")

# Generate with streamer - tokens appear in real-time
output = model.generate(
    **inputs,
    max_new_tokens=50,
    do_sample=True,
    temperature=0.8,
    streamer=streamer,
    pad_token_id=tokenizer.eos_token_id
)

print("\n\n[Generation complete]")</code></pre>
                            </div>
                        </div>

<pre><code class="language-python"># Advanced: Controlled Generation with Logits Processors
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer,
    LogitsProcessor,
    LogitsProcessorList
)
import torch

class KeywordBoostProcessor(LogitsProcessor):
    """
    Custom logits processor that boosts probability of 
    specific keywords during generation.
    """
    def __init__(self, tokenizer, keywords, boost_factor=2.0):
        self.keyword_ids = []
        for keyword in keywords:
            # Get all token IDs that could represent this keyword
            tokens = tokenizer.encode(keyword, add_special_tokens=False)
            self.keyword_ids.extend(tokens)
        self.boost_factor = boost_factor
    
    def __call__(self, input_ids, scores):
        """Boost scores for keyword tokens."""
        for token_id in self.keyword_ids:
            scores[:, token_id] *= self.boost_factor
        return scores

class BannedWordsProcessor(LogitsProcessor):
    """Processor that prevents generation of certain words."""
    def __init__(self, tokenizer, banned_words):
        self.banned_ids = []
        for word in banned_words:
            tokens = tokenizer.encode(word, add_special_tokens=False)
            self.banned_ids.extend(tokens)
    
    def __call__(self, input_ids, scores):
        """Set banned token scores to negative infinity."""
        for token_id in self.banned_ids:
            scores[:, token_id] = float('-inf')
        return scores

# Example usage
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Create processors
keyword_booster = KeywordBoostProcessor(
    tokenizer, 
    keywords=["innovative", "breakthrough", "revolutionary"],
    boost_factor=3.0
)

word_banner = BannedWordsProcessor(
    tokenizer,
    banned_words=["bad", "terrible", "awful"]
)

processors = LogitsProcessorList([keyword_booster, word_banner])

# Generate with custom processors
prompt = "The new technology is"
inputs = tokenizer(prompt, return_tensors="pt")

output = model.generate(
    **inputs,
    max_new_tokens=30,
    do_sample=True,
    temperature=0.8,
    logits_processor=processors,
    pad_token_id=tokenizer.eos_token_id
)

print("Controlled Generation with Custom Processors")
print("=" * 60)
print(f"Prompt: '{prompt}'")
print(f"Boosted keywords: innovative, breakthrough, revolutionary")
print(f"Banned words: bad, terrible, awful")
print(f"Output: {tokenizer.decode(output[0], skip_special_tokens=True)}")</code></pre>

<pre><code class="language-python"># Batch Generation for Efficiency
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import time

def batch_generate(prompts, model, tokenizer, max_new_tokens=50):
    """
    Generate completions for multiple prompts efficiently.
    
    Batching significantly speeds up generation when you have
    multiple prompts to process.
    """
    # Tokenize all prompts with padding
    inputs = tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True
    )
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode all outputs
    results = []
    for i, output in enumerate(outputs):
        text = tokenizer.decode(output, skip_special_tokens=True)
        # Remove the original prompt to get just the completion
        completion = text[len(prompts[i]):].strip()
        results.append(completion)
    
    return results

# Example: batch generation
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(model_name)

prompts = [
    "The future of AI is",
    "Climate change will",
    "Space exploration brings",
    "Renewable energy offers",
    "Healthcare technology enables"
]

print("Batch Generation Example")
print("=" * 60)

# Time comparison: batch vs sequential
start = time.time()
batch_results = batch_generate(prompts, model, tokenizer, max_new_tokens=30)
batch_time = time.time() - start

print(f"\nBatch generation time: {batch_time:.2f}s")
print(f"Prompts processed: {len(prompts)}")
print(f"Average per prompt: {batch_time/len(prompts):.2f}s")

print("\nResults:")
for prompt, result in zip(prompts, batch_results):
    print(f"\n'{prompt}'")
    print(f"→ {result[:60]}...")</code></pre>

                        <h2 id="conclusion"><i class="fas fa-flag-checkered me-2"></i>Conclusion & Next Steps</h2>

                        <p>GPT models and autoregressive text generation have transformed what's possible with language technology. From GPT-1's demonstration of transfer learning to GPT-4's human-level reasoning, the evolution of these models shows the remarkable capabilities that emerge from scaling transformers on massive text corpora. Understanding the architecture, decoding strategies, and prompt engineering techniques covered in this guide provides the foundation for building powerful text generation applications.</p>

                        <p>The key takeaways from this guide include: (1) <strong>Autoregressive modeling</strong> enables natural text generation by predicting tokens sequentially; (2) <strong>Scale matters</strong>—larger models exhibit emergent capabilities like in-context learning; (3) <strong>Decoding strategy choice</strong> dramatically affects output quality and should match your use case; (4) <strong>Prompt engineering</strong> is a crucial skill for getting the best results from any GPT model; and (5) <strong>RLHF alignment</strong> transforms raw language models into helpful, safe assistants.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-road me-2"></i>Your Learning Path Forward</h4>
                            <p>With GPT fundamentals mastered, you're ready to:</p>
                            <ul>
                                <li><strong>Part 11 - Core NLP Tasks:</strong> Apply these models to classification, NER, and POS tagging</li>
                                <li><strong>Part 12 - Advanced Tasks:</strong> Master question answering, summarization, and translation</li>
                                <li><strong>Part 14 - Ethics & Evaluation:</strong> Learn responsible deployment of generative AI</li>
                                <li><strong>Part 16 - Production:</strong> Deploy and optimize models for real-world applications</li>
                            </ul>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-tasks me-2"></i>Practice Exercises</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Hands-On</span>
                                <span class="badge bg-crimson">Skill Building</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><strong>Decoding Experiment:</strong> Generate the same prompt 10 times with different temperature values (0.1 to 2.0). Plot diversity vs. coherence.</li>
                                    <li><strong>Prompt Engineering:</strong> Create a few-shot prompt that converts natural language dates ("next Tuesday", "in two weeks") to ISO format.</li>
                                    <li><strong>Custom Generation:</strong> Implement a LogitsProcessor that enforces a specific text format (e.g., always starts sentences with capital letters).</li>
                                    <li><strong>Comparison Study:</strong> Compare GPT-2, GPT-2-medium, and GPT-2-large on the same creative writing task. Document quality differences.</li>
                                    <li><strong>Application:</strong> Build a story generator that maintains character consistency using prompt chaining.</li>
                                </ol>
                            </div>
                        </div>

                        <p>The GPT family continues to evolve rapidly, with new models, techniques, and capabilities emerging regularly. The principles covered here—autoregressive generation, scaling laws, prompt engineering, and alignment—provide a durable foundation that will remain relevant as the field advances. In the next part of this series, we'll apply these generative capabilities to core NLP tasks, seeing how GPT models can be used for classification, entity recognition, and other foundational challenges.</p>

                        <div class="related-posts">
                            <h3><i class="fas fa-book-reader me-2"></i>Continue the NLP Series</h3>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 9: Pretrained Language Models & Transfer Learning</h5>
                                <p class="text-muted small mb-2">Learn about BERT, pretraining strategies, and fine-tuning.</p>
                                <a href="nlp-pretrained-models-transfer-learning.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 11: Core NLP Tasks</h5>
                                <p class="text-muted small mb-2">Apply NLP to classification, NER, POS tagging, and parsing.</p>
                                <a href="nlp-core-tasks.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 12: Advanced NLP Tasks</h5>
                                <p class="text-muted small mb-2">Master question answering, summarization, and machine translation.</p>
                                <a href="nlp-advanced-tasks.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.</p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook"><i class="fab fa-facebook-f"></i></a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter"><i class="fab fa-twitter"></i></a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn"><i class="fab fa-linkedin-in"></i></a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube"><i class="fab fa-youtube"></i></a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram"><i class="fab fa-instagram"></i></a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest"><i class="fab fa-pinterest-p"></i></a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email"><i class="fas fa-envelope"></i></a>
                    </div>
                </div>
            </div>
            <hr class="bg-secondary">
            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small"><i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a></p>
                    <p class="small mt-3"><a href="/" class="text-light text-decoration-none">Home</a> | <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a></p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">Enjoying this content? ☕ <a href="https://buymeacoffee.com/itswzee" target="_blank" class="text-light" style="text-decoration: underline;">Keep me caffeinated</a> to keep the pixels flowing!</p>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top"><i class="fas fa-arrow-up"></i></button>
    <script src="../../../js/cookie-consent.js"></script>
    <script src="../../../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <script>
        const themes = { 'prism-theme': 'Tomorrow Night', 'prism-default': 'Default', 'prism-dark': 'Dark', 'prism-twilight': 'Twilight', 'prism-okaidia': 'Okaidia', 'prism-solarizedlight': 'Solarized Light' };
        const savedTheme = localStorage.getItem('prism-theme') || 'prism-theme';
        function switchTheme(themeId) { Object.keys(themes).forEach(id => { const link = document.getElementById(id); if (link) link.disabled = true; }); const selectedLink = document.getElementById(themeId); if (selectedLink) { selectedLink.disabled = false; localStorage.setItem('prism-theme', themeId); } document.querySelectorAll('div.code-toolbar select').forEach(dropdown => { dropdown.value = themeId; }); setTimeout(() => Prism.highlightAll(), 10); }
        document.addEventListener('DOMContentLoaded', function() { switchTheme(savedTheme); });
        Prism.plugins.toolbar.registerButton('theme-switcher', function(env) { const select = document.createElement('select'); select.setAttribute('aria-label', 'Select code theme'); Object.keys(themes).forEach(themeId => { const option = document.createElement('option'); option.value = themeId; option.textContent = themes[themeId]; if (themeId === savedTheme) option.selected = true; select.appendChild(option); }); select.addEventListener('change', function(e) { switchTheme(e.target.value); }); return select; });
    </script>

    <script>
        document.addEventListener('DOMContentLoaded', function() { const scrollToTopBtn = document.getElementById('scrollToTop'); window.addEventListener('scroll', function() { if (window.scrollY > 300) { scrollToTopBtn.classList.add('show'); } else { scrollToTopBtn.classList.remove('show'); } }); scrollToTopBtn.addEventListener('click', function() { window.scrollTo({ top: 0, behavior: 'smooth' }); }); });
        function openNav() { document.getElementById('tocSidenav').classList.add('open'); document.getElementById('tocOverlay').classList.add('show'); document.body.style.overflow = 'hidden'; }
        function closeNav() { document.getElementById('tocSidenav').classList.remove('open'); document.getElementById('tocOverlay').classList.remove('show'); document.body.style.overflow = 'auto'; }
        document.addEventListener('keydown', function(e) { if (e.key === 'Escape') closeNav(); });
    </script>
</body>
</html>
