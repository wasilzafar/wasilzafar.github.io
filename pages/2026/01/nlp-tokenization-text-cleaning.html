<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Part 2 of the Complete NLP Series: Master tokenization techniques (word, subword, character), text preprocessing, normalization, stemming, lemmatization, and building robust text cleaning pipelines." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="NLP, Tokenization, Text Cleaning, Text Preprocessing, Stemming, Lemmatization, BPE, WordPiece, SentencePiece, Regex, Stop Words" />
    <meta property="og:title" content="Tokenization & Text Cleaning - Complete NLP Series Part 2" />
    <meta property="og:description" content="Learn to convert raw text into usable input for NLP models‚Äîword, subword, and character tokenization with preprocessing best practices." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-27" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>Tokenization & Text Cleaning - Complete NLP Series Part 2 - Wasil Zafar</title>

    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Font Awesome Icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />

    <!-- Custom Styles -->
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />

    <!-- Prism.js Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />

    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <!-- Google Consent Mode v2 -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('consent', 'default', {
            'ad_storage': 'denied', 'ad_user_data': 'denied', 'ad_personalization': 'denied', 'analytics_storage': 'denied',
            'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE']
        });
        gtag('consent', 'default', { 'ad_storage': 'granted', 'ad_user_data': 'granted', 'ad_personalization': 'granted', 'analytics_storage': 'granted' });
        gtag('set', 'url_passthrough', true);
    </script>

    <!-- Google Tag Manager -->
    <script>
        (function(w, d, s, l, i) {
            w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' });
            var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : '';
            j.async = true; j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    <style>
        .blog-hero { background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%); color: white; padding: 80px 0; }
        .blog-meta { font-size: 0.95rem; color: var(--color-teal); margin-bottom: 1rem; display: flex; align-items: center; flex-wrap: wrap; gap: 1rem; }
        .print-btn { background: var(--color-teal); color: white; border: none; padding: 0.4rem 1rem; border-radius: 4px; font-size: 0.9rem; cursor: pointer; transition: all 0.3s ease; display: inline-flex; align-items: center; gap: 0.5rem; }
        .print-btn:hover { background: var(--color-crimson); transform: translateY(-1px); }
        @media print { .print-btn, nav, .navbar, footer, .back-link, .related-posts, .scroll-to-top, .toc-toggle-btn, .sidenav-toc, .sidenav-overlay { display: none !important; } * { -webkit-print-color-adjust: exact !important; print-color-adjust: exact !important; } .blog-content h2 { color: var(--color-navy) !important; border-bottom: 3px solid var(--color-teal) !important; } .blog-content h3 { color: var(--color-blue) !important; } .highlight-box { background: rgba(59, 151, 151, 0.1) !important; border-left: 4px solid var(--color-teal) !important; } }
        .blog-content { max-width: 900px; margin: 0 auto; font-size: 1.05rem; line-height: 1.8; color: #333; }
        .blog-content h2 { font-size: 1.8rem; font-weight: 700; margin-top: 2.5rem; margin-bottom: 1.5rem; color: var(--color-navy); border-bottom: 3px solid var(--color-teal); padding-bottom: 0.5rem; }
        .blog-content h3 { font-size: 1.3rem; font-weight: 600; margin-top: 2rem; margin-bottom: 1rem; color: var(--color-blue); }
        .blog-content h4 { font-size: 1.1rem; font-weight: 600; margin-top: 1.5rem; margin-bottom: 1rem; color: var(--color-teal); }
        .blog-content p { margin-bottom: 1.2rem; text-align: justify; }
        .blog-content strong { color: var(--color-crimson); }
        .blog-content pre[class*="language-"] { border-radius: 6px; margin: 1.5rem 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); }
        .blog-content p code, .blog-content li code { background: rgba(59, 151, 151, 0.1); color: var(--color-crimson); padding: 0.2rem 0.4rem; border-radius: 3px; font-family: 'Consolas', monospace; font-size: 0.9em; }
        .highlight-box { background: rgba(59, 151, 151, 0.1); border-left: 4px solid var(--color-teal); padding: 1.5rem; margin: 2rem 0; border-radius: 4px; }
        .experiment-card { background: #f8f9fa; border: 1px solid #ddd; border-radius: 8px; padding: 1.5rem; margin-bottom: 1.5rem; transition: all 0.3s ease; }
        .experiment-card:hover { box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1); transform: translateY(-2px); }
        .experiment-card h4 { color: var(--color-crimson); font-weight: 700; margin-bottom: 0.5rem; }
        .bg-teal { background-color: var(--color-teal) !important; }
        .bg-crimson { background-color: var(--color-crimson) !important; }
        .toc-toggle-btn { position: fixed; bottom: 2rem; left: 2rem; width: 50px; height: 50px; background: var(--color-teal); color: white; border: none; border-radius: 50%; font-size: 1.2rem; cursor: pointer; box-shadow: 0 4px 12px rgba(59, 151, 151, 0.4); transition: all 0.3s ease; z-index: 1049; display: flex; align-items: center; justify-content: center; }
        .toc-toggle-btn:hover { background: var(--color-crimson); transform: scale(1.1); }
        .sidenav-toc { height: calc(100% - 64px); width: 0; position: fixed; z-index: 1050; top: 64px; left: 0; background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%); overflow-x: hidden; overflow-y: auto; transition: width 0.4s ease; padding-top: 30px; box-shadow: 4px 0 15px rgba(0, 0, 0, 0.3); }
        .sidenav-toc.open { width: 350px; }
        .sidenav-toc .toc-header { display: flex; align-items: center; justify-content: space-between; padding: 20px 30px; margin-bottom: 20px; border-bottom: 2px solid var(--color-teal); opacity: 0; visibility: hidden; transition: all 0.3s ease; }
        .sidenav-toc.open .toc-header { opacity: 1; visibility: visible; }
        .sidenav-toc .closebtn { font-size: 32px; color: white; background: transparent; border: none; cursor: pointer; transition: all 0.3s ease; }
        .sidenav-toc .closebtn:hover { color: var(--color-crimson); transform: rotate(90deg); }
        .sidenav-toc h3 { color: white; margin: 0; font-weight: 700; font-size: 1.3rem; }
        .sidenav-toc ol { list-style: decimal; padding-left: 30px; margin: 0; color: rgba(255, 255, 255, 0.9); }
        .sidenav-toc ol li { margin-bottom: 8px; }
        .sidenav-toc ul { list-style-type: lower-alpha; padding-left: 30px; margin-top: 8px; }
        .sidenav-toc a { padding: 12px 30px; text-decoration: none; font-size: 0.95rem; color: rgba(255, 255, 255, 0.85); display: block; transition: all 0.3s ease; border-left: 4px solid transparent; }
        .sidenav-toc a:hover { color: white; background: rgba(59, 151, 151, 0.2); border-left-color: var(--color-teal); }
        .sidenav-toc a.active { color: white; background: rgba(191, 9, 47, 0.3); border-left-color: var(--color-crimson); font-weight: 600; }
        .sidenav-overlay { display: none; position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0, 0, 0, 0.5); z-index: 1049; }
        .sidenav-overlay.show { display: block; }
        .reading-time { display: inline-block; background: var(--color-crimson); color: white; padding: 0.3rem 0.8rem; border-radius: 4px; font-size: 0.9rem; }
        .back-link { display: inline-block; color: white; text-decoration: none; transition: all 0.3s ease; margin-bottom: 1rem; opacity: 0.9; }
        .back-link:hover { color: var(--color-teal); transform: translateX(-5px); }
        .related-posts { background: #f8f9fa; border-radius: 8px; padding: 2rem; margin-top: 3rem; }
        .related-posts h3 { color: var(--color-navy); margin-bottom: 1.5rem; }
        .related-post-item { padding: 1rem; border-left: 3px solid var(--color-teal); margin-bottom: 1rem; transition: all 0.3s ease; }
        .related-post-item:hover { background: white; border-left-color: var(--color-crimson); }
        .related-post-item a { color: var(--color-blue); text-decoration: none; font-weight: 600; }
        .related-post-item a:hover { color: var(--color-crimson); }
        div.code-toolbar > .toolbar { opacity: 1; display: flex; gap: 0.5rem; }
        div.code-toolbar > .toolbar > .toolbar-item > button { background: var(--color-teal); color: white; border: none; padding: 0.4rem 0.8rem; border-radius: 4px; font-size: 0.85rem; cursor: pointer; }
        div.code-toolbar > .toolbar > .toolbar-item > select { background: var(--color-navy); color: white; border: 1px solid var(--color-teal); padding: 0.4rem 0.8rem; border-radius: 4px; font-size: 0.85rem; }
        .scroll-to-top { position: fixed; bottom: 2rem; right: 2rem; width: 50px; height: 50px; background: var(--color-teal); color: white; border: none; border-radius: 50%; font-size: 1.2rem; cursor: pointer; display: flex; align-items: center; justify-content: center; opacity: 0; visibility: hidden; transition: all 0.3s ease; box-shadow: 0 4px 12px rgba(59, 151, 151, 0.3); z-index: 999; }
        .scroll-to-top.show { opacity: 1; visibility: visible; }
        .scroll-to-top:hover { background: var(--color-crimson); transform: translateY(-3px); }
        @media (max-width: 768px) { .sidenav-toc.open { width: 280px; } .toc-toggle-btn { left: 15px; } }
        html { scroll-behavior: smooth; }
    </style>
</head>
<body>
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/"><span class="gradient-text">Wasil Zafar</span></a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link" href="/">Home</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#about">About</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#certifications">Certifications</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#interests">Interests</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link"><i class="fas fa-arrow-left me-2"></i>Back to Technology</a>
                <h1 class="display-4 fw-bold mb-3">Tokenization & Text Cleaning</h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 27, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-1"></i>30 min read</span>
                    <button onclick="window.print()" class="print-btn" title="Print this article"><i class="fas fa-print"></i> Print</button>
                </div>
                <p class="lead">Part 2 of 16: Convert raw text into usable input‚Äîword, subword, and character tokenization with text preprocessing pipelines.</p>
            </div>
        </div>
    </section>

    <!-- TOC Toggle Button -->
    <button class="toc-toggle-btn" onclick="openNav()" title="Table of Contents" aria-label="Open Table of Contents"><i class="fas fa-list"></i></button>

    <!-- Side Navigation -->
    <div id="tocSidenav" class="sidenav-toc">
        <div class="toc-header">
            <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
            <button class="closebtn" onclick="closeNav()" aria-label="Close">&times;</button>
        </div>
        <ol>
            <li><a href="#introduction" onclick="closeNav()">Introduction to Tokenization</a></li>
            <li><a href="#word-tokenization" onclick="closeNav()">Word Tokenization</a>
                <ul>
                    <li><a href="#whitespace" onclick="closeNav()">Whitespace Tokenization</a></li>
                    <li><a href="#regex" onclick="closeNav()">Regex-Based Tokenization</a></li>
                    <li><a href="#nltk-spacy" onclick="closeNav()">NLTK & spaCy Tokenizers</a></li>
                </ul>
            </li>
            <li><a href="#subword-tokenization" onclick="closeNav()">Subword Tokenization</a>
                <ul>
                    <li><a href="#bpe" onclick="closeNav()">Byte Pair Encoding (BPE)</a></li>
                    <li><a href="#wordpiece" onclick="closeNav()">WordPiece</a></li>
                    <li><a href="#sentencepiece" onclick="closeNav()">SentencePiece & Unigram</a></li>
                </ul>
            </li>
            <li><a href="#character-tokenization" onclick="closeNav()">Character Tokenization</a></li>
            <li><a href="#text-preprocessing" onclick="closeNav()">Text Preprocessing</a>
                <ul>
                    <li><a href="#lowercasing" onclick="closeNav()">Lowercasing & Normalization</a></li>
                    <li><a href="#stopwords" onclick="closeNav()">Stop Word Removal</a></li>
                    <li><a href="#stemming" onclick="closeNav()">Stemming</a></li>
                    <li><a href="#lemmatization" onclick="closeNav()">Lemmatization</a></li>
                </ul>
            </li>
            <li><a href="#special-cases" onclick="closeNav()">Special Cases & Edge Cases</a></li>
            <li><a href="#pipelines" onclick="closeNav()">Building Preprocessing Pipelines</a></li>
            <li><a href="#conclusion" onclick="closeNav()">Conclusion & Next Steps</a></li>
        </ol>
    </div>

    <!-- Overlay -->
    <div id="tocOverlay" class="sidenav-overlay" onclick="closeNav()"></div>

    <!-- Main Content -->
    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="blog-content">
                        
                        <h2 id="introduction"><i class="fas fa-cut me-2"></i>Introduction to Tokenization</h2>
                        
                        <p>Tokenization is the first step in any NLP pipeline‚Äîconverting raw text into discrete units (tokens) that models can process. In this second part of our series, we'll explore different tokenization strategies and text cleaning techniques.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-star me-2"></i>Key Insight</h4>
                            <p><strong>The choice of tokenization strategy directly impacts model performance. Subword tokenization (BPE, WordPiece) has become the standard for modern neural models.</strong></p>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-map-signs me-2"></i>Complete NLP Series Navigation</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">16-Part Series</span>
                                <span class="badge bg-crimson">NLP Mastery</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><a href="nlp-fundamentals-linguistic-basics.html">NLP Fundamentals & Linguistic Basics</a></li>
                                    <li><strong>Tokenization & Text Cleaning (This Guide)</strong></li>
                                    <li><a href="nlp-text-representation-features.html">Text Representation & Feature Engineering</a></li>
                                    <li><a href="nlp-word-embeddings.html">Word Embeddings</a></li>
                                    <li><a href="nlp-statistical-language-models.html">Statistical Language Models & N-grams</a></li>
                                    <li><a href="nlp-neural-networks.html">Neural Networks for NLP</a></li>
                                    <li><a href="nlp-rnn-lstm-gru.html">RNNs, LSTMs & GRUs</a></li>
                                    <li><a href="nlp-transformers-attention.html">Transformers & Attention Mechanism</a></li>
                                    <li><a href="nlp-pretrained-models-transfer-learning.html">Pretrained Language Models & Transfer Learning</a></li>
                                    <li><a href="nlp-gpt-text-generation.html">GPT Models & Text Generation</a></li>
                                    <li><a href="nlp-core-tasks.html">Core NLP Tasks</a></li>
                                    <li><a href="nlp-advanced-tasks.html">Advanced NLP Tasks</a></li>
                                    <li><a href="nlp-multilingual-crosslingual.html">Multilingual & Cross-lingual NLP</a></li>
                                    <li><a href="nlp-evaluation-ethics.html">Evaluation, Ethics & Responsible NLP</a></li>
                                    <li><a href="nlp-systems-production.html">NLP Systems, Optimization & Production</a></li>
                                    <li><a href="nlp-cutting-edge-research.html">Cutting-Edge & Research Topics</a></li>
                                </ol>
                            </div>
                        </div>

                        <h2 id="word-tokenization"><i class="fas fa-font me-2"></i>Word Tokenization</h2>
                        
                        <p>Word tokenization is the process of splitting text into individual words or word-like units. This is often the first step in traditional NLP pipelines, transforming a continuous string of characters into a sequence of meaningful tokens. The quality of tokenization directly impacts downstream tasks like part-of-speech tagging, named entity recognition, and sentiment analysis.</p>
                        
                        <p>While the concept seems simple‚Äî"just split on spaces"‚Äîreal-world text is far more complex. Consider contractions ("don't" ‚Üí "do" + "n't"?), hyphenated words ("state-of-the-art"), possessives ("John's"), and punctuation handling. Different tokenizers make different decisions about these edge cases, and the "right" choice depends on your specific application and downstream model.</p>
                        
                        <p>Modern NLP has largely moved toward <strong>subword tokenization</strong> for neural models, but word tokenization remains essential for many traditional ML approaches, linguistic analysis, and as a preprocessing step before subword tokenization. Understanding word tokenization fundamentals helps you make informed decisions about your text processing pipeline.</p>

                        <h3 id="whitespace">Whitespace Tokenization</h3>
                        
                        <p>The simplest tokenization approach is splitting on whitespace characters (spaces, tabs, newlines). While naive, this baseline method is surprisingly effective for many applications and serves as a fast, language-agnostic starting point. It's commonly used in information retrieval systems and quick text analysis tasks.</p>
                        
                        <p>However, whitespace tokenization has significant limitations: it doesn't separate punctuation from words ("hello," vs "hello"), struggles with languages that don't use spaces (Chinese, Japanese, Thai), and mishandles contractions and possessives. For production NLP systems, you'll typically need more sophisticated approaches.</p>

<pre><code class="language-python"># Whitespace Tokenization - Simple but limited

text = "Hello, world! How are you doing today? I'm learning NLP."

# Method 1: Python's built-in split()
basic_tokens = text.split()
print("Basic split():")
print(basic_tokens)
# Output: ['Hello,', 'world!', 'How', 'are', 'you', 'doing', 'today?', "I'm", 'learning', 'NLP.']

# Method 2: split on specific characters
import re
whitespace_tokens = re.split(r'\s+', text)
print("\nRegex whitespace split:")
print(whitespace_tokens)

# Method 3: Handle multiple whitespace types
text_with_tabs = "Hello\tworld\nHow  are   you"
clean_tokens = text_with_tabs.split()
print("\nHandling tabs and multiple spaces:")
print(clean_tokens)
# Output: ['Hello', 'world', 'How', 'are', 'you']

# Limitation: Punctuation stays attached
print("\nNote: 'Hello,' includes the comma!")
</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>When to Use Whitespace Tokenization</h4>
                            <p>Whitespace tokenization is suitable for quick prototyping, baseline comparisons, and applications where punctuation handling isn't critical. For production systems, always use a proper tokenizer like NLTK or spaCy that handles edge cases correctly.</p>
                        </div>

                        <h3 id="regex">Regex-Based Tokenization</h3>
                        
                        <p>Regular expressions (regex) provide powerful pattern matching for custom tokenization rules. By defining explicit patterns for what constitutes a token, you gain fine-grained control over how text is split. This approach is particularly useful when you have domain-specific tokenization requirements that standard libraries don't handle well.</p>
                        
                        <p>Regex tokenization lets you handle punctuation, special characters, and complex patterns in a single pass. You can define patterns to extract words, numbers, email addresses, URLs, and other entities simultaneously. The trade-off is complexity‚Äîregex patterns can become difficult to maintain and may have performance implications for very large texts.</p>

<pre><code class="language-python"># Regex-Based Tokenization - Full control over patterns
import re

text = "Hello, world! Email me at john@example.com. Price: $99.99 #NLP"

# Pattern 1: Split on non-word characters
pattern1 = r'\W+'
tokens1 = re.split(pattern1, text)
print("Split on non-word chars:")
print([t for t in tokens1 if t])  # Filter empty strings
# Output: ['Hello', 'world', 'Email', 'me', 'at', 'john', 'example', 'com', ...]

# Pattern 2: Extract word-like tokens (more nuanced)
pattern2 = r'\b\w+\b'
tokens2 = re.findall(pattern2, text)
print("\nExtract word tokens:")
print(tokens2)
# Output: ['Hello', 'world', 'Email', 'me', 'at', 'john', 'example', 'com', ...]

# Pattern 3: Preserve emails, numbers, hashtags, and words
complex_pattern = r'''
    \b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b |  # Email
    \$?\d+\.?\d*                                          |  # Numbers/prices
    \#\w+                                                  |  # Hashtags
    \b\w+\b                                                   # Words
'''
tokens3 = re.findall(complex_pattern, text, re.VERBOSE)
print("\nComplex pattern (emails, prices, hashtags):")
print(tokens3)
# Output: ['Hello', 'world', 'Email', 'me', 'at', 'john@example.com', 'Price', '$99.99', '#NLP']
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-flask me-2"></i>Practical: Custom Tokenizer for Social Media</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Regex</span>
                                <span class="badge bg-crimson">Twitter/X Data</span>
                            </div>
                            <div class="content">
                                <p>Social media text requires special handling for mentions, hashtags, URLs, and emojis:</p>
<pre><code class="language-python"># Social Media Tokenizer
import re

tweet = "Hey @john! Check out https://example.com üöÄ #AI #MachineLearning is amazing! üòä"

# Comprehensive social media pattern
social_pattern = r'''
    https?://[^\s]+                 |  # URLs
    @\w+                            |  # Mentions
    \#\w+                           |  # Hashtags
    [\U0001F600-\U0001F64F]         |  # Emoticons
    [\U0001F300-\U0001F5FF]         |  # Misc Symbols
    [\U0001F680-\U0001F6FF]         |  # Transport/Map
    \b\w+\b                         |  # Words
    [!?]+                              # Punctuation clusters
'''

tokens = re.findall(social_pattern, tweet, re.VERBOSE)
print("Social media tokens:")
for i, token in enumerate(tokens):
    print(f"  {i+1}. '{token}'")

# Output preserves @mentions, #hashtags, URLs, and emojis as single tokens
</code></pre>
                            </div>
                        </div>

                        <h3 id="nltk-spacy">NLTK & spaCy Tokenizers</h3>
                        
                        <p>NLTK (Natural Language Toolkit) and spaCy are the two most popular NLP libraries in Python, each offering robust tokenization. NLTK provides multiple tokenizer options with different trade-offs, while spaCy offers a single, highly optimized tokenizer designed for production use. Both handle contractions, punctuation, and special cases far better than simple regex approaches.</p>
                        
                        <p>NLTK's <code>word_tokenize()</code> uses the Penn Treebank tokenization standard, which separates contractions ("don't" ‚Üí "do", "n't") and handles punctuation intelligently. spaCy's tokenizer is rule-based with customizable exceptions and achieves excellent performance through Cython optimization. For most applications, spaCy is faster and more suitable for production, while NLTK is better for learning and experimentation.</p>

<pre><code class="language-python"># NLTK Tokenization - Multiple options
import nltk
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True)
from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer
from nltk.tokenize import RegexpTokenizer

text = "Dr. Smith's presentation wasn't boring! It covered NLP, ML, and AI. What do you think?"

# Sentence tokenization
sentences = sent_tokenize(text)
print("Sentences:")
for i, sent in enumerate(sentences, 1):
    print(f"  {i}. {sent}")

# Word tokenization (Penn Treebank style)
words = word_tokenize(text)
print("\nWord tokens (NLTK):")
print(words)
# Note: "wasn't" becomes "was" + "n't", "Smith's" becomes "Smith" + "'s"

# TreebankWordTokenizer explicitly
treebank = TreebankWordTokenizer()
tb_tokens = treebank.tokenize(text)
print("\nTreebank tokens:")
print(tb_tokens)
</code></pre>

<pre><code class="language-python"># spaCy Tokenization - Fast, production-ready
import spacy

# Load English model (run: python -m spacy download en_core_web_sm)
nlp = spacy.load("en_core_web_sm")

text = "Dr. Smith's presentation wasn't boring! It covered NLP, ML, and AI. What do you think?"

# Process text
doc = nlp(text)

# Extract tokens
print("spaCy tokens:")
for token in doc:
    print(f"  '{token.text}' - POS: {token.pos_}, Lemma: {token.lemma_}")

# Token-level information available
print("\nToken attributes:")
for token in doc:
    if not token.is_space:
        print(f"  {token.text:12} | is_alpha: {token.is_alpha} | is_punct: {token.is_punct} | is_stop: {token.is_stop}")
</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-balance-scale me-2"></i>NLTK vs spaCy: When to Use Which</h4>
                            <p><strong>Choose NLTK</strong> when you need educational tools, multiple algorithm options, or are working on research. <strong>Choose spaCy</strong> for production systems, when speed matters, or when you need an integrated NLP pipeline (tokenization + POS + NER in one pass).</p>
                        </div>

                        <h2 id="subword-tokenization"><i class="fas fa-puzzle-piece me-2"></i>Subword Tokenization</h2>
                        
                        <p>Subword tokenization represents a fundamental shift in how we process text for neural models. Instead of treating whole words as atomic units (which creates massive vocabularies and out-of-vocabulary problems) or individual characters (which loses semantic meaning), subword methods find a middle ground by breaking words into meaningful subunits.</p>
                        
                        <p>The key insight is that many words share common prefixes, suffixes, and roots. By learning these recurring patterns, subword tokenizers can represent any word‚Äîincluding rare and novel words‚Äîusing a fixed vocabulary of subword units. This is why modern language models like GPT, BERT, and T5 all use subword tokenization. The three main algorithms are <strong>Byte Pair Encoding (BPE)</strong>, <strong>WordPiece</strong>, and <strong>Unigram/SentencePiece</strong>.</p>

                        <h3 id="bpe">Byte Pair Encoding (BPE)</h3>
                        
                        <p>Byte Pair Encoding, originally a data compression algorithm, was adapted for NLP by Sennrich et al. (2016). BPE iteratively merges the most frequent pair of adjacent tokens in the training corpus, starting from individual characters. After many iterations, common words remain whole while rare words are split into subwords. The number of merge operations determines vocabulary size.</p>
                        
                        <p>BPE is used by GPT-2, GPT-3, GPT-4, RoBERTa, and many other models. Its popularity stems from its simplicity, effectiveness, and ability to handle any text including rare words, typos, and foreign words by falling back to character-level tokens when needed.</p>

<pre><code class="language-python"># Understanding BPE: Step-by-step demonstration
# This shows the algorithm conceptually

def simple_bpe_demo():
    """Demonstrate BPE algorithm concept."""
    # Initial corpus (word frequencies)
    corpus = {
        'low': 5,
        'lower': 2,
        'newest': 6,
        'widest': 3
    }
    
    # Step 1: Start with character vocabulary + end-of-word symbol
    print("Step 1: Initial character-level representation")
    char_corpus = {}
    for word, freq in corpus.items():
        chars = ' '.join(list(word)) + ' </w>'  # </w> marks word boundary
        char_corpus[chars] = freq
        print(f"  '{word}' ({freq}x) ‚Üí '{chars}'")
    
    # Step 2: Count adjacent pairs
    print("\nStep 2: Count bigram pairs")
    pairs = {}
    for word, freq in char_corpus.items():
        symbols = word.split()
        for i in range(len(symbols) - 1):
            pair = (symbols[i], symbols[i+1])
            pairs[pair] = pairs.get(pair, 0) + freq
    
    # Show top pairs
    sorted_pairs = sorted(pairs.items(), key=lambda x: -x[1])
    for pair, count in sorted_pairs[:5]:
        print(f"  {pair}: {count}")
    
    print("\nStep 3: Merge most frequent pair ('e', 's') ‚Üí 'es'")
    print("Step 4: Repeat until desired vocabulary size reached")
    
simple_bpe_demo()
</code></pre>

<pre><code class="language-python"># Using BPE with Hugging Face Tokenizers
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

# Create a BPE tokenizer from scratch
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = Whitespace()

# Training data (in practice, use large corpus)
training_data = [
    "Machine learning is transforming technology.",
    "Deep learning models learn representations automatically.",
    "Transformers revolutionized natural language processing.",
    "Neural networks can learn complex patterns."
]

# Save to temporary files for training
import tempfile
import os

with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
    f.write('\n'.join(training_data))
    temp_file = f.name

# Train tokenizer
trainer = BpeTrainer(vocab_size=100, special_tokens=["[UNK]", "[PAD]", "[CLS]", "[SEP]"])
tokenizer.train([temp_file], trainer)

# Test tokenization
test_text = "Transformers learn representations"
encoding = tokenizer.encode(test_text)
print(f"Text: '{test_text}'")
print(f"Tokens: {encoding.tokens}")
print(f"Token IDs: {encoding.ids}")

# Cleanup
os.unlink(temp_file)
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-robot me-2"></i>Using GPT-2's BPE Tokenizer</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Hugging Face</span>
                                <span class="badge bg-crimson">GPT-2</span>
                            </div>
                            <div class="content">
<pre><code class="language-python"># GPT-2 BPE Tokenizer via Hugging Face
from transformers import GPT2Tokenizer

# Load pre-trained tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Example texts
texts = [
    "Hello, world!",
    "Tokenization is fundamental to NLP.",
    "Pneumonoultramicroscopicsilicovolcanoconiosis",  # Longest English word
    "transformers revolutionized NLP"
]

for text in texts:
    tokens = tokenizer.tokenize(text)
    ids = tokenizer.encode(text)
    print(f"\nText: '{text}'")
    print(f"Tokens ({len(tokens)}): {tokens}")
    print(f"IDs: {ids}")
    # Decode back
    decoded = tokenizer.decode(ids)
    print(f"Decoded: '{decoded}'")
</code></pre>
                            </div>
                        </div>

                        <h3 id="wordpiece">WordPiece</h3>
                        
                        <p>WordPiece, developed by Google, is similar to BPE but uses a different criterion for merging tokens. Instead of choosing the most frequent pair, WordPiece selects the pair that maximizes the likelihood of the training corpus when merged. This subtle difference leads to slightly different vocabulary construction and is the tokenization method used by BERT and its variants (DistilBERT, RoBERTa variants, ALBERT).</p>
                        
                        <p>A distinctive feature of WordPiece is its use of the <code>##</code> prefix to indicate subword tokens that continue a word. For example, "playing" might become ["play", "##ing"]. This makes it easy to identify word boundaries and reconstruct original text from tokens.</p>

<pre><code class="language-python"># WordPiece Tokenization with BERT
from transformers import BertTokenizer

# Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Example texts showing WordPiece behavior
texts = [
    "I love playing basketball.",
    "Unbelievably amazing!",
    "Tokenization preprocessing pipeline",
    "COVID-19 pandemic affected worldwide economies."
]

for text in texts:
    # Tokenize
    tokens = tokenizer.tokenize(text)
    ids = tokenizer.encode(text, add_special_tokens=True)
    
    print(f"\nText: '{text}'")
    print(f"Tokens: {tokens}")
    print(f"IDs: {ids}")
    
    # Show ## continuation markers
    continuations = [t for t in tokens if t.startswith('##')]
    if continuations:
        print(f"Subword continuations: {continuations}")

# WordPiece handles unknown words by breaking them down
unknown_word = "supercalifragilisticexpialidocious"
print(f"\nUnknown word: '{unknown_word}'")
print(f"WordPiece tokens: {tokenizer.tokenize(unknown_word)}")
</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-code me-2"></i>BPE vs WordPiece: Key Difference</h4>
                            <p><strong>BPE</strong> merges the most frequent pair at each step. <strong>WordPiece</strong> merges the pair that maximizes likelihood: P(corpus) / (P(a) √ó P(b)) for merging tokens a and b. In practice, both produce similar results, but WordPiece tends to create slightly more linguistically meaningful subwords.</p>
                        </div>

                        <h3 id="sentencepiece">SentencePiece & Unigram</h3>
                        
                        <p>SentencePiece, developed by Google, is a language-independent tokenization library that treats text as a raw stream of Unicode characters without requiring pre-tokenization (splitting on spaces). This makes it ideal for languages like Japanese, Chinese, and Thai where words aren't separated by spaces. SentencePiece supports both BPE and Unigram algorithms.</p>
                        
                        <p>The <strong>Unigram</strong> algorithm takes a different approach than BPE. Instead of iteratively merging tokens, it starts with a large vocabulary and progressively removes tokens that least impact the corpus likelihood. This probabilistic approach allows Unigram to output multiple possible tokenizations for the same text, which can be useful for data augmentation. T5, ALBERT, and XLNet use SentencePiece with Unigram.</p>

<pre><code class="language-python"># SentencePiece with Unigram Model
import sentencepiece as spm
import tempfile
import os

# Training data
training_text = """Machine learning is a subset of artificial intelligence.
Deep learning uses neural networks with many layers.
Natural language processing enables computers to understand human language.
Tokenization is the first step in most NLP pipelines.
Subword tokenization helps handle rare and unknown words effectively.
"""

# Save training data to file
with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
    f.write(training_text)
    train_file = f.name

# Train SentencePiece model with Unigram
model_prefix = tempfile.mktemp()
spm.SentencePieceTrainer.train(
    input=train_file,
    model_prefix=model_prefix,
    vocab_size=100,
    model_type='unigram',  # or 'bpe'
    character_coverage=1.0,
    pad_id=0,
    unk_id=1,
    bos_id=2,
    eos_id=3
)

# Load trained model
sp = spm.SentencePieceProcessor()
sp.load(f'{model_prefix}.model')

# Tokenize text
test_text = "Deep learning models learn representations"
print(f"Text: '{test_text}'")
print(f"Tokens: {sp.encode_as_pieces(test_text)}")
print(f"IDs: {sp.encode_as_ids(test_text)}")

# Unigram can sample different tokenizations
print("\nMultiple tokenizations (Unigram sampling):")
for i in range(3):
    tokens = sp.encode(test_text, enable_sampling=True, alpha=0.1, nbest_size=-1)
    print(f"  Sample {i+1}: {sp.decode_ids(tokens)}")

# Cleanup
os.unlink(train_file)
os.unlink(f'{model_prefix}.model')
os.unlink(f'{model_prefix}.vocab')
</code></pre>

<pre><code class="language-python"># Using T5 Tokenizer (SentencePiece-based)
from transformers import T5Tokenizer

# Load T5 tokenizer
tokenizer = T5Tokenizer.from_pretrained('t5-small')

# T5 uses SentencePiece with special handling
texts = [
    "Translate English to French: Hello, how are you?",
    "summarize: Machine learning is a field of study.",
    "The quick brown fox jumps over the lazy dog."
]

for text in texts:
    tokens = tokenizer.tokenize(text)
    ids = tokenizer.encode(text)
    print(f"\nText: '{text}'")
    print(f"Tokens: {tokens}")
    print(f"IDs: {ids}")

# Note: T5 uses '‚ñÅ' (Unicode character) for word boundaries, not spaces
print("\nNote: '‚ñÅ' indicates the start of a new word in SentencePiece")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-table me-2"></i>Tokenizer Comparison Table</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Reference</span>
                                <span class="badge bg-crimson">Algorithm Comparison</span>
                            </div>
                            <div class="content">
                                <table class="table table-bordered">
                                    <thead class="table-dark">
                                        <tr>
                                            <th>Algorithm</th>
                                            <th>Used By</th>
                                            <th>Subword Marker</th>
                                            <th>Key Feature</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td><strong>BPE</strong></td>
                                            <td>GPT-2, GPT-3, GPT-4, RoBERTa</td>
                                            <td>None (or ƒ† for space)</td>
                                            <td>Frequency-based merging</td>
                                        </tr>
                                        <tr>
                                            <td><strong>WordPiece</strong></td>
                                            <td>BERT, DistilBERT, Electra</td>
                                            <td>## prefix</td>
                                            <td>Likelihood-based merging</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Unigram</strong></td>
                                            <td>T5, ALBERT, XLNet</td>
                                            <td>‚ñÅ prefix</td>
                                            <td>Probabilistic, sampling-capable</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>

                        <h2 id="character-tokenization"><i class="fas fa-keyboard me-2"></i>Character Tokenization</h2>
                        
                        <p>Character tokenization treats each character as a separate token. While this creates very small vocabularies (typically 100-300 characters for most languages), it results in much longer sequences that are computationally expensive to process. Character-level models must learn to compose characters into meaningful units, which requires more model capacity and training data.</p>
                        
                        <p>Despite these challenges, character tokenization has advantages: it naturally handles any word including misspellings, neologisms, and morphologically rich languages. It's particularly useful for tasks like text generation where character-level control is needed, or for languages with complex writing systems. Some models like ELMo use character-level representations as a component alongside word-level features.</p>

<pre><code class="language-python"># Character Tokenization Examples

def char_tokenize(text, add_special=False):
    """Simple character tokenizer."""
    tokens = list(text)
    if add_special:
        tokens = ['[CLS]'] + tokens + ['[SEP]']
    return tokens

# Basic character tokenization
text = "Hello World!"
char_tokens = char_tokenize(text)
print(f"Text: '{text}'")
print(f"Character tokens: {char_tokens}")
print(f"Number of tokens: {len(char_tokens)}")

# With special tokens
char_tokens_special = char_tokenize(text, add_special=True)
print(f"\nWith special tokens: {char_tokens_special}")

# Create character vocabulary
vocab = sorted(set(text.lower()))
char_to_id = {char: i for i, char in enumerate(vocab)}
print(f"\nCharacter vocabulary: {char_to_id}")

# Encode to IDs
ids = [char_to_id.get(c.lower(), -1) for c in text]
print(f"Encoded IDs: {ids}")
</code></pre>

<pre><code class="language-python"># Character-level processing with PyTorch
import string

# Build comprehensive character vocabulary
all_chars = string.ascii_letters + string.digits + string.punctuation + ' \n\t'
vocab = {char: idx for idx, char in enumerate(all_chars)}
vocab['[UNK]'] = len(vocab)
vocab['[PAD]'] = len(vocab) + 1

print(f"Vocabulary size: {len(vocab)}")
print(f"Sample mappings: 'a'={vocab['a']}, 'Z'={vocab['Z']}, '!'={vocab['!']}")

# Encode function
def encode_chars(text, vocab, max_len=50):
    """Encode text to character IDs with padding."""
    ids = [vocab.get(c, vocab['[UNK]']) for c in text]
    # Pad or truncate
    if len(ids) < max_len:
        ids += [vocab['[PAD]']] * (max_len - len(ids))
    else:
        ids = ids[:max_len]
    return ids

# Example
text = "Deep Learning"
encoded = encode_chars(text, vocab, max_len=20)
print(f"\nText: '{text}'")
print(f"Encoded (padded to 20): {encoded}")

# Decode back
id_to_char = {v: k for k, v in vocab.items()}
decoded = ''.join([id_to_char[i] for i in encoded if id_to_char[i] != '[PAD]'])
print(f"Decoded: '{decoded}'")
</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-chart-bar me-2"></i>Sequence Length Comparison</h4>
                            <p>For the sentence "Tokenization is important": <strong>Word-level</strong> = 3 tokens, <strong>Subword (BPE)</strong> ‚âà 5-7 tokens, <strong>Character-level</strong> = 24 tokens. Longer sequences mean more computation in attention-based models (O(n¬≤) complexity), which is why subword tokenization is the standard for Transformers.</p>
                        </div>

                        <h2 id="text-preprocessing"><i class="fas fa-broom me-2"></i>Text Preprocessing</h2>
                        
                        <p>Text preprocessing transforms raw, messy text into a clean, standardized format suitable for NLP models. The preprocessing steps you choose significantly impact model performance‚Äîtoo aggressive preprocessing loses important information, while too little leaves noise that confuses models. The right balance depends on your specific task and model architecture.</p>
                        
                        <p>Common preprocessing steps include lowercasing, removing punctuation, handling special characters, removing stop words, and normalizing text (stemming or lemmatization). Modern neural models like BERT often require minimal preprocessing since they learn robust representations from raw text, but traditional ML approaches and some specific applications still benefit from careful preprocessing.</p>

                        <h3 id="lowercasing">Lowercasing & Normalization</h3>
                        
                        <p>Lowercasing converts all text to lowercase, reducing vocabulary size by merging case variants ("The", "the", "THE" ‚Üí "the"). While this simplifies processing, it loses information‚Äî"apple" (fruit) vs "Apple" (company), "US" (country) vs "us" (pronoun). Modern models like BERT have cased and uncased versions, letting you choose based on whether case distinctions matter for your task.</p>
                        
                        <p>Text normalization goes beyond lowercasing to standardize various text elements: converting Unicode characters to ASCII, expanding contractions, standardizing whitespace, and handling encoding issues. This is especially important when combining text from multiple sources with different formatting conventions.</p>

<pre><code class="language-python"># Text Lowercasing and Normalization
import unicodedata
import re

text = "HELLO World! Caf√© na√Øve r√©sum√©... It's a TEST  with   extra spaces."

# Basic lowercasing
lower_text = text.lower()
print(f"Original: {text}")
print(f"Lowercased: {lower_text}")

# Unicode normalization (handling accents)
# NFD: Decompose characters (√© ‚Üí e + combining accent)
# NFC: Compose characters (e + combining accent ‚Üí √©)
normalized_nfd = unicodedata.normalize('NFD', text)
print(f"\nNFD normalized: {normalized_nfd}")

# Remove accents (convert to ASCII)
def remove_accents(text):
    """Remove accents from text."""
    nfkd = unicodedata.normalize('NFKD', text)
    return ''.join(c for c in nfkd if not unicodedata.combining(c))

ascii_text = remove_accents(text)
print(f"Accents removed: {ascii_text}")

# Normalize whitespace
def normalize_whitespace(text):
    """Replace multiple spaces with single space."""
    return ' '.join(text.split())

clean_text = normalize_whitespace(text)
print(f"\nWhitespace normalized: {clean_text}")
</code></pre>

<pre><code class="language-python"># Comprehensive Text Normalization Pipeline
import re
import html

def normalize_text(text):
    """Complete text normalization."""
    # Decode HTML entities
    text = html.unescape(text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Expand contractions
    contractions = {
        "won't": "will not", "can't": "cannot", "n't": " not",
        "'re": " are", "'s": " is", "'d": " would",
        "'ll": " will", "'ve": " have", "'m": " am"
    }
    for contraction, expansion in contractions.items():
        text = text.replace(contraction, expansion)
    
    # Normalize unicode
    text = text.encode('ascii', 'ignore').decode('ascii')
    
    # Remove extra whitespace
    text = ' '.join(text.split())
    
    return text

# Test
raw_text = "I won't go there! It's a caf√©... &amp; more"
cleaned = normalize_text(raw_text)
print(f"Raw: {raw_text}")
print(f"Normalized: {cleaned}")
# Output: "i will not go there! it is a cafe... & more"
</code></pre>

                        <h3 id="stopwords">Stop Word Removal</h3>
                        
                        <p>Stop words are common words like "the", "is", "at", "which" that appear frequently but carry little semantic meaning. Removing them reduces vocabulary size and can improve performance for traditional ML models like TF-IDF based classifiers. However, stop word removal is increasingly controversial‚Äîneural models often benefit from keeping stop words since they help understand sentence structure and relationships.</p>
                        
                        <p>Different NLP libraries have different stop word lists, and you may need to customize them for your domain. For example, in a medical context, "patient" shouldn't be a stop word even though it's common. Be thoughtful about stop word removal: for sentiment analysis, words like "not" are crucial and shouldn't be removed even though they're common.</p>

<pre><code class="language-python"># Stop Word Removal with NLTK and spaCy
import nltk
nltk.download('stopwords', quiet=True)
from nltk.corpus import stopwords

# Get NLTK stop words
nltk_stops = set(stopwords.words('english'))
print(f"NLTK stop words count: {len(nltk_stops)}")
print(f"Sample: {list(nltk_stops)[:10]}")

# Example text
text = "The quick brown fox jumps over the lazy dog in the park"
words = text.lower().split()

# Remove stop words
filtered_words = [w for w in words if w not in nltk_stops]
print(f"\nOriginal: {words}")
print(f"Filtered: {filtered_words}")
# Output: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', 'park']
</code></pre>

<pre><code class="language-python"># spaCy Stop Words with Customization
import spacy

nlp = spacy.load("en_core_web_sm")

# View spaCy stop words
print(f"spaCy stop words count: {len(nlp.Defaults.stop_words)}")

# Process text
text = "The product is not good, but the service was excellent!"
doc = nlp(text)

# Filter stop words
filtered = [token.text for token in doc if not token.is_stop and not token.is_punct]
print(f"Original: {text}")
print(f"Filtered: {filtered}")

# Customize stop words
print("\n--- Customizing stop words ---")
# Add custom stop word
nlp.Defaults.stop_words.add("product")
nlp.vocab["product"].is_stop = True

# Remove from stop words (important for negation!)
nlp.Defaults.stop_words.discard("not")
nlp.vocab["not"].is_stop = False

# Re-process
doc2 = nlp(text)
filtered2 = [token.text for token in doc2 if not token.is_stop and not token.is_punct]
print(f"Custom filtered: {filtered2}")
# Now includes "not" but excludes "product"
</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-exclamation-circle me-2"></i>When NOT to Remove Stop Words</h4>
                            <p><strong>Sentiment Analysis:</strong> "This is not good" vs "This is good" ‚Äî removing "not" changes meaning completely. <strong>Question Answering:</strong> Words like "who", "what", "where" are stop words but essential for understanding queries. <strong>Neural Models:</strong> BERT, GPT etc. learn from context including stop words; removing them often hurts performance.</p>
                        </div>

                        <h3 id="stemming">Stemming</h3>
                        
                        <p>Stemming reduces words to their root form by removing suffixes using rule-based heuristics. The resulting "stem" isn't necessarily a valid word‚Äî"running", "runs", "ran" might all become "run", but "studies" becomes "studi". This aggressive normalization reduces vocabulary size but can merge unrelated words and create non-words.</p>
                        
                        <p>The most popular stemmer is the <strong>Porter Stemmer</strong>, with the <strong>Snowball Stemmer</strong> (Porter2) being its improved version. Stemming is fast and works reasonably well for information retrieval (search engines), but for tasks requiring linguistic accuracy, lemmatization is preferred. Stemming was more popular before neural models; modern deep learning approaches often skip stemming entirely.</p>

<pre><code class="language-python"># Stemming with NLTK
import nltk
nltk.download('punkt', quiet=True)
from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer
from nltk.tokenize import word_tokenize

# Initialize stemmers
porter = PorterStemmer()
snowball = SnowballStemmer('english')
lancaster = LancasterStemmer()  # Most aggressive

# Test words
words = ['running', 'runs', 'ran', 'easily', 'fairly', 'studies', 
         'studying', 'connected', 'connection', 'connecting']

print("Stemmer Comparison:")
print(f"{'Word':<15} {'Porter':<12} {'Snowball':<12} {'Lancaster':<12}")
print("-" * 51)
for word in words:
    p = porter.stem(word)
    s = snowball.stem(word)
    l = lancaster.stem(word)
    print(f"{word:<15} {p:<12} {s:<12} {l:<12}")
</code></pre>

<pre><code class="language-python"># Stemming a Full Sentence
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt_tab', quiet=True)

stemmer = PorterStemmer()

sentence = "The striped cats are running and playing happily in the gardens"

# Tokenize and stem
tokens = word_tokenize(sentence.lower())
stemmed = [stemmer.stem(token) for token in tokens]

print(f"Original: {sentence}")
print(f"Tokens: {tokens}")
print(f"Stemmed: {stemmed}")

# Create stemmed sentence
stemmed_sentence = ' '.join(stemmed)
print(f"\nStemmed sentence: {stemmed_sentence}")
# Output: "the stripe cat are run and play happili in the garden"
# Note: Some stems aren't valid words ("happili", "stripe")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-code-branch me-2"></i>Stemmer Behavior Differences</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Comparison</span>
                                <span class="badge bg-crimson">Aggressiveness</span>
                            </div>
                            <div class="content">
                                <ul>
                                    <li><strong>Porter Stemmer:</strong> Conservative, widely used, good balance of precision and recall</li>
                                    <li><strong>Snowball (Porter2):</strong> Improved Porter, slightly better results, supports multiple languages</li>
                                    <li><strong>Lancaster:</strong> Most aggressive, faster but over-stems (merges unrelated words)</li>
                                </ul>
                                <p class="mb-0"><em>Rule of thumb: Start with Snowball. Use Porter for compatibility with older systems. Avoid Lancaster unless you need maximum vocabulary reduction.</em></p>
                            </div>
                        </div>

                        <h3 id="lemmatization">Lemmatization</h3>
                        
                        <p>Lemmatization reduces words to their base dictionary form (lemma) using vocabulary and morphological analysis. Unlike stemming, lemmatization always produces valid words: "better" ‚Üí "good", "running" ‚Üí "run", "studies" ‚Üí "study". This linguistic accuracy makes lemmatization preferable for tasks where meaning matters.</p>
                        
                        <p>Lemmatization typically requires knowledge of the word's part-of-speech (POS) to work correctly. For example, "meeting" as a noun stays "meeting", but as a verb becomes "meet". spaCy's lemmatizer is POS-aware by default, while NLTK's WordNet lemmatizer requires you to specify POS. The trade-off is that lemmatization is slower than stemming.</p>

<pre><code class="language-python"># Lemmatization with NLTK WordNet
import nltk
nltk.download('wordnet', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('averaged_perceptron_tagger_eng', quiet=True)
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

lemmatizer = WordNetLemmatizer()

# Basic lemmatization (defaults to noun)
words = ['cats', 'running', 'better', 'studies', 'geese', 'wolves']
print("Basic lemmatization (noun assumed):")
for word in words:
    lemma = lemmatizer.lemmatize(word)
    print(f"  {word} ‚Üí {lemma}")

# With POS tags (much better results)
print("\nWith POS tags:")
print(f"  running (verb) ‚Üí {lemmatizer.lemmatize('running', pos='v')}")
print(f"  running (noun) ‚Üí {lemmatizer.lemmatize('running', pos='n')}")
print(f"  better (adjective) ‚Üí {lemmatizer.lemmatize('better', pos='a')}")
print(f"  studies (verb) ‚Üí {lemmatizer.lemmatize('studies', pos='v')}")

# Helper function to convert NLTK POS tags to WordNet format
def get_wordnet_pos(nltk_tag):
    """Map NLTK POS tags to WordNet POS tags."""
    if nltk_tag.startswith('J'):
        return wordnet.ADJ
    elif nltk_tag.startswith('V'):
        return wordnet.VERB
    elif nltk_tag.startswith('N'):
        return wordnet.NOUN
    elif nltk_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # default
</code></pre>

<pre><code class="language-python"># spaCy Lemmatization (POS-aware by default)
import spacy

nlp = spacy.load("en_core_web_sm")

sentence = "The striped cats are running and playing happily in the gardens"
doc = nlp(sentence)

print("spaCy Lemmatization (with POS):")
print(f"{'Token':<12} {'Lemma':<12} {'POS':<8}")
print("-" * 32)
for token in doc:
    if not token.is_punct and not token.is_space:
        print(f"{token.text:<12} {token.lemma_:<12} {token.pos_:<8}")

# Get just lemmas
lemmas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]
print(f"\nLemmatized tokens: {lemmas}")
</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-balance-scale me-2"></i>Stemming vs Lemmatization</h4>
                            <table class="table table-sm table-bordered mt-3">
                                <thead class="table-dark">
                                    <tr>
                                        <th>Aspect</th>
                                        <th>Stemming</th>
                                        <th>Lemmatization</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Output</strong></td>
                                        <td>Root (may not be real word)</td>
                                        <td>Lemma (valid dictionary word)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Speed</strong></td>
                                        <td>Faster (rule-based)</td>
                                        <td>Slower (dictionary lookup)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Accuracy</strong></td>
                                        <td>Lower (over-stemming)</td>
                                        <td>Higher (linguistically correct)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Use Case</strong></td>
                                        <td>Search, IR, high-speed processing</td>
                                        <td>Text understanding, chatbots, Q&A</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <h2 id="special-cases"><i class="fas fa-exclamation-triangle me-2"></i>Special Cases & Edge Cases</h2>
                        
                        <p>Real-world text is messy. Beyond standard words, you'll encounter URLs, email addresses, phone numbers, emojis, hashtags, mentions, abbreviations, contractions, and domain-specific notation. Each requires thoughtful handling‚Äîsometimes you want to preserve them as-is, sometimes normalize them, and sometimes remove them entirely.</p>
                        
                        <p>The right approach depends on your task. For sentiment analysis, emojis carry important signal and should be preserved or converted to text. For information extraction, URLs and emails are valuable entities. For topic modeling, you might want to remove them to focus on content words. Always consider what information is relevant for your specific use case.</p>

<pre><code class="language-python"># Handling URLs, Emails, and Mentions
import re

text = """Contact us at support@example.com or visit https://www.example.com/page?id=123
@john mentioned #MachineLearning is üî•! Call us at (555) 123-4567."""

print("Original text:")
print(text)

# Pattern definitions
patterns = {
    'url': r'https?://[^\s]+',
    'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
    'mention': r'@\w+',
    'hashtag': r'#\w+',
    'phone': r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}',
}

# Extract each type
print("\nExtracted entities:")
for entity_type, pattern in patterns.items():
    matches = re.findall(pattern, text, re.IGNORECASE)
    print(f"  {entity_type}: {matches}")

# Replace with placeholders (useful for models)
def normalize_entities(text):
    """Replace entities with standardized tokens."""
    text = re.sub(r'https?://[^\s]+', '[URL]', text)
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL]', text, flags=re.IGNORECASE)
    text = re.sub(r'@\w+', '[MENTION]', text)
    text = re.sub(r'#(\w+)', r'[HASHTAG_\1]', text)  # Preserve hashtag content
    text = re.sub(r'\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}', '[PHONE]', text)
    return text

normalized = normalize_entities(text)
print(f"\nNormalized:\n{normalized}")
</code></pre>

<pre><code class="language-python"># Handling Emojis and Special Unicode
import re

text = "I love this! üòä‚ù§Ô∏èüî• It's amazing üëç #blessed"

# Option 1: Remove all emojis
def remove_emojis(text):
    """Remove emoji characters."""
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map
        u"\U0001F1E0-\U0001F1FF"  # flags
        u"\U00002702-\U000027B0"  # dingbats
        u"\U0001F900-\U0001F9FF"  # supplemental symbols
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub('', text)

print(f"Original: {text}")
print(f"Emojis removed: {remove_emojis(text)}")

# Option 2: Convert emojis to text descriptions
try:
    import emoji
    demojized = emoji.demojize(text, delimiters=(" [", "] "))
    print(f"Emojis as text: {demojized}")
except ImportError:
    print("Install emoji package: pip install emoji")

# Option 3: Replace with placeholder
def replace_emojis(text):
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F900-\U0001F9FF"
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub('[EMOJI]', text)

print(f"Emojis as tokens: {replace_emojis(text)}")
</code></pre>

<pre><code class="language-python"># Handling Contractions and Abbreviations
import re

# Contraction expansion dictionary
CONTRACTIONS = {
    "won't": "will not", "can't": "cannot", "couldn't": "could not",
    "shouldn't": "should not", "wouldn't": "would not", "didn't": "did not",
    "doesn't": "does not", "don't": "do not", "hasn't": "has not",
    "haven't": "have not", "hadn't": "had not", "isn't": "is not",
    "aren't": "are not", "wasn't": "was not", "weren't": "were not",
    "i'm": "i am", "you're": "you are", "he's": "he is", "she's": "she is",
    "it's": "it is", "we're": "we are", "they're": "they are",
    "i've": "i have", "you've": "you have", "we've": "we have",
    "they've": "they have", "i'll": "i will", "you'll": "you will",
    "he'll": "he will", "she'll": "she will", "we'll": "we will",
    "they'll": "they will", "i'd": "i would", "you'd": "you would",
    "he'd": "he would", "she'd": "she would", "we'd": "we would",
    "let's": "let us", "that's": "that is", "who's": "who is",
    "what's": "what is", "here's": "here is", "there's": "there is",
}

def expand_contractions(text):
    """Expand contractions in text."""
    text_lower = text.lower()
    for contraction, expansion in CONTRACTIONS.items():
        text_lower = text_lower.replace(contraction, expansion)
    return text_lower

text = "I won't say I can't do it. That's what they'd expect!"
expanded = expand_contractions(text)
print(f"Original: {text}")
print(f"Expanded: {expanded}")

# Common abbreviations
ABBREVIATIONS = {
    "mr.": "mister", "mrs.": "missus", "dr.": "doctor",
    "jr.": "junior", "sr.": "senior", "vs.": "versus",
    "etc.": "et cetera", "e.g.": "for example", "i.e.": "that is",
    "govt.": "government", "dept.": "department"
}

def expand_abbreviations(text):
    """Expand common abbreviations."""
    text_lower = text.lower()
    for abbr, full in ABBREVIATIONS.items():
        text_lower = text_lower.replace(abbr, full)
    return text_lower

abbr_text = "Dr. Smith vs. Mr. Jones, e.g., in the govt. dept."
print(f"\nAbbreviations: {abbr_text}")
print(f"Expanded: {expand_abbreviations(abbr_text)}")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-globe me-2"></i>Handling Multi-lingual Text</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Unicode</span>
                                <span class="badge bg-crimson">Languages</span>
                            </div>
                            <div class="content">
<pre><code class="language-python"># Multi-lingual Text Handling
import unicodedata

# Mixed language text
text = "Hello ‰Ω†Â•Ω ŸÖÿ±ÿ≠ÿ®ÿß –ü—Ä–∏–≤–µ—Ç „Åì„Çì„Å´„Å°„ÅØ üåç"

print(f"Original: {text}")

# Detect script/language of each character
def analyze_characters(text):
    """Analyze Unicode categories of characters."""
    for char in text:
        if char.strip():
            name = unicodedata.name(char, 'UNKNOWN')
            category = unicodedata.category(char)
            print(f"  '{char}' - {name[:30]:<30} ({category})")

print("\nCharacter analysis:")
analyze_characters(text[:10])  # First 10 chars

# Filter to keep only certain scripts
def keep_latin(text):
    """Keep only Latin characters."""
    return ''.join(c for c in text if unicodedata.category(c).startswith(('L',)) 
                   and ord(c) < 128 or c.isspace())

print(f"\nLatin only: {keep_latin(text)}")
</code></pre>
                            </div>
                        </div>

                        <h2 id="pipelines"><i class="fas fa-stream me-2"></i>Building Preprocessing Pipelines</h2>
                        
                        <p>A preprocessing pipeline chains together multiple text processing steps in a specific order. The order matters‚Äîyou typically want to normalize text before tokenization, and remove stop words after tokenization. A well-designed pipeline is modular (easy to add/remove steps), configurable (adjustable parameters), and reproducible (same input always gives same output).</p>
                        
                        <p>For production systems, consider using established pipelines from spaCy or Hugging Face rather than building from scratch. These handle edge cases and are optimized for performance. However, understanding how to build custom pipelines gives you flexibility for domain-specific requirements and helps you debug when things go wrong.</p>

<pre><code class="language-python"># Building a Modular Text Preprocessing Pipeline
import re
import string
import nltk
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from typing import List, Callable

class TextPreprocessor:
    """Modular text preprocessing pipeline."""
    
    def __init__(self):
        self.steps: List[Callable] = []
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
    
    def add_step(self, func: Callable) -> 'TextPreprocessor':
        """Add a preprocessing step."""
        self.steps.append(func)
        return self  # Enable chaining
    
    def process(self, text: str) -> str:
        """Apply all preprocessing steps."""
        for step in self.steps:
            text = step(text)
        return text
    
    # Built-in preprocessing functions
    @staticmethod
    def lowercase(text: str) -> str:
        return text.lower()
    
    @staticmethod
    def remove_urls(text: str) -> str:
        return re.sub(r'https?://\S+|www\.\S+', '', text)
    
    @staticmethod
    def remove_html(text: str) -> str:
        return re.sub(r'<[^>]+>', '', text)
    
    @staticmethod
    def remove_punctuation(text: str) -> str:
        return text.translate(str.maketrans('', '', string.punctuation))
    
    @staticmethod
    def remove_numbers(text: str) -> str:
        return re.sub(r'\d+', '', text)
    
    @staticmethod
    def normalize_whitespace(text: str) -> str:
        return ' '.join(text.split())
    
    def remove_stopwords(self, text: str) -> str:
        tokens = word_tokenize(text)
        return ' '.join([t for t in tokens if t not in self.stop_words])
    
    def lemmatize(self, text: str) -> str:
        tokens = word_tokenize(text)
        return ' '.join([self.lemmatizer.lemmatize(t) for t in tokens])

# Create and configure pipeline
pipeline = TextPreprocessor()
pipeline.add_step(TextPreprocessor.remove_urls)
pipeline.add_step(TextPreprocessor.remove_html)
pipeline.add_step(TextPreprocessor.lowercase)
pipeline.add_step(TextPreprocessor.remove_punctuation)
pipeline.add_step(TextPreprocessor.remove_numbers)
pipeline.add_step(pipeline.remove_stopwords)
pipeline.add_step(pipeline.lemmatize)
pipeline.add_step(TextPreprocessor.normalize_whitespace)

# Test the pipeline
raw_text = """<p>Check out https://example.com for 100+ ML tutorials! 
It's AMAZING - the best resources I've found in 2024.</p>"""

print("Raw text:")
print(raw_text)
print("\nProcessed:")
print(pipeline.process(raw_text))
</code></pre>

<pre><code class="language-python"># Production-Ready Pipeline with spaCy
import spacy
from spacy.tokens import Doc

# Load model
nlp = spacy.load("en_core_web_sm")

# Custom pipeline component
@spacy.Language.component("custom_preprocessor")
def custom_preprocessor(doc: Doc) -> Doc:
    """Custom spaCy pipeline component."""
    # Access doc attributes and modify as needed
    # Note: spaCy docs are immutable, so we typically filter during analysis
    return doc

# Add custom component to pipeline
if "custom_preprocessor" not in nlp.pipe_names:
    nlp.add_pipe("custom_preprocessor", last=True)

def preprocess_with_spacy(text: str, 
                          remove_stops: bool = True,
                          lemmatize: bool = True,
                          remove_punct: bool = True) -> str:
    """Preprocess text using spaCy."""
    doc = nlp(text)
    
    tokens = []
    for token in doc:
        # Skip based on settings
        if remove_punct and token.is_punct:
            continue
        if remove_stops and token.is_stop:
            continue
        if token.is_space:
            continue
            
        # Get lemma or original text
        if lemmatize:
            tokens.append(token.lemma_.lower())
        else:
            tokens.append(token.text.lower())
    
    return ' '.join(tokens)

# Test
text = "The striped cats are quickly running through beautiful gardens!"
print(f"Original: {text}")
print(f"Processed: {preprocess_with_spacy(text)}")
print(f"Keep stops: {preprocess_with_spacy(text, remove_stops=False)}")
</code></pre>

<pre><code class="language-python"># Complete Pipeline for Different Use Cases
from dataclasses import dataclass
from enum import Enum
from typing import Optional
import re

class TaskType(Enum):
    CLASSIFICATION = "classification"
    SENTIMENT = "sentiment"
    NER = "ner"
    SEARCH = "search"

@dataclass
class PipelineConfig:
    """Configuration for preprocessing pipeline."""
    lowercase: bool = True
    remove_urls: bool = True
    remove_emails: bool = True
    remove_numbers: bool = False
    remove_punctuation: bool = True
    remove_stopwords: bool = True
    lemmatize: bool = True
    min_token_length: int = 2
    
    @classmethod
    def for_task(cls, task: TaskType) -> 'PipelineConfig':
        """Get recommended config for specific task."""
        configs = {
            TaskType.CLASSIFICATION: cls(remove_stopwords=True, lemmatize=True),
            TaskType.SENTIMENT: cls(remove_stopwords=False, lemmatize=False),  # Keep "not" etc.
            TaskType.NER: cls(lowercase=False, remove_punctuation=False),  # Preserve case
            TaskType.SEARCH: cls(remove_stopwords=True, lemmatize=True),
        }
        return configs.get(task, cls())

def create_pipeline(config: PipelineConfig):
    """Create preprocessing function from config."""
    def preprocess(text: str) -> str:
        if config.remove_urls:
            text = re.sub(r'https?://\S+', '', text)
        if config.remove_emails:
            text = re.sub(r'\S+@\S+\.\S+', '', text)
        if config.lowercase:
            text = text.lower()
        if config.remove_punctuation:
            text = re.sub(r'[^\w\s]', '', text)
        if config.remove_numbers:
            text = re.sub(r'\d+', '', text)
        # ... add more steps
        return ' '.join(text.split())
    return preprocess

# Usage
print("Task-specific configurations:")
for task in TaskType:
    config = PipelineConfig.for_task(task)
    print(f"\n{task.value}:")
    print(f"  lowercase={config.lowercase}, remove_stops={config.remove_stopwords}")
</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-check-circle me-2"></i>Pipeline Best Practices</h4>
                            <ul>
                                <li><strong>Order matters:</strong> URL removal ‚Üí lowercase ‚Üí tokenize ‚Üí stop words ‚Üí lemmatize</li>
                                <li><strong>Be reproducible:</strong> Save your pipeline config, version your preprocessing code</li>
                                <li><strong>Task-specific:</strong> Different NLP tasks need different preprocessing‚Äîdon't use one-size-fits-all</li>
                                <li><strong>Test thoroughly:</strong> Check edge cases (empty strings, special characters, very long texts)</li>
                                <li><strong>Document decisions:</strong> Explain why you chose each preprocessing step</li>
                            </ul>
                        </div>

                        <h2 id="conclusion"><i class="fas fa-flag-checkered me-2"></i>Conclusion & Next Steps</h2>
                        
                        <p>Tokenization and text preprocessing are foundational skills that every NLP practitioner must master. We've covered the full spectrum from simple whitespace splitting to sophisticated subword algorithms like BPE and WordPiece that power modern language models. Understanding these techniques helps you make informed decisions about your text processing pipeline.</p>
                        
                        <p>Key takeaways from this guide:</p>
                        <ul>
                            <li><strong>Word tokenization</strong> (NLTK, spaCy) works well for traditional ML but creates vocabulary issues</li>
                            <li><strong>Subword tokenization</strong> (BPE, WordPiece, SentencePiece) is standard for neural models‚Äîhandles any text including rare words</li>
                            <li><strong>Text preprocessing</strong> (lowercasing, stop words, stemming, lemmatization) must be task-appropriate‚Äîaggressive cleaning isn't always better</li>
                            <li><strong>Special cases</strong> (URLs, emojis, contractions) require thoughtful handling based on your use case</li>
                            <li><strong>Pipelines</strong> should be modular, reproducible, and well-documented</li>
                        </ul>
                        
                        <p>Modern deep learning models like BERT and GPT use their own tokenizers and often perform well with minimal preprocessing. However, understanding traditional preprocessing remains valuable for debugging, working with smaller models, and domain-specific applications.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-tasks me-2"></i>Practical Exercises</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Hands-On</span>
                                <span class="badge bg-crimson">Practice</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><strong>Compare tokenizers:</strong> Tokenize the same paragraph with whitespace, NLTK, spaCy, and GPT-2 BPE. Analyze the differences.</li>
                                    <li><strong>Build a Twitter preprocessor:</strong> Create a pipeline that handles mentions, hashtags, URLs, emojis, and common slang.</li>
                                    <li><strong>Train custom BPE:</strong> Use the Hugging Face tokenizers library to train a BPE tokenizer on a domain-specific corpus (e.g., medical or legal text).</li>
                                    <li><strong>Benchmark preprocessing:</strong> Compare model accuracy with different preprocessing configurations on a sentiment analysis dataset.</li>
                                    <li><strong>Handle edge cases:</strong> Create test cases for your pipeline including empty strings, Unicode, mixed languages, and very long texts.</li>
                                </ol>
                            </div>
                        </div>

                        <div class="highlight-box">
                            <h4><i class="fas fa-arrow-right me-2"></i>Next in the Series</h4>
                            <p>In <a href="nlp-text-representation-features.html"><strong>Part 3: Text Representation & Feature Engineering</strong></a>, we'll learn how to convert tokenized text into numerical representations that machine learning models can process. We'll cover Bag of Words, TF-IDF, N-grams, and set the stage for understanding word embeddings.</p>
                        </div>

                        <!-- Related Posts -->
                        <div class="related-posts">
                            <h3><i class="fas fa-book-reader me-2"></i>Continue the NLP Series</h3>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 1: NLP Fundamentals & Linguistic Basics</h5>
                                <p class="text-muted small mb-2">Understand what language is and how machines process it.</p>
                                <a href="nlp-fundamentals-linguistic-basics.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 3: Text Representation & Feature Engineering</h5>
                                <p class="text-muted small mb-2">Turn text into numbers with Bag of Words, TF-IDF, and N-grams.</p>
                                <a href="nlp-text-representation-features.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 4: Word Embeddings</h5>
                                <p class="text-muted small mb-2">Capture meaning and similarity with Word2Vec, GloVe, and FastText.</p>
                                <a href="nlp-word-embeddings.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.</p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook"><i class="fab fa-facebook-f"></i></a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter"><i class="fab fa-twitter"></i></a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn"><i class="fab fa-linkedin-in"></i></a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube"><i class="fab fa-youtube"></i></a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram"><i class="fab fa-instagram"></i></a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest"><i class="fab fa-pinterest-p"></i></a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email"><i class="fas fa-envelope"></i></a>
                    </div>
                </div>
            </div>
            <hr class="bg-secondary">
            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small"><i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a></p>
                    <p class="small mt-3"><a href="/" class="text-light text-decoration-none">Home</a> | <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a></p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">Enjoying this content? ‚òï <a href="https://buymeacoffee.com/itswzee" target="_blank" class="text-light" style="text-decoration: underline;">Keep me caffeinated</a> to keep the pixels flowing!</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top"><i class="fas fa-arrow-up"></i></button>
    <script src="../../../js/cookie-consent.js"></script>
    <script src="../../../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <script>
        const themes = { 'prism-theme': 'Tomorrow Night', 'prism-default': 'Default', 'prism-dark': 'Dark', 'prism-twilight': 'Twilight', 'prism-okaidia': 'Okaidia', 'prism-solarizedlight': 'Solarized Light' };
        const savedTheme = localStorage.getItem('prism-theme') || 'prism-theme';
        function switchTheme(themeId) { Object.keys(themes).forEach(id => { const link = document.getElementById(id); if (link) link.disabled = true; }); const selectedLink = document.getElementById(themeId); if (selectedLink) { selectedLink.disabled = false; localStorage.setItem('prism-theme', themeId); } document.querySelectorAll('div.code-toolbar select').forEach(dropdown => { dropdown.value = themeId; }); setTimeout(() => Prism.highlightAll(), 10); }
        document.addEventListener('DOMContentLoaded', function() { switchTheme(savedTheme); });
        Prism.plugins.toolbar.registerButton('theme-switcher', function(env) { const select = document.createElement('select'); select.setAttribute('aria-label', 'Select code theme'); Object.keys(themes).forEach(themeId => { const option = document.createElement('option'); option.value = themeId; option.textContent = themes[themeId]; if (themeId === savedTheme) option.selected = true; select.appendChild(option); }); select.addEventListener('change', function(e) { switchTheme(e.target.value); }); return select; });
    </script>

    <script>
        document.addEventListener('DOMContentLoaded', function() { const scrollToTopBtn = document.getElementById('scrollToTop'); window.addEventListener('scroll', function() { if (window.scrollY > 300) { scrollToTopBtn.classList.add('show'); } else { scrollToTopBtn.classList.remove('show'); } }); scrollToTopBtn.addEventListener('click', function() { window.scrollTo({ top: 0, behavior: 'smooth' }); }); });
        function openNav() { document.getElementById('tocSidenav').classList.add('open'); document.getElementById('tocOverlay').classList.add('show'); document.body.style.overflow = 'hidden'; }
        function closeNav() { document.getElementById('tocSidenav').classList.remove('open'); document.getElementById('tocOverlay').classList.remove('show'); document.body.style.overflow = 'auto'; }
        document.addEventListener('keydown', function(e) { if (e.key === 'Escape') closeNav(); });
    </script>
</body>
</html>
