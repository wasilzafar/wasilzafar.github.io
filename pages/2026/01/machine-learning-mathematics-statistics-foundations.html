<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Understand the mathematical and statistical foundations behind key machine learning techniques. Learn Linear Regression, Logistic Regression, SVM, PCA, k-Means, and Neural Networks with beginner-friendly explanations." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="Machine Learning, Mathematics, Statistics, Linear Regression, Logistic Regression, SVM, PCA, k-Means, Neural Networks, MLE, Optimization" />
    <meta property="og:title" content="Machine Learning Foundations: Mathematics & Statistics Explained for Beginners" />
    <meta property="og:description" content="Understand the mathematical and statistical foundations behind key machine learning techniques with beginner-friendly explanations." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-15" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>Machine Learning Foundations: Mathematics & Statistics Explained - Wasil Zafar</title>

    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />

    <!-- Custom Styles -->
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />

    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <!-- Google Consent Mode v2 -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        
        gtag('consent', 'default', {
            'ad_storage': 'denied',
            'ad_user_data': 'denied',
            'ad_personalization': 'denied',
            'analytics_storage': 'denied',
            'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE']
        });
        
        gtag('consent', 'default', {
            'ad_storage': 'granted',
            'ad_user_data': 'granted',
            'ad_personalization': 'granted',
            'analytics_storage': 'granted'
        });
        
        gtag('set', 'url_passthrough', true);
    </script>

    <!-- Google Tag Manager -->
    <script>
        (function(w, d, s, l, i) {
            w[l] = w[l] || [];
            w[l].push({
                'gtm.start': new Date().getTime(),
                event: 'gtm.js'
            });
            var f = d.getElementsByTagName(s)[0],
                j = d.createElement(s),
                dl = l != 'dataLayer' ? '&l=' + l : '';
            j.async = true;
            j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    <style>
        /* Blog Post Specific Styles */
        .blog-hero {
            background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%);
            color: white;
            padding: 80px 0;
        }

        .blog-header {
            margin-bottom: 2rem;
        }

        .blog-meta {
            font-size: 0.95rem;
            color: var(--color-teal);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }

        .blog-meta span {
            margin-right: 0.5rem;
        }

        .print-btn {
            background: var(--color-teal);
            color: white;
            border: none;
            padding: 0.4rem 1rem;
            border-radius: 4px;
            font-size: 0.9rem;
            cursor: pointer;
            transition: all 0.3s ease;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
        }

        .print-btn:hover {
            background: var(--color-crimson);
            transform: translateY(-1px);
        }

        @media print {
            /* Hide print button and navigation */
            .print-btn,
            nav,
            footer,
            .back-link,
            .related-posts,
            .scroll-to-top { display: none; }
            
            /* Force color printing */
            * {
                -webkit-print-color-adjust: exact !important;
                print-color-adjust: exact !important;
                color-adjust: exact !important;
            }
            
            /* Preserve header colors */
            .blog-content h2 {
                color: var(--color-navy) !important;
                border-bottom: 3px solid var(--color-teal) !important;
                page-break-after: avoid;
            }
            
            .blog-content h3 {
                color: var(--color-blue) !important;
                page-break-after: avoid;
            }
            
            .blog-content h4 {
                color: var(--color-crimson) !important;
                page-break-after: avoid;
            }
            
            /* Preserve strong text color */
            .blog-content strong {
                color: var(--color-crimson) !important;
            }
            
            /* Preserve highlight boxes */
            .highlight-box {
                background: rgba(59, 151, 151, 0.1) !important;
                border-left: 4px solid var(--color-teal) !important;
                page-break-inside: avoid;
            }
            
            /* Preserve experiment cards */
            .experiment-card {
                border: 1px solid #ddd !important;
                page-break-inside: avoid;
            }
            
            .experiment-card h4 {
                color: var(--color-crimson) !important;
            }
            
            /* Preserve badges */
            .badge {
                -webkit-print-color-adjust: exact !important;
                print-color-adjust: exact !important;
            }
            
            .bg-teal {
                background-color: var(--color-teal) !important;
                color: white !important;
            }
            
            .bg-crimson {
                background-color: var(--color-crimson) !important;
                color: white !important;
            }
            
            /* Preserve TOC box */
            .toc-box {
                border: 2px solid var(--color-teal) !important;
                page-break-inside: avoid;
            }
            
            .toc-box h3 {
                color: var(--color-navy) !important;
            }
            
            .toc-box a {
                color: var(--color-blue) !important;
            }
            
            /* Code blocks */
            pre[class*="language-"] {
                page-break-inside: avoid;
                border: 1px solid #ddd !important;
            }
            
            /* Reading time badge */
            .reading-time {
                background: var(--color-crimson) !important;
                color: white !important;
            }
            
            /* Page breaks */
            .blog-content h2 {
                page-break-before: auto;
            }
            
            /* Ensure good spacing */
            body {
                font-size: 12pt;
                line-height: 1.6;
            }
        }

        .blog-content {
            max-width: 900px;
            margin: 0 auto;
            font-size: 1.05rem;
            line-height: 1.8;
            color: #333;
        }

        .blog-content h2 {
            font-size: 1.8rem;
            font-weight: 700;
            margin-top: 2.5rem;
            margin-bottom: 1.5rem;
            color: var(--color-navy);
            border-bottom: 3px solid var(--color-teal);
            padding-bottom: 0.5rem;
        }

        .blog-content h3 {
            font-size: 1.3rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: var(--color-blue);
        }

        .blog-content p {
            margin-bottom: 1.2rem;
            text-align: justify;
        }

        .blog-content strong {
            color: var(--color-crimson);
        }

        .highlight-box {
            background: rgba(59, 151, 151, 0.1);
            border-left: 4px solid var(--color-teal);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 4px;
        }

        .technique-card {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: all 0.3s ease;
        }

        .technique-card:hover {
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
            transform: translateY(-2px);
        }

        .technique-card h4 {
            color: var(--color-crimson);
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        .toc-box {
            background: #f8f9fa;
            border: 2px solid var(--color-teal);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 2rem 0;
        }

        .toc-box h4 {
            color: var(--color-navy);
            font-weight: 700;
            margin-bottom: 1rem;
        }

        .toc-box ul {
            margin-bottom: 0;
            padding-left: 1.5rem;
        }

        .toc-box li {
            margin-bottom: 0.5rem;
        }

        .toc-box a {
            color: var(--color-blue);
            text-decoration: none;
            transition: color 0.2s ease;
        }

        .toc-box a:hover {
            color: var(--color-crimson);
            text-decoration: underline;
        }

        .reading-time {
            display: inline-block;
            background: var(--color-crimson);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 4px;
            font-size: 0.9rem;
            margin-left: 0.5rem;
        }

        .back-link {
            display: inline-block;
            color: white;
            text-decoration: none;
            transition: all 0.3s ease;
            margin-bottom: 1rem;
            opacity: 0.9;
        }

        .back-link:hover {
            color: var(--color-teal);
            opacity: 1;
            transform: translateX(-5px);
        }

        .related-posts {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 2rem;
            margin-top: 3rem;
        }

        .related-posts h3 {
            color: var(--color-navy);
            margin-bottom: 1.5rem;
        }

        .related-post-item {
            padding: 1rem;
            border-left: 3px solid var(--color-teal);
            margin-bottom: 1rem;
            transition: all 0.3s ease;
        }

        .related-post-item:hover {
            background: white;
            border-left-color: var(--color-crimson);
        }

        .related-post-item a {
            color: var(--color-blue);
            text-decoration: none;
            font-weight: 600;
        }

        .related-post-item a:hover {
            color: var(--color-crimson);
        }

        /* Summary Table Styles */
        .summary-table {
            width: 100%;
            margin: 2rem 0;
            border-collapse: collapse;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        .summary-table thead {
            background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%);
            color: white;
        }

        .summary-table th {
            padding: 1rem;
            text-align: left;
            font-weight: 600;
            border: 1px solid #ddd;
        }

        .summary-table td {
            padding: 1rem;
            border: 1px solid #ddd;
            background: white;
        }

        .summary-table tbody tr:hover {
            background: rgba(59, 151, 151, 0.05);
        }

        .summary-table tbody tr:nth-child(even) {
            background: #f8f9fa;
        }

        .summary-table tbody tr:nth-child(even):hover {
            background: rgba(59, 151, 151, 0.08);
        }

        /* Scroll-to-Top Button */
        .scroll-to-top {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            background: var(--color-teal);
            color: white;
            border: none;
            border-radius: 50%;
            font-size: 1.2rem;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            box-shadow: 0 4px 12px rgba(59, 151, 151, 0.3);
            z-index: 999;
        }

        .scroll-to-top.show {
            opacity: 1;
            visibility: visible;
        }

        .scroll-to-top:hover {
            background: var(--color-crimson);
            transform: translateY(-3px);
            box-shadow: 0 6px 16px rgba(191, 9, 47, 0.4);
        }

        .scroll-to-top:active {
            transform: translateY(-1px);
        }

        @media (max-width: 768px) {
            .scroll-to-top {
                bottom: 1rem;
                right: 1rem;
                width: 45px;
                height: 45px;
                font-size: 1rem;
            }
        }
    </style>
</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript>
        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe>
    </noscript>

    <!-- GDPR Cookie Consent Banner -->
    <div id="cookieBanner" class="light display-bottom" style="display: none;">
        <div id="closeIcon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
                <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"></path>
            </svg>
        </div>
        
        <div class="content-wrap">
            <div class="msg-wrap">
                <div class="title-wrap">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="20" height="20">
                        <path fill="#3B9797" d="M510.52 255.82c-69.97-.85-126.47-57.69-126.47-127.86-70.17 0-127-56.49-127.86-126.45-27.26-4.14-55.13.3-79.72 12.82l-69.13 35.22a132.221 132.221 0 0 0-57.79 57.81l-35.1 68.88a132.645 132.645 0 0 0-12.82 80.95l12.08 76.27a132.521 132.521 0 0 0 37.16 70.37l54.64 54.64a132.036 132.036 0 0 0 70.37 37.16l76.27 12.15c27.51 4.36 55.7-.11 80.95-12.8l68.88-35.08a132.166 132.166 0 0 0 57.79-57.81l35.1-68.88c12.56-24.64 17.01-52.58 12.91-79.91zM176 368c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32zm32-160c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32zm160 128c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32z"></path>
                    </svg>
                    <h4 style="margin: 0; font-size: 18px; color: var(--color-navy); font-weight: 700;">Cookie Consent</h4>
                </div>
                <p style="font-size: 14px; line-height: 1.6; color: var(--color-navy); margin-bottom: 15px;">
                    We use cookies to enhance your browsing experience, serve personalized content, and analyze our traffic. 
                    By clicking "Accept All", you consent to our use of cookies. See our 
                    <a href="/privacy-policy.html" style="color: var(--color-teal); border-bottom: 1px dotted var(--color-teal);">Privacy Policy</a> 
                    for more information.
                </p>
                
                <div id="cookieSettings" style="display: none;">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="14" height="14">
                        <path fill="currentColor" d="M487.4 315.7l-42.6-24.6c4.3-23.2 4.3-47 0-70.2l42.6-24.6c4.9-2.8 7.1-8.6 5.5-14-11.1-35.6-30-67.8-54.7-94.6-3.8-4.1-10-5.1-14.8-2.3L380.8 110c-17.9-15.4-38.5-27.3-60.8-35.1V25.8c0-5.6-3.9-10.5-9.4-11.7-36.7-8.2-74.3-7.8-109.2 0-5.5 1.2-9.4 6.1-9.4 11.7V75c-22.2 7.9-42.8 19.8-60.8 35.1L88.7 85.5c-4.9-2.8-11-1.9-14.8 2.3-24.7 26.7-43.6 58.9-54.7 94.6-1.7 5.4.6 11.2 5.5 14L67.3 221c-4.3 23.2-4.3 47 0 70.2l-42.6 24.6c-4.9 2.8-7.1 8.6-5.5 14 11.1 35.6 30 67.8 54.7 94.6 3.8 4.1 10 5.1 14.8 2.3l42.6-24.6c17.9 15.4 38.5 27.3 60.8 35.1v49.2c0 5.6 3.9 10.5 9.4 11.7 36.7 8.2 74.3 7.8 109.2 0 5.5-1.2 9.4-6.1 9.4-11.7v-49.2c22.2-7.9 42.8-19.8 60.8-35.1l42.6 24.6c4.9 2.8 11 1.9 14.8-2.3 24.7-26.7 43.6-58.9 54.7-94.6 1.5-5.5-.7-11.3-5.6-14.1zM256 336c-44.1 0-80-35.9-80-80s35.9-80 80-80 80 35.9 80 80-35.9 80-80 80z"></path>
                    </svg>
                    <span style="margin-left: 5px; font-size: 12px; font-weight: 600; color: var(--color-navy);">Customize Settings</span>
                </div>
                
                <div id="cookieTypes" style="display: none; margin-top: 15px; padding-top: 15px; border-top: 1px solid rgba(59, 151, 151, 0.2);">
                    <h5 style="font-size: 12px; font-weight: 700; color: var(--color-navy); margin-bottom: 10px; text-transform: uppercase;">Cookie Preferences</h5>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" checked disabled style="margin-top: 2px; margin-right: 8px; cursor: not-allowed;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Essential Cookies (Required)</strong>
                                <span style="font-size: 12px; color: #666;">Necessary for the website to function properly.</span>
                            </div>
                        </label>
                    </div>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" id="analyticsCookies" checked style="margin-top: 2px; margin-right: 8px;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Analytics Cookies</strong>
                                <span style="font-size: 12px; color: #666;">Help us understand how you interact with the website.</span>
                            </div>
                        </label>
                    </div>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" id="marketingCookies" style="margin-top: 2px; margin-right: 8px;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Marketing Cookies</strong>
                                <span style="font-size: 12px; color: #666;">Used to deliver relevant advertisements.</span>
                            </div>
                        </label>
                    </div>
                </div>
            </div>
            
            <div class="btn-wrap">
                <button id="cookieAccept" style="background: var(--color-teal); color: white; font-weight: 600;">Accept All</button>
                <button id="cookieReject" style="background: transparent; color: var(--color-navy); border: 2px solid var(--color-teal); font-weight: 600;">Reject All</button>
                <button id="cookieSave" style="background: var(--color-blue); color: white; font-weight: 600; display: none;">Save Preferences</button>
            </div>
        </div>
    </div>

    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/">
                <span class="gradient-text">Wasil Zafar</span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="/">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#about">About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#skills">Skills</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#certifications">Certifications</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#interests">Interests</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Blog Hero Section -->
    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link">
                    <i class="fas fa-arrow-left me-2"></i>Back to Technology
                </a>
                <h1 class="display-4 fw-bold mb-3">
                    Machine Learning Foundations: Mathematics & Statistics Explained for Beginners
                </h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 15, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-1"></i>45 min read</span>
                    <button onclick="window.print()" class="print-btn" title="Print this article">
                        <i class="fas fa-print"></i> Print
                    </button>
                </div>
                <p class="lead">Demystify machine learning by understanding the mathematical and statistical principles that power the algorithms. A comprehensive beginner-friendly guide covering 35+ techniques from classical ML to cutting-edge AI systems including Transformers, Diffusion Models, RLHF, and Agentic AI.</p>
            </div>
        </div>
    </section>

    <!-- Main Content -->
    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-10 mx-auto">
                    <div class="blog-content">
                        <!-- Table of Contents -->
                        <div class="toc-box">
                            <h4><i class="fas fa-list me-2"></i>Table of Contents</h4>
                            <ul>
                                <li><a href="#introduction">Introduction</a></li>
                                <li><a href="#summary-table">Quick Reference: ML Techniques Summary</a></li>
                                <li><strong>Classical Machine Learning</strong>
                                    <ul>
                                        <li><a href="#linear-regression">Linear Regression</a></li>
                                        <li><a href="#logistic-regression">Logistic Regression</a></li>
                                        <li><a href="#naive-bayes">Naive Bayes</a></li>
                                        <li><a href="#knn">k-Nearest Neighbors (k-NN)</a></li>
                                        <li><a href="#svm">Support Vector Machines (SVM)</a></li>
                                        <li><a href="#decision-trees">Decision Trees</a></li>
                                        <li><a href="#random-forest">Random Forest</a></li>
                                        <li><a href="#gradient-boosting">Gradient Boosting</a></li>
                                    </ul>
                                </li>
                                <li><strong>Unsupervised Learning</strong>
                                    <ul>
                                        <li><a href="#pca">Principal Component Analysis (PCA)</a></li>
                                        <li><a href="#kmeans">k-Means Clustering</a></li>
                                        <li><a href="#gmm">Gaussian Mixture Models (GMM)</a></li>
                                        <li><a href="#hmm">Hidden Markov Models (HMM)</a></li>
                                    </ul>
                                </li>
                                <li><strong>Deep Learning Foundations</strong>
                                    <ul>
                                        <li><a href="#neural-nets">Neural Networks (MLP)</a></li>
                                        <li><a href="#cnns">Convolutional Neural Networks (CNNs)</a></li>
                                        <li><a href="#rnns">Recurrent Neural Networks (RNN/LSTM)</a></li>
                                    </ul>
                                </li>
                                <li><strong>Modern Architectures</strong>
                                    <ul>
                                        <li><a href="#transformers">Transformers</a></li>
                                        <li><a href="#embeddings">Embedding Models</a></li>
                                        <li><a href="#self-supervised">Self-Supervised Learning</a></li>
                                        <li><a href="#autoregressive">Autoregressive Models</a></li>
                                        <li><a href="#diffusion">Diffusion Models</a></li>
                                        <li><a href="#flow-based">Flow-Based Models</a></li>
                                    </ul>
                                </li>
                                <li><strong>Reinforcement Learning</strong>
                                    <ul>
                                        <li><a href="#rl">Reinforcement Learning</a></li>
                                        <li><a href="#deep-rl">Deep Reinforcement Learning</a></li>
                                        <li><a href="#rlhf">RLHF (Reinforcement Learning from Human Feedback)</a></li>
                                        <li><a href="#model-based-rl">Model-Based RL & Planning</a></li>
                                        <li><a href="#mcts">Monte Carlo Tree Search</a></li>
                                    </ul>
                                </li>
                                <li><strong>Advanced AI Systems</strong>
                                    <ul>
                                        <li><a href="#gnns">Graph Neural Networks</a></li>
                                        <li><a href="#neuro-symbolic">Neuro-Symbolic AI</a></li>
                                        <li><a href="#memory-networks">Memory-Augmented Networks</a></li>
                                        <li><a href="#rag">Retrieval-Augmented Generation (RAG)</a></li>
                                        <li><a href="#multi-agent">Multi-Agent Learning</a></li>
                                    </ul>
                                </li>
                                <li><strong>Learning Paradigms</strong>
                                    <ul>
                                        <li><a href="#meta-learning">Meta-Learning</a></li>
                                        <li><a href="#in-context">In-Context Learning</a></li>
                                        <li><a href="#continual">Continual Learning</a></li>
                                        <li><a href="#causal">Causal Machine Learning</a></li>
                                        <li><a href="#adversarial">Adversarial Training</a></li>
                                    </ul>
                                </li>
                                <li><a href="#conclusion">Conclusion</a></li>
                            </ul>
                        </div>

                        <!-- Introduction -->
                        <h2 id="introduction">Introduction</h2>
                        <p>Machine learning can seem like magic—algorithms that learn from data and make predictions without being explicitly programmed. But behind this "magic" lies rigorous mathematics and statistics. Understanding these foundations is crucial for anyone who wants to move beyond using ML as a black box and truly grasp how and why these techniques work.</p>

                        <p>In this comprehensive guide, we'll explore <strong>35+ machine learning techniques</strong> spanning the entire AI landscape—from classical algorithms like Linear Regression to cutting-edge systems like Transformers, Diffusion Models, and RLHF. Each technique is presented through the lens of its mathematical and statistical underpinnings, making complex concepts accessible to beginners while providing depth for practitioners.</p>

                        <div class="highlight-box">
                            <strong><i class="fas fa-lightbulb me-2"></i>Key Insight:</strong> Every machine learning algorithm is essentially an optimization problem—we're trying to find the best parameters that minimize error or maximize some objective function. The math tells us HOW to find those parameters, while statistics tells us WHY they work and when to trust them. This principle holds whether you're fitting a simple linear regression or training a multi-billion parameter language model.
                        </div>

                        <h3>What You'll Learn</h3>
                        <p>This guide is organized into major categories that reflect the evolution and diversity of machine learning:</p>
                        <ul>
                            <li><strong>Classical Machine Learning:</strong> The foundational algorithms that still power much of industry ML today</li>
                            <li><strong>Unsupervised Learning:</strong> Techniques for finding patterns in unlabeled data</li>
                            <li><strong>Deep Learning Foundations:</strong> Neural networks and their powerful variants</li>
                            <li><strong>Modern Architectures:</strong> Transformers, embeddings, and generative models that define 2020s AI</li>
                            <li><strong>Reinforcement Learning:</strong> Agents that learn through interaction and reward</li>
                            <li><strong>Advanced AI Systems:</strong> Hybrid approaches combining multiple paradigms</li>
                            <li><strong>Learning Paradigms:</strong> Meta-learning, continual learning, and causal inference</li>
                        </ul>

                        <p>Whether you're a student, aspiring data scientist, ML engineer, or curious developer, this article will help you build intuition about what's happening under the hood of modern AI systems.</p>

                        <!-- Summary Table -->
                        <h2 id="summary-table">Quick Reference: ML Techniques Summary</h2>
                        <p>Before diving into details, here's a comprehensive overview of machine learning techniques from classical algorithms to cutting-edge AI systems. This roadmap shows the mathematical and statistical foundations, plus real-world applications:</p>

                        <table class="summary-table">
                            <thead>
                                <tr>
                                    <th>ML Technique</th>
                                    <th>Core Mathematical Foundations</th>
                                    <th>Statistical Foundations</th>
                                    <th>Where It's Used Today</th>
                                </tr>
                            </thead>
                            <tbody>
                                <!-- Classical Machine Learning -->
                                <tr style="background: rgba(19, 36, 64, 0.05); font-weight: 600;">
                                    <td colspan="4" style="color: var(--color-navy);">CLASSICAL MACHINE LEARNING</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#linear-regression" style="color: var(--color-navy); text-decoration: none;">Linear Regression</a></strong></td>
                                    <td>Linear algebra, optimization</td>
                                    <td>Gaussian noise, MLE</td>
                                    <td>Baselines, forecasting</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#logistic-regression" style="color: var(--color-navy); text-decoration: none;">Logistic Regression</a></strong></td>
                                    <td>Calculus, convex optimization</td>
                                    <td>Bernoulli, cross-entropy</td>
                                    <td>Classification, risk models</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#naive-bayes" style="color: var(--color-navy); text-decoration: none;">Naive Bayes</a></strong></td>
                                    <td>Probability theory</td>
                                    <td>Conditional independence</td>
                                    <td>Text classification, spam filtering</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#knn" style="color: var(--color-navy); text-decoration: none;">k-Nearest Neighbors</a></strong></td>
                                    <td>Metric spaces, distance functions</td>
                                    <td>Non-parametric, kernel density</td>
                                    <td>Recommendation, similarity search</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#svm" style="color: var(--color-navy); text-decoration: none;">Support Vector Machines</a></strong></td>
                                    <td>Convex optimization, kernel trick</td>
                                    <td>Margin theory, VC dimension</td>
                                    <td>Image classification, bioinformatics</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#decision-trees" style="color: var(--color-navy); text-decoration: none;">Decision Trees</a></strong></td>
                                    <td>Information theory, recursive partitioning</td>
                                    <td>Entropy, Gini impurity</td>
                                    <td>Interpretable ML, credit scoring</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#random-forest" style="color: var(--color-navy); text-decoration: none;">Random Forest</a></strong></td>
                                    <td>Ensemble learning, bootstrap</td>
                                    <td>Bagging, variance reduction</td>
                                    <td>Feature importance, competitions</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#gradient-boosting" style="color: var(--color-navy); text-decoration: none;">Gradient Boosting</a></strong></td>
                                    <td>Gradient descent, additive models</td>
                                    <td>Loss minimization, regularization</td>
                                    <td>Kaggle, fraud detection, ranking</td>
                                </tr>
                                
                                <!-- Unsupervised Learning -->
                                <tr style="background: rgba(19, 36, 64, 0.05); font-weight: 600;">
                                    <td colspan="4" style="color: var(--color-navy);">UNSUPERVISED LEARNING</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#pca" style="color: var(--color-navy); text-decoration: none;">Principal Component Analysis</a></strong></td>
                                    <td>Linear algebra, eigendecomposition</td>
                                    <td>Variance maximization, orthogonality</td>
                                    <td>Dimensionality reduction, visualization</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#kmeans" style="color: var(--color-navy); text-decoration: none;">k-Means Clustering</a></strong></td>
                                    <td>Optimization, iterative refinement</td>
                                    <td>Distance-based, centroid estimation</td>
                                    <td>Customer segmentation, compression</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#hierarchical" style="color: var(--color-navy); text-decoration: none;">Hierarchical Clustering</a></strong></td>
                                    <td>Graph theory, linkage metrics</td>
                                    <td>Distance matrices, dendrogram</td>
                                    <td>Taxonomy, gene analysis</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#tsne" style="color: var(--color-navy); text-decoration: none;">t-SNE</a></strong></td>
                                    <td>Non-linear dimensionality reduction</td>
                                    <td>Probability distributions, KL divergence</td>
                                    <td>High-dim visualization, embeddings</td>
                                </tr>
                                
                                <!-- Deep Learning Foundations -->
                                <tr style="background: rgba(19, 36, 64, 0.05); font-weight: 600;">
                                    <td colspan="4" style="color: var(--color-navy);">DEEP LEARNING FOUNDATIONS</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#neural-nets" style="color: var(--color-navy); text-decoration: none;">Neural Networks (MLPs)</a></strong></td>
                                    <td>Backpropagation, chain rule</td>
                                    <td>Universal approximation, SGD</td>
                                    <td>Tabular data, embeddings</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#cnns" style="color: var(--color-navy); text-decoration: none;">Convolutional Neural Networks</a></strong></td>
                                    <td>Convolutions, pooling, hierarchical features</td>
                                    <td>Translation invariance, spatial hierarchy</td>
                                    <td>Computer vision, image classification</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#rnns" style="color: var(--color-navy); text-decoration: none;">Recurrent Neural Networks</a></strong></td>
                                    <td>Temporal dynamics, BPTT</td>
                                    <td>Sequential modeling, hidden states</td>
                                    <td>Time series, legacy NLP</td>
                                </tr>
                                
                                <!-- Modern Architectures -->
                                <tr style="background: rgba(59, 151, 151, 0.15); font-weight: 600;">
                                    <td colspan="4" style="color: var(--color-teal);">MODERN ARCHITECTURES</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#transformers" style="color: var(--color-teal); text-decoration: none;">Transformers</a></strong></td>
                                    <td>Self-attention, matrix multiplication</td>
                                    <td>Parallel processing, positional encoding</td>
                                    <td>LLMs, GPT, BERT, translation</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#attention" style="color: var(--color-teal); text-decoration: none;">Attention Mechanisms</a></strong></td>
                                    <td>Weighted aggregation, softmax</td>
                                    <td>Context modeling, query-key-value</td>
                                    <td>Machine translation, image captioning</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#word-embeddings" style="color: var(--color-teal); text-decoration: none;">Word Embeddings</a></strong></td>
                                    <td>Vector spaces, cosine similarity</td>
                                    <td>Distributional semantics, co-occurrence</td>
                                    <td>NLP preprocessing, semantic search</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#gans" style="color: var(--color-teal); text-decoration: none;">GANs</a></strong></td>
                                    <td>Minimax game theory, Nash equilibrium</td>
                                    <td>Adversarial training, discriminator loss</td>
                                    <td>Image generation, deepfakes, art</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#vae" style="color: var(--color-teal); text-decoration: none;">Autoencoders (VAE)</a></strong></td>
                                    <td>Latent space, reconstruction loss</td>
                                    <td>Probabilistic encoding, KL divergence</td>
                                    <td>Anomaly detection, denoising, compression</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#diffusion" style="color: var(--color-teal); text-decoration: none;">Diffusion Models</a></strong></td>
                                    <td>Stochastic processes, reverse diffusion</td>
                                    <td>Gaussian noise, denoising score matching</td>
                                    <td>DALL-E, Stable Diffusion, Midjourney</td>
                                </tr>
                                
                                <!-- Reinforcement Learning -->
                                <tr style="background: rgba(19, 36, 64, 0.05); font-weight: 600;">
                                    <td colspan="4" style="color: var(--color-navy);">REINFORCEMENT LEARNING</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#q-learning" style="color: var(--color-navy); text-decoration: none;">Q-Learning</a></strong></td>
                                    <td>Dynamic programming, Bellman equation</td>
                                    <td>Value iteration, temporal difference</td>
                                    <td>Game AI, robotics control</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#policy-gradients" style="color: var(--color-navy); text-decoration: none;">Policy Gradients</a></strong></td>
                                    <td>Gradient ascent, policy optimization</td>
                                    <td>Stochastic policies, REINFORCE</td>
                                    <td>Robotics, autonomous vehicles</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#actor-critic" style="color: var(--color-navy); text-decoration: none;">Actor-Critic</a></strong></td>
                                    <td>Dual networks, advantage estimation</td>
                                    <td>Variance reduction, bias-variance trade-off</td>
                                    <td>AlphaGo, continuous control</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#dqn" style="color: var(--color-navy); text-decoration: none;">Deep Q-Networks (DQN)</a></strong></td>
                                    <td>Neural function approximation, experience replay</td>
                                    <td>Off-policy learning, target networks</td>
                                    <td>Atari games, game AI</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#ppo" style="color: var(--color-navy); text-decoration: none;">Proximal Policy Optimization</a></strong></td>
                                    <td>Clipped objectives, trust regions</td>
                                    <td>Policy constraint, KL penalty</td>
                                    <td>ChatGPT RLHF, robotics</td>
                                </tr>
                                
                                <!-- Advanced AI Systems -->
                                <tr style="background: rgba(59, 151, 151, 0.15); font-weight: 600;">
                                    <td colspan="4" style="color: var(--color-teal);">ADVANCED AI SYSTEMS</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#rag" style="color: var(--color-teal); text-decoration: none;">Retrieval-Augmented Generation</a></strong></td>
                                    <td>Vector databases, semantic retrieval</td>
                                    <td>Information retrieval, ranking</td>
                                    <td>ChatGPT plugins, enterprise chatbots</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#rlhf" style="color: var(--color-teal); text-decoration: none;">RLHF</a></strong></td>
                                    <td>Reward modeling, preference learning</td>
                                    <td>Human feedback, Bradley-Terry model</td>
                                    <td>ChatGPT, Claude, instruction tuning</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#moe" style="color: var(--color-teal); text-decoration: none;">Mixture of Experts</a></strong></td>
                                    <td>Sparse activation, gating networks</td>
                                    <td>Ensemble specialization, routing</td>
                                    <td>GPT-4, large-scale models</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#nas" style="color: var(--color-teal); text-decoration: none;">Neural Architecture Search</a></strong></td>
                                    <td>Optimization, search algorithms</td>
                                    <td>Performance estimation, hyperparameter tuning</td>
                                    <td>EfficientNet, AutoML</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#agentic-ai" style="color: var(--color-teal); text-decoration: none;">Agentic AI</a></strong></td>
                                    <td>Multi-step reasoning, tool use</td>
                                    <td>Planning, decision trees</td>
                                    <td>LangChain, AutoGPT, AI assistants</td>
                                </tr>
                                
                                <!-- Learning Paradigms -->
                                <tr style="background: rgba(19, 36, 64, 0.05); font-weight: 600;">
                                    <td colspan="4" style="color: var(--color-navy);">LEARNING PARADIGMS</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#transfer-learning" style="color: var(--color-navy); text-decoration: none;">Transfer Learning</a></strong></td>
                                    <td>Feature reuse, fine-tuning</td>
                                    <td>Domain adaptation, pre-training</td>
                                    <td>Fine-tuning LLMs, computer vision</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#few-shot" style="color: var(--color-navy); text-decoration: none;">Few-Shot Learning</a></strong></td>
                                    <td>Meta-learning, prototype networks</td>
                                    <td>Low-data regimes, similarity metrics</td>
                                    <td>GPT prompting, medical imaging</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#self-supervised" style="color: var(--color-navy); text-decoration: none;">Self-Supervised Learning</a></strong></td>
                                    <td>Pretext tasks, contrastive learning</td>
                                    <td>Unlabeled data, representation learning</td>
                                    <td>BERT, SimCLR, foundation models</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#continual" style="color: var(--color-navy); text-decoration: none;">Continual Learning</a></strong></td>
                                    <td>Catastrophic forgetting mitigation</td>
                                    <td>Sequential task learning, replay buffers</td>
                                    <td>Lifelong agents, adaptive systems</td>
                                </tr>
                                <tr>
                                    <td><strong><a href="#causal" style="color: var(--color-navy); text-decoration: none;">Causal Inference</a></strong></td>
                                    <td>DAGs, do-calculus, interventions</td>
                                    <td>Confounding, counterfactuals</td>
                                    <td>A/B testing, policy evaluation</td>
                                </tr>
                            </tbody>
                        </table>

                        <!-- CLASSICAL MACHINE LEARNING -->
                        <h2 id="linear-regression" style="margin-top: 3rem;">Classical Machine Learning</h2>
                        
                        <h3 id="linear-regression">Linear Regression</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-chart-line me-2"></i>Linear Regression</h4>
                            <p><strong>Core Idea:</strong> Find the best straight line (or hyperplane) that fits your data points, minimizing prediction errors.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Linear regression models the relationship between input features <strong>X</strong> and output <strong>y</strong> as:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε
                            </p>
                            <p>Where β (beta) coefficients are learned parameters and ε (epsilon) represents Gaussian noise. In matrix form: <strong>y = Xβ + ε</strong></p>
                            
                            <p>The optimal solution uses <strong>linear algebra</strong> to solve the normal equation:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                β = (XᵀX)⁻¹Xᵀy
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Maximum Likelihood Estimation (MLE):</strong> Assumes errors follow a Gaussian (normal) distribution</li>
                                <li><strong>Least Squares:</strong> Minimizing sum of squared errors is equivalent to MLE under Gaussian noise assumption</li>
                                <li><strong>Assumptions:</strong> Linearity, independence, homoscedasticity (constant variance), normality of errors</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> When errors are normally distributed, the least squares solution is the maximum likelihood estimate—it's the most probable model given the data.</p>
                            
                            <p><strong>Used For:</strong> Baseline models, forecasting (sales, stock prices), understanding feature relationships, quick prototyping</p>
                        </div>

                        <h3 id="logistic-regression">Logistic Regression</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-divide me-2"></i>Logistic Regression</h4>
                            <p><strong>Core Idea:</strong> Transform linear predictions into probabilities between 0 and 1 for classification tasks.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses the <strong>sigmoid function</strong> to squash linear outputs into probabilities:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                P(y=1|x) = σ(z) = 1 / (1 + e⁻ᶻ) where z = β₀ + β₁x₁ + ... + βₙxₙ
                            </p>
                            
                            <p>Optimization uses <strong>calculus</strong> (gradient descent) to minimize the loss function:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Loss = -[y log(p) + (1-y) log(1-p)]  (Cross-Entropy)
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Bernoulli Distribution:</strong> Models binary outcomes (0 or 1, yes/no, true/false)</li>
                                <li><strong>Maximum Likelihood Estimation:</strong> Finds parameters that maximize probability of observed data</li>
                                <li><strong>Log-Odds (Logit):</strong> The linear combination z represents the log of odds ratio</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> The sigmoid function naturally models probability, and cross-entropy loss heavily penalizes confident wrong predictions, pushing the model toward correct classifications.</p>
                            
                            <p><strong>Used For:</strong> Binary classification (spam/not spam, fraud detection), medical diagnosis, click-through rate prediction, risk scoring</p>
                        </div>

                        <h3 id="naive-bayes">Naive Bayes</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-brain me-2"></i>Naive Bayes</h4>
                            <p><strong>Core Idea:</strong> Use Bayes' theorem to calculate the probability of each class given the features, assuming features are independent.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Bayes' theorem in <strong>probability theory</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                P(Class|Features) = [P(Features|Class) × P(Class)] / P(Features)
                            </p>
                            
                            <p>The "naive" assumption simplifies this by treating features as <strong>conditionally independent</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                P(x₁,x₂,...,xₙ|Class) = P(x₁|Class) × P(x₂|Class) × ... × P(xₙ|Class)
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Conditional Independence:</strong> Assumes each feature contributes independently to the probability</li>
                                <li><strong>Prior Probabilities:</strong> P(Class) learned from training data frequency</li>
                                <li><strong>Likelihood:</strong> P(Features|Class) estimated from training distribution</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Even though the independence assumption is usually violated in real data, it works surprisingly well because we only need the correct ranking of probabilities, not accurate absolute values.</p>
                            
                            <p><strong>Used For:</strong> Text classification (spam filtering, sentiment analysis), document categorization, real-time prediction (fast training/inference)</p>
                        </div>

                        <h3 id="knn">k-Nearest Neighbors (k-NN)</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-users me-2"></i>k-Nearest Neighbors</h4>
                            <p><strong>Core Idea:</strong> Classify new points based on the majority class of their k closest neighbors in feature space.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>metric spaces</strong> and distance functions (typically Euclidean):</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                d(x, x') = √[(x₁-x'₁)² + (x₂-x'₂)² + ... + (xₙ-x'ₙ)²]
                            </p>
                            
                            <p>Prediction is made by majority vote (classification) or averaging (regression) of the k nearest neighbors.</p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Non-parametric:</strong> Makes no assumptions about underlying data distribution</li>
                                <li><strong>Lazy Learning:</strong> Stores all training data; computation happens at prediction time</li>
                                <li><strong>Kernel Density Estimation:</strong> Implicitly estimates local probability density</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Based on the assumption that similar inputs should produce similar outputs. The "curse of dimensionality" means it works best in low-dimensional spaces where distance is meaningful.</p>
                            
                            <p><strong>Used For:</strong> Recommendation systems, similarity search, pattern recognition, anomaly detection, filling missing values</p>
                        </div>

                        <h3 id="svm">Support Vector Machines (SVM)</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-vector-square me-2"></i>Support Vector Machines</h4>
                            <p><strong>Core Idea:</strong> Find the decision boundary (hyperplane) that maximizes the margin between classes.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>SVM solves a <strong>convex optimization</strong> problem to find the maximum-margin hyperplane:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Minimize: ½||w||² + C∑ξᵢ<br>
                                Subject to: yᵢ(w·xᵢ + b) ≥ 1 - ξᵢ
                            </p>
                            
                            <p>Where w is the weight vector, C is the regularization parameter, and ξ (xi) are slack variables allowing some misclassification.</p>
                            
                            <p><strong>Kernel Trick:</strong> Map data to higher dimensions using kernel functions (RBF, polynomial) without explicit transformation:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                K(x, x') = φ(x) · φ(x')  (e.g., RBF: K(x,x') = exp(-γ||x-x'||²))
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Margin Theory:</strong> Larger margins lead to better generalization (VC dimension, structural risk minimization)</li>
                                <li><strong>Support Vectors:</strong> Only points near the decision boundary (support vectors) matter</li>
                                <li><strong>Regularization:</strong> C parameter trades off margin width vs. training accuracy</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Maximizing the margin provides a buffer zone that helps the model generalize well to unseen data, even with limited training examples.</p>
                            
                            <p><strong>Used For:</strong> High-dimensional classification (text, genomics), image classification, anomaly detection, kernel methods for non-linear problems</p>
                        </div>

                        <h3 id="decision-trees">Decision Trees</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-sitemap me-2"></i>Decision Trees</h4>
                            <p><strong>Core Idea:</strong> Build a tree structure where each node asks a yes/no question about a feature, splitting data into purer subsets.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>recursive partitioning</strong> to split data. At each node, choose the split that maximizes information gain or minimizes impurity.</p>
                            
                            <p><strong>Splitting Criteria:</strong></p>
                            <ul>
                                <li><strong>Entropy (Information Gain):</strong> H(S) = -∑ p(c) log₂ p(c)</li>
                                <li><strong>Gini Impurity:</strong> Gini(S) = 1 - ∑ p(c)²</li>
                                <li><strong>Variance Reduction:</strong> For regression, minimize variance in child nodes</li>
                            </ul>
                            
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Information Gain = Entropy(parent) - Σ [|child|/|parent| × Entropy(child)]
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Entropy from Information Theory:</strong> Measures uncertainty/disorder in data</li>
                                <li><strong>Gini Coefficient:</strong> Probability of misclassification if label assigned randomly</li>
                                <li><strong>Greedy Algorithm:</strong> Locally optimal splits at each step</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Each split increases purity (reduces uncertainty), gradually separating classes. The tree structure naturally captures non-linear relationships and feature interactions.</p>
                            
                            <p><strong>Used For:</strong> Interpretable ML, medical diagnosis, credit scoring, feature engineering, baseline models, embedded in Random Forests/Gradient Boosting</p>
                        </div>

                        <h3 id="random-forest">Random Forest</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-tree me-2"></i>Random Forest</h4>
                            <p><strong>Core Idea:</strong> Train many decision trees on random subsets of data and features, then average their predictions to reduce overfitting.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>bagging (bootstrap aggregating)</strong> and the <strong>Law of Large Numbers</strong>:</p>
                            <ol>
                                <li>Create B bootstrap samples (random sampling with replacement)</li>
                                <li>Train a decision tree on each sample using random feature subset</li>
                                <li>Average predictions: ŷ = (1/B) ∑ f_b(x) for regression, majority vote for classification</li>
                            </ol>
                            
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Variance(average) = Variance(individual) / B  (when trees uncorrelated)
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Variance Reduction:</strong> Averaging reduces variance without increasing bias</li>
                                <li><strong>Law of Large Numbers:</strong> As B increases, average converges to expected value</li>
                                <li><strong>Decorrelation:</strong> Random feature selection makes trees less correlated, improving ensemble</li>
                                <li><strong>Out-of-Bag Error:</strong> Use ~37% of data not sampled for each tree as validation set</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Individual trees overfit in different ways. Averaging cancels out their errors while preserving correct predictions, leading to robust generalization.</p>
                            
                            <p><strong>Used For:</strong> Tabular data (Kaggle competitions), feature importance, regression/classification when interpretability isn't critical, handling missing data</p>
                        </div>

                        <h3 id="gradient-boosting">Gradient Boosting</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-rocket me-2"></i>Gradient Boosting (XGBoost, LightGBM)</h4>
                            <p><strong>Core Idea:</strong> Sequentially train weak learners (shallow trees) where each new tree corrects the errors of previous trees.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>functional gradient descent</strong> in function space:</p>
                            <ol>
                                <li>Start with initial prediction F₀(x) (e.g., mean)</li>
                                <li>For m = 1 to M:</li>
                                <ul>
                                    <li>Compute residuals: rᵢ = -∂L(yᵢ, F(xᵢ))/∂F(xᵢ)</li>
                                    <li>Fit tree hₘ(x) to residuals</li>
                                    <li>Update: Fₘ(x) = Fₘ₋₁(x) + η·hₘ(x)</li>
                                </ul>
                            </ol>
                            
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Final Model: F(x) = F₀(x) + η·Σ hₘ(x)  (Additive Model)
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Additive Modeling:</strong> Builds complex function as sum of simple functions</li>
                                <li><strong>Gradient Descent:</strong> Each tree steps in direction of steepest decrease in loss</li>
                                <li><strong>Regularization:</strong> Learning rate η, tree depth, min samples per leaf prevent overfitting</li>
                                <li><strong>Second-Order Methods:</strong> XGBoost uses Newton-Raphson (2nd derivatives) for faster convergence</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> By focusing on mistakes (residuals), each tree learns what previous ensemble got wrong. The sequential nature allows complex patterns to emerge gradually.</p>
                            
                            <p><strong>Used For:</strong> Winning Kaggle competitions, click-through rate prediction, ranking problems, fraud detection, time series forecasting</p>
                        </div>

                        <!-- UNSUPERVISED LEARNING -->
                        <h2 id="pca" style="margin-top: 3rem;">Unsupervised Learning</h2>
                        
                        <h3 id="pca">Principal Component Analysis (PCA)</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-compress me-2"></i>Principal Component Analysis</h4>
                            <p><strong>Core Idea:</strong> Find new axes (principal components) that capture maximum variance in the data, enabling dimensionality reduction.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>eigenvalue decomposition</strong> or <strong>Singular Value Decomposition (SVD)</strong>:</p>
                            <ol>
                                <li>Center data: X̃ = X - mean(X)</li>
                                <li>Compute covariance matrix: C = (1/n)X̃ᵀX̃</li>
                                <li>Find eigenvectors and eigenvalues: Cv = λv</li>
                                <li>Sort eigenvectors by eigenvalue (descending)</li>
                                <li>Project data onto top k eigenvectors</li>
                            </ol>
                            
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                X_reduced = X̃ · V_k  where V_k = [v₁, v₂, ..., v_k]
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Variance Maximization:</strong> First PC captures most variance, second captures most remaining variance (orthogonal to first), etc.</li>
                                <li><strong>Covariance Structure:</strong> Eigenvectors point in directions of maximum spread</li>
                                <li><strong>Information Preservation:</strong> Retain top k PCs to capture desired % of total variance (e.g., 95%)</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> High variance directions typically contain more signal than noise. PCA finds a compact representation that preserves the most information.</p>
                            
                            <p><strong>Used For:</strong> Dimensionality reduction before ML, data visualization (2D/3D plots), noise reduction, feature extraction, compressing images</p>
                        </div>

                        <h3 id="kmeans">k-Means Clustering</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-project-diagram me-2"></i>k-Means Clustering</h4>
                            <p><strong>Core Idea:</strong> Partition data into k clusters by iteratively assigning points to nearest centroid and updating centroids.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>Euclidean geometry</strong> to minimize within-cluster sum of squares:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Minimize: Σ Σ ||xᵢ - μ_k||²  (sum over k clusters, points in each cluster)
                            </p>
                            
                            <p><strong>Algorithm (Lloyd's Algorithm):</strong></p>
                            <ol>
                                <li>Initialize k centroids randomly</li>
                                <li>Repeat until convergence:</li>
                                <ul>
                                    <li>Assign each point to nearest centroid</li>
                                    <li>Update centroids to mean of assigned points</li>
                                </ul>
                            </ol>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Spherical Gaussian Assumption:</strong> Works best when clusters are roughly spherical and similar size</li>
                                <li><strong>EM Algorithm:</strong> k-means is special case of Expectation-Maximization for Gaussian mixtures</li>
                                <li><strong>Voronoi Tessellation:</strong> Creates regions where all points are closer to one centroid than others</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Iteratively improves cluster quality by moving centroids to "center of mass" and reassigning points. Guaranteed to converge (though possibly to local minimum).</p>
                            
                            <p><strong>Used For:</strong> Customer segmentation, image compression (color quantization), document clustering, anomaly detection, data preprocessing</p>
                        </div>

                        <h3 id="hierarchical">Hierarchical Clustering</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-project-diagram me-2"></i>Hierarchical Clustering</h4>
                            <p><strong>Core Idea:</strong> Build a tree (dendrogram) of clusters by either merging small clusters into larger ones (agglomerative) or splitting large clusters into smaller ones (divisive).</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>graph theory</strong> and distance metrics with linkage criteria:</p>
                            <ul>
                                <li><strong>Single Linkage:</strong> Distance between closest points in clusters</li>
                                <li><strong>Complete Linkage:</strong> Distance between farthest points in clusters</li>
                                <li><strong>Average Linkage:</strong> Average distance between all pairs of points</li>
                                <li><strong>Ward's Method:</strong> Minimizes within-cluster variance when merging</li>
                            </ul>
                            
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                d(C₁, C₂) = min{d(x, y) : x ∈ C₁, y ∈ C₂}  (Single Linkage)
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Distance Matrix:</strong> Pairwise distances between all data points stored in symmetric matrix</li>
                                <li><strong>Dendrogram:</strong> Tree structure visualizes cluster hierarchy at all scales</li>
                                <li><strong>No Assumption on k:</strong> Unlike k-means, doesn't require pre-specifying number of clusters</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> The dendrogram reveals cluster structure at multiple resolutions, allowing you to cut the tree at any height to get desired granularity. Different linkage methods capture different cluster shapes.</p>
                            
                            <p><strong>Used For:</strong> Taxonomy (biology), gene expression analysis, social network communities, document organization, phylogenetic trees</p>
                        </div>

                        <h3 id="tsne">t-SNE (t-Distributed Stochastic Neighbor Embedding)</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-cube me-2"></i>t-SNE</h4>
                            <p><strong>Core Idea:</strong> Non-linear dimensionality reduction that maps high-dimensional data to 2D/3D while preserving local neighborhood structure, making similar points cluster together.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>probability distributions</strong> to model similarity:</p>
                            <ol>
                                <li><strong>High-dimensional space:</strong> Model pairwise similarities using Gaussian distribution</li>
                                <li><strong>Low-dimensional space:</strong> Model similarities using Student's t-distribution (heavy tails prevent crowding)</li>
                                <li><strong>Optimize:</strong> Minimize KL divergence between high-dim and low-dim probability distributions</li>
                            </ol>
                            
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                KL(P||Q) = Σᵢⱼ pᵢⱼ log(pᵢⱼ/qᵢⱼ)
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Kullback-Leibler Divergence:</strong> Measures how one probability distribution differs from another</li>
                                <li><strong>Student's t-Distribution:</strong> Heavy tails help spread out clusters in low dimensions</li>
                                <li><strong>Perplexity:</strong> Hyperparameter controlling effective number of neighbors (typical: 5-50)</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> By using different distributions in high and low dimensions, t-SNE prevents the "crowding problem" where dissimilar points get squashed together, resulting in clearer visual separation of clusters.</p>
                            
                            <p><strong>Used For:</strong> Visualizing high-dimensional embeddings (word2vec, BERT), exploring image datasets, understanding neural network representations, exploratory data analysis</p>
                        </div>

                        <h3 id="gmm">Gaussian Mixture Models (GMM)</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-layer-group me-2"></i>Gaussian Mixture Models</h4>
                            <p><strong>Core Idea:</strong> Model data as a mixture of multiple Gaussian distributions, each representing a cluster with its own mean and covariance.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>linear algebra</strong> and <strong>probability theory</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                P(x) = Σ π_k · N(x | μ_k, Σ_k)
                            </p>
                            <p>Where π_k are mixing coefficients (weights summing to 1), and N(x | μ_k, Σ_k) is a multivariate Gaussian.</p>
                            
                            <p><strong>Expectation-Maximization (EM) Algorithm:</strong></p>
                            <ul>
                                <li><strong>E-step:</strong> Compute probability that each point belongs to each cluster (soft assignment)</li>
                                <li><strong>M-step:</strong> Update parameters (means, covariances, weights) to maximize likelihood</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Latent Variables:</strong> Cluster membership is hidden; EM infers it probabilistically</li>
                                <li><strong>Maximum Likelihood:</strong> Finds parameters that make observed data most probable</li>
                                <li><strong>Soft Clustering:</strong> Points can belong to multiple clusters with different probabilities</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> More flexible than k-means—handles elliptical clusters, different sizes, and provides uncertainty estimates. EM provably increases likelihood at each iteration.</p>
                            
                            <p><strong>Used For:</strong> Density estimation, anomaly detection, speaker recognition, image segmentation, soft clustering when uncertainty matters</p>
                        </div>

                        <h3 id="hmm">Hidden Markov Models (HMM)</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-random me-2"></i>Hidden Markov Models</h4>
                            <p><strong>Core Idea:</strong> Model sequential data where the system has hidden states that transition over time, producing observable outputs.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Based on <strong>Markov chains</strong> and <strong>probability theory</strong>:</p>
                            <ul>
                                <li><strong>Hidden states:</strong> S = {s₁, s₂, ..., s_N}</li>
                                <li><strong>Transition probabilities:</strong> A = P(s_t | s_{t-1})  (Markov property)</li>
                                <li><strong>Emission probabilities:</strong> B = P(o_t | s_t)  (observation given state)</li>
                                <li><strong>Initial probabilities:</strong> π = P(s_1)</li>
                            </ul>
                            
                            <p><strong>Key Algorithms:</strong></p>
                            <ul>
                                <li><strong>Forward-Backward:</strong> Compute P(observations | model)</li>
                                <li><strong>Viterbi:</strong> Find most likely sequence of hidden states</li>
                                <li><strong>Baum-Welch:</strong> Learn parameters from data (special case of EM)</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Markov Property:</strong> Future depends only on present, not past (memoryless)</li>
                                <li><strong>Bayesian Inference:</strong> Infer hidden states from observations</li>
                                <li><strong>Dynamic Programming:</strong> Efficient computation via memoization</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Captures temporal dependencies while keeping computation tractable. The Markov assumption simplifies inference without losing too much modeling power.</p>
                            
                            <p><strong>Used For:</strong> Speech recognition, gene sequence analysis, part-of-speech tagging in NLP, gesture recognition, time series analysis</p>
                        </div>

                        <!-- DEEP LEARNING FOUNDATIONS -->
                        <h2 id="neural-nets" style="margin-top: 3rem;">Deep Learning Foundations</h2>
                        
                        <h3 id="neural-nets">Neural Networks (Multilayer Perceptron)</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-network-wired me-2"></i>Neural Networks (MLP)</h4>
                            <p><strong>Core Idea:</strong> Stack layers of artificial neurons that transform inputs through non-linear activations, learning hierarchical representations.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Combines <strong>linear algebra</strong> (matrix operations) and <strong>calculus</strong> (backpropagation):</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                z = Wx + b  (linear transformation)<br>
                                a = σ(z)  (non-linear activation)
                            </p>
                            
                            <p><strong>Forward pass:</strong> Data flows through layers: x → h₁ → h₂ → ... → ŷ</p>
                            <p><strong>Backpropagation:</strong> Compute gradients via chain rule:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                ∂L/∂W = ∂L/∂ŷ · ∂ŷ/∂a · ∂a/∂z · ∂z/∂W
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Empirical Risk Minimization (ERM):</strong> Minimize average loss over training data</li>
                                <li><strong>Regularization:</strong> L1/L2 penalties, dropout, early stopping prevent overfitting</li>
                                <li><strong>Universal Approximation Theorem:</strong> With enough neurons, can approximate any continuous function</li>
                                <li><strong>Stochastic Gradient Descent:</strong> Update weights using mini-batches for efficiency</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Non-linear activations allow learning complex decision boundaries. Depth enables hierarchical feature learning—lower layers detect simple patterns, higher layers combine them into abstractions.</p>
                            
                            <p><strong>Used For:</strong> General function approximation, tabular data, recommender systems, time series, foundational component of all deep learning</p>
                        </div>

                        <h3 id="cnns">Convolutional Neural Networks (CNNs)</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-image me-2"></i>Convolutional Neural Networks</h4>
                            <p><strong>Core Idea:</strong> Use convolutional filters that slide across images to detect local patterns, preserving spatial structure.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Based on <strong>convolution operations</strong> from signal processing:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                (f * g)[i,j] = Σ Σ f[m,n] · g[i-m, j-n]
                            </p>
                            
                            <p><strong>Key components:</strong></p>
                            <ul>
                                <li><strong>Convolutional layers:</strong> Learn filters (e.g., edge detectors) through backprop</li>
                                <li><strong>Pooling layers:</strong> Downsample via max/average pooling for spatial invariance</li>
                                <li><strong>Fully connected layers:</strong> Final classification based on extracted features</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Translation Invariance:</strong> Same filter applied everywhere learns location-independent features</li>
                                <li><strong>Parameter Sharing:</strong> Reusing weights across spatial locations reduces overfitting</li>
                                <li><strong>Hierarchical Features:</strong> Early layers: edges/textures → Middle: parts/patterns → Deep: objects/concepts</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Convolution exploits spatial structure—nearby pixels are correlated. Weight sharing provides strong inductive bias for vision tasks while reducing parameters dramatically.</p>
                            
                            <p><strong>Used For:</strong> Image classification, object detection, facial recognition, medical imaging, autonomous vehicles, video analysis</p>
                        </div>

                        <h3 id="rnns">Recurrent Neural Networks (RNN/LSTM)</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-sync me-2"></i>Recurrent Neural Networks</h4>
                            <p><strong>Core Idea:</strong> Process sequences by maintaining hidden state that gets updated at each time step, enabling memory of past inputs.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>recurrence relations</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                h_t = σ(W_h · h_{t-1} + W_x · x_t + b)<br>
                                y_t = softmax(W_y · h_t)
                            </p>
                            
                            <p><strong>LSTM (Long Short-Term Memory):</strong> Solves vanishing gradient problem with gating mechanisms:</p>
                            <ul>
                                <li><strong>Forget gate:</strong> What to remove from cell state</li>
                                <li><strong>Input gate:</strong> What new information to store</li>
                                <li><strong>Output gate:</strong> What to output based on cell state</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Sequence Modeling:</strong> Captures temporal dependencies via hidden state</li>
                                <li><strong>Backpropagation Through Time (BPTT):</strong> Unfold network across time for gradient computation</li>
                                <li><strong>Vanishing/Exploding Gradients:</strong> LSTM/GRU architectures address this with skip connections</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Hidden state acts as memory, allowing network to maintain context. LSTM gates learn what to remember/forget, enabling learning of long-range dependencies.</p>
                            
                            <p><strong>Used For:</strong> Language modeling, machine translation, speech recognition, time series forecasting, video captioning, music generation</p>
                        </div>

                        <!-- MODERN ARCHITECTURES -->
                        <h2 style="margin-top: 3rem;">Modern Architectures</h2>
                        
                        <h3 id="transformers">Transformers</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-bolt me-2"></i>Transformers ⚡</h4>
                            <p><strong>Core Idea:</strong> Replace recurrence with self-attention—allow every position to attend to all positions simultaneously, enabling parallel processing.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p><strong>Self-Attention</strong> uses <strong>matrix operations</strong> and <strong>dot products</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Attention(Q, K, V) = softmax(QKᵀ/√d_k) · V
                            </p>
                            <p>Where Q (queries), K (keys), V (values) are learned linear projections of inputs.</p>
                            
                            <p><strong>Multi-Head Attention:</strong> Run h parallel attention mechanisms and concatenate:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                MultiHead(Q,K,V) = Concat(head₁,...,head_h) · W^O
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Token Likelihood:</strong> Trained to predict next token via maximum likelihood</li>
                                <li><strong>Cross-Entropy Loss:</strong> Measures difference between predicted and true token distributions</li>
                                <li><strong>Positional Encoding:</strong> Sine/cosine functions inject sequence order information</li>
                                <li><strong>Layer Normalization & Residuals:</strong> Stabilize training of very deep networks</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Attention allows modeling long-range dependencies without recurrence. Each token directly accesses all other tokens, avoiding information bottleneck. Parallelization enables training on massive datasets.</p>
                            
                            <p><strong>Used For:</strong> Large Language Models (GPT, BERT, Claude), machine translation, code generation, multimodal AI (CLIP, Flamingo), protein folding (AlphaFold)</p>
                        </div>

                        <h3 id="attention">Attention Mechanisms</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-eye me-2"></i>Attention Mechanisms</h4>
                            <p><strong>Core Idea:</strong> Dynamically weight different parts of the input based on their relevance to the current task, allowing the model to "focus" on important information.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>weighted aggregation</strong> with learned attention scores:</p>
                            <ol>
                                <li><strong>Compute scores:</strong> How much each input relates to query</li>
                                <li><strong>Apply softmax:</strong> Convert scores to probability distribution (weights)</li>
                                <li><strong>Weighted sum:</strong> Combine values using attention weights</li>
                            </ol>
                            
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Attention(Q, K, V) = softmax(Q·Kᵀ/√dₖ) · V
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Softmax Normalization:</strong> Ensures attention weights sum to 1 (valid probability distribution)</li>
                                <li><strong>Query-Key-Value:</strong> Q determines what to look for, K what's available, V what to retrieve</li>
                                <li><strong>Scaled Dot Product:</strong> Dividing by √dₖ prevents saturation of softmax for large dimensions</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Allows model to dynamically select relevant information rather than processing all inputs equally. The learned attention patterns often correspond to interpretable relationships (e.g., grammatical dependencies in text).</p>
                            
                            <p><strong>Used For:</strong> Machine translation, image captioning, question answering, document summarization, speech recognition, Transformers</p>
                        </div>

                        <h3 id="word-embeddings">Word Embeddings (Word2Vec, GloVe)</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-font me-2"></i>Word Embeddings</h4>
                            <p><strong>Core Idea:</strong> Represent words as dense vectors in continuous space where semantically similar words are close together, enabling arithmetic operations on meaning.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>vector spaces</strong> and <strong>distributional hypothesis</strong>:</p>
                            <p><strong>Word2Vec (Skip-gram):</strong> Predict context words from center word</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                P(context|word) = softmax(v_context · v_word)
                            </p>
                            
                            <p><strong>GloVe:</strong> Factorize co-occurrence matrix to capture global statistics</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                wᵢ · w̃ⱼ + bᵢ + b̃ⱼ = log(Xᵢⱼ)
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Distributional Semantics:</strong> "You shall know a word by the company it keeps"</li>
                                <li><strong>Co-occurrence Statistics:</strong> Words appearing in similar contexts have similar meanings</li>
                                <li><strong>Cosine Similarity:</strong> Measure semantic similarity as angle between vectors</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> By training on massive text corpora, embeddings capture semantic and syntactic relationships. Famous example: king - man + woman ≈ queen (vector arithmetic captures analogies).</p>
                            
                            <p><strong>Used For:</strong> NLP preprocessing, semantic search, text classification, recommendation systems, sentiment analysis, named entity recognition</p>
                        </div>

                        <h3 id="gans">Generative Adversarial Networks (GANs)</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-palette me-2"></i>Generative Adversarial Networks</h4>
                            <p><strong>Core Idea:</strong> Train two neural networks in competition: a Generator creates fake data, while a Discriminator tries to distinguish real from fake, pushing both to improve.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>minimax game theory</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                min_G max_D V(D,G) = 𝔼[log D(x)] + 𝔼[log(1 - D(G(z)))]
                            </p>
                            
                            <p>Where:</p>
                            <ul>
                                <li><strong>Generator G:</strong> Maps random noise z to fake data G(z)</li>
                                <li><strong>Discriminator D:</strong> Outputs probability that input is real (not fake)</li>
                                <li><strong>Nash Equilibrium:</strong> Both networks reach optimal performance when generator produces perfect fakes</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Adversarial Training:</strong> Two-player zero-sum game drives both networks to improve</li>
                                <li><strong>Mode Collapse:</strong> Generator may learn to produce only subset of possible outputs</li>
                                <li><strong>Implicit Density Estimation:</strong> Generator learns data distribution without explicit modeling</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> The adversarial setup creates a curriculum where the task difficulty increases automatically. Discriminator feedback guides generator toward realistic outputs without needing explicit pixel-level supervision.</p>
                            
                            <p><strong>Used For:</strong> Image generation, style transfer, deepfakes, data augmentation, super-resolution, artistic AI (StyleGAN, Midjourney components)</p>
                        </div>

                        <h3 id="vae">Variational Autoencoders (VAE)</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-compress-arrows-alt me-2"></i>Variational Autoencoders</h4>
                            <p><strong>Core Idea:</strong> Encode data into a structured latent space where interpolation makes sense, then decode back to original space, enabling generation of new samples.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>variational inference</strong> and <strong>reparameterization trick</strong>:</p>
                            <ol>
                                <li><strong>Encoder:</strong> Maps input to probability distribution in latent space (mean and variance)</li>
                                <li><strong>Sampling:</strong> Sample latent vector from distribution (z ~ N(μ, σ²))</li>
                                <li><strong>Decoder:</strong> Reconstructs input from latent sample</li>
                            </ol>
                            
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Loss = Reconstruction Loss + KL Divergence<br>
                                ℒ = ||x - x̂||² + KL(q(z|x) || p(z))
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Probabilistic Encoding:</strong> Encoder outputs distribution parameters, not single point</li>
                                <li><strong>KL Divergence:</strong> Regularizes latent space to follow prior distribution (usually standard normal)</li>
                                <li><strong>Reparameterization Trick:</strong> Enables backpropagation through sampling operation</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> By forcing latent space to be continuous and structured (via KL term), VAEs enable smooth interpolation between data points and generation of novel samples by sampling from latent space.</p>
                            
                            <p><strong>Used For:</strong> Anomaly detection, image generation, denoising, data compression, molecule design, recommendation systems</p>
                        </div>

                        <h3 id="embeddings">Embedding Models</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-code-branch me-2"></i>Embedding Models</h4>
                            <p><strong>Core Idea:</strong> Map discrete tokens (words, images) into continuous vector spaces where semantic similarity corresponds to geometric proximity.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>metric learning</strong> and <strong>distance functions</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                embedding = Encoder(input) → v ∈ ℝ^d<br>
                                similarity(v₁, v₂) = cosine(v₁, v₂) = v₁·v₂ / (||v₁|| ||v₂||)
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Contrastive Learning:</strong> Pull similar items together, push dissimilar apart</li>
                                <li><strong>Triplet Loss:</strong> anchor-positive distance < anchor-negative distance + margin</li>
                                <li><strong>InfoNCE Loss:</strong> Maximize mutual information between positive pairs</li>
                                <li><strong>Negative Sampling:</strong> Efficiently learn from positive and negative examples</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Continuous representations enable smooth interpolation and generalization. Learned embeddings capture semantic relationships (e.g., king - man + woman ≈ queen).</p>
                            
                            <p><strong>Used For:</strong> Semantic search, RAG systems, recommendation engines, duplicate detection, zero-shot learning, transfer learning</p>
                        </div>

                        <h3 id="self-supervised">Self-Supervised Learning</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-graduation-cap me-2"></i>Self-Supervised Learning</h4>
                            <p><strong>Core Idea:</strong> Learn representations from unlabeled data by creating pretext tasks where labels come from the data itself.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Based on <strong>representation learning</strong> and <strong>information theory</strong>:</p>
                            
                            <p><strong>Common Pretext Tasks:</strong></p>
                            <ul>
                                <li><strong>Masked Language Modeling:</strong> Predict masked tokens (BERT): P(x_mask | x_context)</li>
                                <li><strong>Contrastive Predictive Coding:</strong> Maximize I(z_t; z_{t+k})  (mutual information)</li>
                                <li><strong>Rotation Prediction:</strong> Predict image rotation angle</li>
                                <li><strong>Jigsaw Puzzles:</strong> Reorder shuffled image patches</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Mutual Information Maximization:</strong> Learned representations preserve relevant information</li>
                                <li><strong>Data Augmentation:</strong> Create multiple views of same input; representations should be invariant</li>
                                <li><strong>Bootstrap:</strong> Use model's own predictions as pseudo-labels (momentum encoder)</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Solving pretext tasks forces learning of useful representations. No manual labels needed—scales to internet-scale datasets. Transfers well to downstream tasks.</p>
                            
                            <p><strong>Used For:</strong> Foundation models (GPT, BERT, CLIP), pre-training for limited labeled data, learning from unlabeled images/text/audio</p>
                        </div>

                        <h3 id="autoregressive">Autoregressive Models</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-stream me-2"></i>Autoregressive Models</h4>
                            <p><strong>Core Idea:</strong> Generate sequences by predicting next token conditioned on all previous tokens, modeling the joint distribution as a product of conditionals.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Based on <strong>probability chain rule</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                P(x₁, x₂, ..., x_n) = P(x₁) · P(x₂|x₁) · P(x₃|x₁,x₂) · ... · P(x_n|x₁,...,x_{n-1})
                            </p>
                            
                            <p><strong>Training:</strong> Maximize log-likelihood (cross-entropy):</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                ℒ = Σ log P(x_t | x_&lt;t; θ)
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Maximum Likelihood Estimation:</strong> Find parameters that maximize probability of training data</li>
                                <li><strong>Teacher Forcing:</strong> Use true previous tokens during training (not model's predictions)</li>
                                <li><strong>Sampling Strategies:</strong> Greedy, beam search, nucleus sampling, temperature scaling</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Decomposing joint distribution into conditionals makes intractable problems tractable. Model learns to capture dependencies and generate coherent sequences token by token.</p>
                            
                            <p><strong>Used For:</strong> Text generation (GPT models), code completion, image generation (pixel-by-pixel), speech synthesis, music composition</p>
                        </div>

                        <h3 id="diffusion">Diffusion Models</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-paint-brush me-2"></i>Diffusion Models</h4>
                            <p><strong>Core Idea:</strong> Learn to reverse a gradual noising process—train a model to denoise data step by step, enabling high-quality generation.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Based on <strong>stochastic processes</strong> and <strong>variational inference</strong>:</p>
                            
                            <p><strong>Forward diffusion (add noise):</strong></p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                q(x_t | x_{t-1}) = N(x_t; √(1-β_t)·x_{t-1}, β_t·I)
                            </p>
                            
                            <p><strong>Reverse process (denoise):</strong></p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                p_θ(x_{t-1} | x_t) = N(x_{t-1}; μ_θ(x_t, t), Σ_θ(x_t, t))
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Variational Lower Bound:</strong> Maximize ELBO to train denoising network</li>
                                <li><strong>Score Matching:</strong> Learn gradient of log-density (score function)</li>
                                <li><strong>Markov Chain:</strong> Each step depends only on previous step</li>
                                <li><strong>Langevin Dynamics:</strong> Stochastic differential equations guide sampling</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Gradual denoising allows model to learn at multiple scales. Each step is easier to learn than direct generation. Produces diverse, high-fidelity samples.</p>
                            
                            <p><strong>Used For:</strong> Image generation (Stable Diffusion, DALL-E 2), video synthesis, 3D generation, audio synthesis, image editing/inpainting</p>
                        </div>

                        <h3 id="flow-based">Flow-Based Models</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-exchange-alt me-2"></i>Flow-Based Models</h4>
                            <p><strong>Core Idea:</strong> Learn invertible transformations that map simple distributions (e.g., Gaussian) to complex data distributions.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>Jacobians</strong> and <strong>change of variables</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                z = f(x)  (invertible transformation)<br>
                                p_x(x) = p_z(f(x)) · |det(∂f/∂x)|
                            </p>
                            
                            <p><strong>Key properties:</strong></p>
                            <ul>
                                <li><strong>Invertibility:</strong> Can go from data to latent (f) and back (f⁻¹)</li>
                                <li><strong>Exact likelihood:</strong> No variational bound needed</li>
                                <li><strong>Bijective:</strong> One-to-one mapping preserves all information</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Exact Likelihood Estimation:</strong> Directly compute log p(x), no approximation</li>
                                <li><strong>Normalizing Flows:</strong> Stack invertible transformations: f = f_K ∘ ... ∘ f_1</li>
                                <li><strong>Jacobian Determinant:</strong> Accounts for volume change under transformation</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Invertibility enables both density estimation and sampling. Can compute exact probabilities unlike VAEs. Principled probabilistic framework.</p>
                            
                            <p><strong>Used For:</strong> Density estimation, anomaly detection, exact likelihood for model comparison, generative modeling with tractable probabilities</p>
                        </div>

                        <!-- REINFORCEMENT LEARNING -->
                        <h2 style="margin-top: 3rem;">Reinforcement Learning</h2>
                        
                        <h3 id="q-learning">Q-Learning</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-table me-2"></i>Q-Learning</h4>
                            <p><strong>Core Idea:</strong> Learn an action-value function Q(s,a) that estimates the expected cumulative reward for taking action a in state s, enabling optimal decision-making.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>dynamic programming</strong> and the <strong>Bellman equation</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]
                            </p>
                            
                            <p>Where:</p>
                            <ul>
                                <li><strong>α:</strong> Learning rate (how much to update)</li>
                                <li><strong>γ:</strong> Discount factor (importance of future rewards)</li>
                                <li><strong>r:</strong> Immediate reward</li>
                                <li><strong>max Q(s',a'):</strong> Best future value from next state</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Temporal Difference Learning:</strong> Update estimates based on difference between prediction and reality</li>
                                <li><strong>Off-Policy:</strong> Learn optimal policy while following exploratory policy (ε-greedy)</li>
                                <li><strong>Convergence:</strong> Provably converges to optimal Q* under tabular representation and sufficient exploration</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Bellman equation provides recursive relationship between current and future values. Iterative updates gradually propagate reward information backward through state-action space.</p>
                            
                            <p><strong>Used For:</strong> Game AI (simple grid worlds), robot navigation, resource allocation, foundational RL algorithm</p>
                        </div>

                        <h3 id="policy-gradients">Policy Gradients (REINFORCE)</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-chart-line me-2"></i>Policy Gradients</h4>
                            <p><strong>Core Idea:</strong> Directly optimize the policy (action selection strategy) using gradient ascent on expected reward, rather than learning value functions.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>gradient ascent</strong> on expected return:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                ∇_θ J(θ) = 𝔼[∇_θ log π_θ(a|s) · G_t]
                            </p>
                            
                            <p>Where:</p>
                            <ul>
                                <li><strong>π_θ(a|s):</strong> Stochastic policy parameterized by θ</li>
                                <li><strong>G_t:</strong> Return (cumulative discounted reward) from time t</li>
                                <li><strong>Policy Gradient Theorem:</strong> Shows how to compute gradient of expected return</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>REINFORCE Algorithm:</strong> Monte Carlo estimate of policy gradient</li>
                                <li><strong>High Variance:</strong> Stochastic gradients can have high variance (addressed with baselines)</li>
                                <li><strong>On-Policy:</strong> Must use samples from current policy</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Directly optimizes what we care about (expected reward). Works with continuous action spaces. Gradient points toward actions that led to high rewards.</p>
                            
                            <p><strong>Used For:</strong> Robotics (continuous control), dialogue systems, autonomous vehicles, any domain with complex action spaces</p>
                        </div>

                        <h3 id="actor-critic">Actor-Critic Methods</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-user-friends me-2"></i>Actor-Critic</h4>
                            <p><strong>Core Idea:</strong> Combine value-based and policy-based methods using two networks: Actor (policy) decides actions, Critic (value function) evaluates them.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>advantage estimation</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Actor: ∇_θ log π_θ(a|s) · A(s,a)<br>
                                Critic: δ = r + γV(s') - V(s)
                            </p>
                            
                            <p>Where:</p>
                            <ul>
                                <li><strong>A(s,a):</strong> Advantage function (how much better than average is this action)</li>
                                <li><strong>δ:</strong> TD error used to update critic</li>
                                <li><strong>Dual Networks:</strong> Actor and Critic trained simultaneously</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Variance Reduction:</strong> Critic provides baseline, reducing policy gradient variance</li>
                                <li><strong>Bias-Variance Trade-off:</strong> Introduces some bias but dramatically reduces variance</li>
                                <li><strong>Bootstrap:</strong> Uses value estimates (not Monte Carlo) for faster learning</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Actor benefits from reduced variance gradients thanks to Critic's feedback. Critic learns faster with actor's exploration. Together they're more sample-efficient than pure policy gradients.</p>
                            
                            <p><strong>Used For:</strong> AlphaGo/AlphaZero, continuous control, real-time decision-making, complex strategy games</p>
                        </div>

                        <h3 id="dqn">Deep Q-Networks (DQN)</h3>
                        <div class="technique-card">
                            <h4><i class="fas fa-network-wired me-2"></i>Deep Q-Networks</h4>
                            <p><strong>Core Idea:</strong> Use deep neural networks to approximate Q-function for high-dimensional state spaces (e.g., raw pixels), enabling RL on complex tasks.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Extends Q-learning with <strong>neural function approximation</strong>:</p>
                            <ol>
                                <li><strong>Experience Replay:</strong> Store transitions (s,a,r,s') in buffer, sample randomly for training</li>
                                <li><strong>Target Network:</strong> Separate frozen network for stable targets</li>
                                <li><strong>Loss Function:</strong> Minimize TD error with neural network</li>
                            </ol>
                            
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Loss = (r + γ max Q_target(s',a') - Q(s,a))²
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Off-Policy Learning:</strong> Learn from stored experiences, breaking correlation in data</li>
                                <li><strong>Target Network:</strong> Periodically updated copy prevents moving target problem</li>
                                <li><strong>ε-Greedy Exploration:</strong> Balance exploration and exploitation</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Experience replay breaks temporal correlation, making training stable. Target network prevents feedback loops. Deep networks can learn complex patterns from raw inputs.</p>
                            
                            <p><strong>Used For:</strong> Atari games (landmark achievement), game AI, robotic control from pixels, any domain with high-dimensional states</p>
                        </div>

                        <h3 id="ppo">Proximal Policy Optimization (PPO)</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-rocket me-2"></i>Proximal Policy Optimization</h4>
                            <p><strong>Core Idea:</strong> Improve policy gradients with clipped objective that prevents excessively large policy updates, ensuring stable and efficient learning.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>clipped surrogate objective</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                L^CLIP(θ) = 𝔼[min(r_t(θ)·A_t, clip(r_t(θ), 1-ε, 1+ε)·A_t)]
                            </p>
                            
                            <p>Where:</p>
                            <ul>
                                <li><strong>r_t(θ):</strong> Probability ratio π_new/π_old (how much policy changed)</li>
                                <li><strong>A_t:</strong> Advantage estimate</li>
                                <li><strong>ε:</strong> Clip range (typically 0.2)</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Trust Region:</strong> Limits policy updates to prevent catastrophic performance collapse</li>
                                <li><strong>KL Penalty (optional):</strong> Additional constraint on policy divergence</li>
                                <li><strong>Multiple Epochs:</strong> Reuse data for several gradient steps (sample efficient)</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Clipping prevents overly aggressive policy updates that could destroy learned behavior. Balances exploration with stability. Simple to implement yet very effective.</p>
                            
                            <p><strong>Used For:</strong> ChatGPT RLHF training, OpenAI Five (Dota 2), robotics, continuous control, current state-of-the-art for many RL tasks</p>
                        </div>

                        <h3 id="rl">Reinforcement Learning (General)</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-robot me-2"></i>Reinforcement Learning</h4>
                            <p><strong>Core Idea:</strong> Agent learns optimal behavior by trial-and-error interaction with environment, maximizing cumulative reward.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Based on <strong>dynamic programming</strong> and <strong>Markov Decision Processes</strong>, optimizing for expected return through value functions and policy gradients.</p>
                            
                            <p><strong>Statistical Foundation:</strong> Expected reward, Bellman equations, exploration-exploitation tradeoffs (ε-greedy, UCB)</p>
                            
                            <p><strong>Used For:</strong> Game playing, robotics, resource allocation, autonomous navigation, recommendation systems, dialog systems</p>
                        </div>

                        <h3 id="deep-rl">Deep Reinforcement Learning (General)</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-gamepad me-2"></i>Deep Reinforcement Learning</h4>
                            <p><strong>Core Idea:</strong> Combine neural networks with RL to handle high-dimensional state spaces (images, continuous control).</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Neural network function approximation with experience replay, target networks, and policy gradients (DQN, A3C, PPO)</p>
                            
                            <p><strong>Statistical Foundation:</strong> Policy gradients, actor-critic methods, off-policy learning with importance sampling</p>
                            
                            <p><strong>Used For:</strong> Atari games (DQN), Go (AlphaGo), robotic manipulation, autonomous driving, real-time strategy games</p>
                        </div>

                        <h3 id="rlhf">RLHF (Reinforcement Learning from Human Feedback)</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-users-cog me-2"></i>RLHF</h4>
                            <p><strong>Core Idea:</strong> Fine-tune language models using human preferences as reward signal, aligning model outputs with human values.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> PPO optimization with KL regularization, preference modeling via Bradley-Terry, three-stage process (SFT, reward modeling, RL optimization)</p>
                            
                            <p><strong>Statistical Foundation:</strong> Preference modeling (pairwise comparisons), reward model training, KL divergence constraints to prevent drift</p>
                            
                            <p><strong>Used For:</strong> ChatGPT, Claude, aligned LLMs, reducing harmful outputs, improving helpfulness/honesty, following instructions</p>
                        </div>

                        <h3 id="model-based-rl">Model-Based RL & Planning</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-brain me-2"></i>Model-Based RL & Planning</h4>
                            <p><strong>Core Idea:</strong> Learn a model of environment dynamics, then use it to plan actions by simulating future outcomes.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Control theory, Model Predictive Control (MPC), forward dynamics models for transition and reward prediction</p>
                            
                            <p><strong>Statistical Foundation:</strong> Transition modeling P(s'|s,a), uncertainty quantification via ensembles, planning under uncertainty</p>
                            
                            <p><strong>Used For:</strong> Agent reasoning, robotic control, simulation-based planning, Dota 2/StarCraft AI, sample-efficient RL</p>
                        </div>

                        <h3 id="mcts">Monte Carlo Tree Search</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-chess me-2"></i>Monte Carlo Tree Search</h4>
                            <p><strong>Core Idea:</strong> Build search tree incrementally using random simulations, balancing exploration and exploitation via UCB.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Tree search with Monte Carlo sampling, UCB1 selection policy, four phases (selection, expansion, simulation, backpropagation)</p>
                            
                            <p><strong>Statistical Foundation:</strong> Upper Confidence Bounds, Law of Large Numbers, regret bounds for exploration-exploitation</p>
                            
                            <p><strong>Used For:</strong> Game AI (Go, Chess with AlphaZero), strategic planning, decision-making under uncertainty, combinatorial optimization</p>
                        </div>

                        <!-- ADVANCED AI SYSTEMS -->
                        <h2 style="margin-top: 3rem;">Advanced AI Systems</h2>
                        
                        <h3 id="gnns">Graph Neural Networks</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-project-diagram me-2"></i>Graph Neural Networks</h4>
                            <p><strong>Core Idea:</strong> Extend neural networks to graph-structured data by propagating and aggregating information along edges.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Graph theory, message passing framework, aggregation functions (GCN, GraphSAGE, GAT with attention)</p>
                            
                            <p><strong>Statistical Foundation:</strong> Permutation invariance, spectral graph theory, inductive bias on graph structure</p>
                            
                            <p><strong>Used For:</strong> Knowledge graphs, molecular property prediction, social networks, recommendation systems, protein structures, traffic forecasting</p>
                        </div>

                        <h3 id="neuro-symbolic">Neuro-Symbolic AI</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-puzzle-piece me-2"></i>Neuro-Symbolic AI</h4>
                            <p><strong>Core Idea:</strong> Combine neural networks (learning from data) with symbolic reasoning (logic, rules, knowledge) for interpretable AI.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Logic + optimization, differentiable logic operations, constraint satisfaction as soft constraints on neural predictions</p>
                            
                            <p><strong>Statistical Foundation:</strong> Semantic loss functions, program synthesis, logical rule enforcement during training</p>
                            
                            <p><strong>Used For:</strong> Visual question answering, program synthesis, knowledge base reasoning, verifiable AI, scientific discovery</p>
                        </div>

                        <h3 id="memory-networks">Memory-Augmented Networks</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-database me-2"></i>Memory-Augmented Networks</h4>
                            <p><strong>Core Idea:</strong> Equip neural networks with external memory that can be read from and written to, enabling long-term storage.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Attention mechanisms over memory slots, content-addressable memory, differentiable read/write operations</p>
                            
                            <p><strong>Statistical Foundation:</strong> Retrieval theory, soft attention as differentiable memory lookup, external state for generalization</p>
                            
                            <p><strong>Used For:</strong> Long-term agent memory, question answering over documents, one-shot learning, algorithmic tasks (sorting, graphs)</p>
                        </div>

                        <h3 id="rag">Retrieval-Augmented Generation (RAG)</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-search me-2"></i>Retrieval-Augmented Generation</h4>
                            <p><strong>Core Idea:</strong> Enhance language models by retrieving relevant documents from external knowledge base before generating responses.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Vector space retrieval (dense embeddings, BM25) combined with conditional generation P(output | query, docs)</p>
                            
                            <p><strong>Statistical Foundation:</strong> Information retrieval, latent variable models (marginalizing over retrieved documents), mixture of experts</p>
                            
                            <p><strong>Used For:</strong> Enterprise chatbots, question answering, customer support, code assistants, research tools, grounded text generation</p>
                        </div>

                        <h3 id="moe">Mixture of Experts (MoE)</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-users-cog me-2"></i>Mixture of Experts</h4>
                            <p><strong>Core Idea:</strong> Use sparse activation where only a subset of model parameters ("experts") activate for each input, enabling massive scale with computational efficiency.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>gating network</strong> and <strong>sparse routing</strong>:</p>
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                y = Σᵢ G(x)ᵢ · Eᵢ(x)
                            </p>
                            
                            <p>Where:</p>
                            <ul>
                                <li><strong>G(x):</strong> Gating function selects which experts to activate (typically top-k)</li>
                                <li><strong>Eᵢ(x):</strong> i-th expert's output (usually a neural network layer)</li>
                                <li><strong>Sparse Activation:</strong> Only k out of n experts process each input</li>
                            </ul>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Ensemble Specialization:</strong> Different experts learn different patterns/domains</li>
                                <li><strong>Load Balancing:</strong> Ensure experts are used evenly (auxiliary loss)</li>
                                <li><strong>Conditional Computation:</strong> Adaptive routing based on input characteristics</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Sparsity means only a fraction of parameters active per input, allowing models with trillions of parameters to be computationally feasible. Experts can specialize in different sub-tasks.</p>
                            
                            <p><strong>Used For:</strong> GPT-4 (rumored), Switch Transformers, large-scale language models, multimodal models, scaling to extreme sizes</p>
                        </div>

                        <h3 id="nas">Neural Architecture Search (NAS)</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-search-plus me-2"></i>Neural Architecture Search</h4>
                            <p><strong>Core Idea:</strong> Automatically design neural network architectures by searching over possible configurations, optimizing for both accuracy and efficiency.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Uses <strong>optimization algorithms</strong> and <strong>search strategies</strong>:</p>
                            <ul>
                                <li><strong>Search Space:</strong> Define possible operations (conv, pooling, etc.) and connection patterns</li>
                                <li><strong>Search Strategy:</strong> RL, evolutionary algorithms, or gradient-based DARTS</li>
                                <li><strong>Performance Estimation:</strong> Predict accuracy without full training (early stopping, weight sharing)</li>
                            </ul>
                            
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Optimize: accuracy(architecture) - λ · cost(architecture)
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Multi-Objective Optimization:</strong> Trade-off accuracy, latency, model size, energy</li>
                                <li><strong>Hyperparameter Tuning:</strong> Architecture as hyperparameter space</li>
                                <li><strong>Transfer Learning:</strong> Architectures found on one task transfer to others</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> Automates tedious manual architecture design. Discovers novel patterns (e.g., depthwise separable convolutions rediscovered). Can optimize for hardware constraints.</p>
                            
                            <p><strong>Used For:</strong> EfficientNet, MobileNet, AutoML platforms, hardware-specific optimization, discovering SOTA architectures</p>
                        </div>

                        <h3 id="agentic-ai">Agentic AI (Tool-Using Agents)</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-robot me-2"></i>Agentic AI</h4>
                            <p><strong>Core Idea:</strong> Language models that can use external tools (search, calculators, APIs) and perform multi-step reasoning to accomplish complex tasks autonomously.</p>
                            
                            <p><strong>Mathematical Foundation:</strong></p>
                            <p>Combines <strong>planning</strong>, <strong>tool use</strong>, and <strong>iterative refinement</strong>:</p>
                            <ol>
                                <li><strong>Task Decomposition:</strong> Break complex goal into sub-tasks</li>
                                <li><strong>Tool Selection:</strong> Choose appropriate tools for each sub-task</li>
                                <li><strong>Execution & Feedback:</strong> Run tool, observe result, adjust plan</li>
                                <li><strong>Synthesis:</strong> Combine results into final answer</li>
                            </ol>
                            
                            <p style="text-align: center; font-family: monospace; background: white; padding: 1rem; border-radius: 4px; margin: 1rem 0;">
                                Agent Loop: Observe → Plan → Act → Reflect → Repeat
                            </p>
                            
                            <p><strong>Statistical Foundation:</strong></p>
                            <ul>
                                <li><strong>Hierarchical Planning:</strong> Decompose into decision trees or DAGs</li>
                                <li><strong>Reward Modeling:</strong> Learn which tool sequences succeed</li>
                                <li><strong>Few-Shot Learning:</strong> Tool use demonstrated via prompting examples</li>
                            </ul>
                            
                            <p><strong>Why It Works:</strong> LLMs provide reasoning backbone, tools provide grounding and capabilities beyond text (calculations, web search, code execution). Iterative feedback enables error correction.</p>
                            
                            <p><strong>Used For:</strong> LangChain, AutoGPT, coding assistants (Copilot), research agents, task automation, customer support bots with database access</p>
                        </div>

                        <h3 id="multi-agent">Multi-Agent Learning</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-users me-2"></i>Multi-Agent Learning</h4>
                            <p><strong>Core Idea:</strong> Multiple agents learn simultaneously in shared environment, coordinating or competing to achieve goals.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Game theory, Nash equilibria, centralized training with decentralized execution (CTDE)</p>
                            
                            <p><strong>Statistical Foundation:</strong> Nash Q-learning, mean field approximation, cooperative (QMIX) and competitive (zero-sum games) setups</p>
                            
                            <p><strong>Used For:</strong> Cooperative agents (rescue robots), autonomous vehicles (traffic), game AI (Dota, StarCraft), economic simulations, swarm robotics</p>
                        </div>

                        <!-- LEARNING PARADIGMS -->
                        <h2 style="margin-top: 3rem;">Learning Paradigms</h2>
                        
                        <h3 id="transfer-learning">Transfer Learning</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-exchange-alt me-2"></i>Transfer Learning</h4>
                            <p><strong>Core Idea:</strong> Leverage knowledge learned from one task/domain to improve performance on a related task with limited data.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Feature extraction from pre-trained models (frozen layers), fine-tuning (gradient descent on subset of parameters), domain adaptation</p>
                            
                            <p><strong>Statistical Foundation:</strong> Prior knowledge incorporation, Bayesian priors from source task, distribution shift between source and target domains</p>
                            
                            <p><strong>Why It Works:</strong> Early layers learn general features (edges, textures in vision; syntax in NLP) that transfer across tasks, while later layers specialize. Pre-training on large datasets provides better weight initialization than random.</p>
                            
                            <p><strong>Used For:</strong> Computer vision (ImageNet pre-training → medical imaging), NLP (BERT/GPT fine-tuning → specific tasks), low-resource domains, reducing training time/data requirements</p>
                        </div>

                        <h3 id="few-shot">Few-Shot Learning</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-database me-2"></i>Few-Shot Learning</h4>
                            <p><strong>Core Idea:</strong> Train models to classify new categories with only a few labeled examples per class (1-shot, 5-shot, etc.).</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Metric learning (learn embedding space where similar classes cluster), prototypical networks (class prototypes = mean embeddings), matching networks, Siamese networks</p>
                            
                            <p><strong>Statistical Foundation:</strong> Meta-learning over task distributions, episodic training (sample N-way K-shot episodes), distance metrics in learned feature space</p>
                            
                            <p><strong>Why It Works:</strong> Instead of learning parameters for each class, learn a similarity function or embedding space that generalizes. Meta-training on many few-shot tasks teaches the model how to learn from limited examples.</p>
                            
                            <p><strong>Used For:</strong> Rare disease diagnosis (limited patient data), new product categorization (e-commerce), personalization, wildlife species identification, GPT-3/4 in-context learning</p>
                        </div>

                        <h3 id="meta-learning">Meta-Learning</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-layer-group me-2"></i>Meta-Learning (Learning to Learn)</h4>
                            <p><strong>Core Idea:</strong> Train models to adapt quickly to new tasks with minimal data by learning optimal learning strategies.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Bi-level optimization (outer loop over tasks, inner loop within task), MAML, prototypical networks</p>
                            
                            <p><strong>Statistical Foundation:</strong> Bayesian adaptation, transfer learning across task distributions, learning priors that generalize</p>
                            
                            <p><strong>Used For:</strong> Few-shot learning, rapid adaptation, personalization, robot learning (new environments), drug discovery</p>
                        </div>

                        <h3 id="in-context">In-Context Learning</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-magic me-2"></i>In-Context Learning</h4>
                            <p><strong>Core Idea:</strong> Large language models learn new tasks from examples provided in the prompt without parameter updates.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Sequence modeling, conditional probability P(answer | examples, question) via autoregressive LMs</p>
                            
                            <p><strong>Statistical Foundation:</strong> Bayesian interpretation (infer latent task), implicit meta-learning during pre-training, attention patterns (induction heads)</p>
                            
                            <p><strong>Used For:</strong> Prompt engineering, GPT-3/4 applications, task specification without fine-tuning, rapid prototyping, instruction following</p>
                        </div>

                        <h3 id="continual">Continual Learning</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-infinity me-2"></i>Continual Learning (Lifelong Learning)</h4>
                            <p><strong>Core Idea:</strong> Learn sequence of tasks without forgetting previous ones (avoid catastrophic forgetting).</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Optimization constraints (EWC - Elastic Weight Consolidation), regularization, replay, or architecture expansion</p>
                            
                            <p><strong>Statistical Foundation:</strong> Fisher information for parameter importance, distribution shift handling, Bayesian posterior updates</p>
                            
                            <p><strong>Used For:</strong> Lifelong AI agents, robots learning continuously, personalized models, adaptive systems, online learning scenarios</p>
                        </div>

                        <h3 id="causal">Causal Machine Learning</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-project-diagram me-2"></i>Causal Machine Learning</h4>
                            <p><strong>Core Idea:</strong> Move beyond correlation to understand cause-and-effect relationships, enabling robust predictions under interventions.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Causal graphs (DAGs), do-calculus, structural causal models, counterfactual reasoning</p>
                            
                            <p><strong>Statistical Foundation:</strong> Counterfactuals, confounding adjustment, instrumental variables, propensity score matching</p>
                            
                            <p><strong>Used For:</strong> Treatment effect estimation (medicine, economics), policy decisions, root cause analysis, fair ML, robust prediction under distribution shift</p>
                        </div>

                        <h3 id="adversarial">Adversarial Training</h3>
                        <div class="technique-card" style="border: 2px solid var(--color-teal);">
                            <h4><i class="fas fa-shield-alt me-2"></i>Adversarial Training</h4>
                            <p><strong>Core Idea:</strong> Train models to be robust against adversarial examples by including perturbed inputs in training data.</p>
                            
                            <p><strong>Mathematical Foundation:</strong> Min-max optimization: min_θ max_δ L(f_θ(x+δ), y), FGSM, PGD attacks</p>
                            
                            <p><strong>Statistical Foundation:</strong> Robust statistics, minimax theorem, certified robustness via convex relaxations</p>
                            
                            <p><strong>Used For:</strong> AI safety, robust image classification, security-critical applications, defending against attacks, improving generalization</p>
                        </div>

                        <!-- Conclusion -->
                        <h2 id="conclusion">Conclusion</h2>
                        <p>Machine learning has evolved from simple statistical methods to complex AI systems that power everything from search engines to autonomous agents. But at its core, every technique relies on fundamental mathematical and statistical principles—optimization, probability, linear algebra, and calculus.</p>
                        
                        <p>Understanding these foundations doesn't just help you implement algorithms; it enables you to:</p>
                        <ul>
                            <li><strong>Choose the right tool</strong> for your problem by understanding what each technique optimizes for</li>
                            <li><strong>Debug models</strong> when they don't work as expected</li>
                            <li><strong>Innovate</strong> by combining techniques in novel ways</li>
                            <li><strong>Stay current</strong> as new architectures emerge—they're usually variations on these core principles</li>
                        </ul>

                        <p>Whether you're working with classical ML on tabular data or building the next generation of AI agents, the mathematical foundations remain your most powerful tool for understanding and advancing the field.</p>

                        <!-- Related Posts -->
                        <div class="related-posts">
                            <h3><i class="fas fa-link me-2"></i>Related Reading</h3>
                            <div class="related-post-item">
                                <a href="../12/python-data-science-numpy-foundations.html">Python Data Science Series Part 1: NumPy Foundations</a>
                                <p class="mb-0 small mt-2">Master the fundamental library for numerical computing that powers ML implementations.</p>
                            </div>
                            <div class="related-post-item">
                                <a href="../12/python-data-science-machine-learning.html">Python Data Science Series Part 4: Machine Learning with Scikit-learn</a>
                                <p class="mb-0 small mt-2">Learn practical implementations of ML algorithms with hands-on Python code examples.</p>
                            </div>
                            <div class="related-post-item">
                                <a href="../../11/discovery-of-humans-evolution.html">Discovery of Human Evolution: How Science Uncovers Our Origins</a>
                                <p class="mb-0 small mt-2">Explore the evolution of data science and the journey from classical statistics to modern AI.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">
                        I'm always interested in discussing machine learning, AI, and the mathematical foundations behind intelligent systems. Feel free to reach out!
                    </p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook">
                            <i class="fab fa-facebook-f"></i>
                        </a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter">
                            <i class="fab fa-twitter"></i>
                        </a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn">
                            <i class="fab fa-linkedin-in"></i>
                        </a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube">
                            <i class="fab fa-youtube"></i>
                        </a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram">
                            <i class="fab fa-instagram"></i>
                        </a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest">
                            <i class="fab fa-pinterest-p"></i>
                        </a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email">
                            <i class="fas fa-envelope"></i>
                        </a>
                    </div>
                </div>
            </div>

            <hr class="bg-secondary">

            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small mb-2">
                        <i class="fas fa-icons me-2"></i>Icons from <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a>
                    </p>
                    <p class="small">
                        <a href="/" class="text-light text-decoration-none">Home</a> | 
                        <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | 
                        <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a>
                    </p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">
                        Published by <strong>Wasil Zafar</strong> | <time>January 15, 2026</time>
                    </p>
                </div>
            </div>
        </div>
    </footer>

    <!-- Bootstrap 5 JS Bundle -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Scroll-to-Top Button -->
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top">
        <i class="fas fa-arrow-up"></i>
    </button>


    <!-- GDPR Cookie Consent -->
    <script src="../../../js/cookie-consent.js"></script>

    <!-- Custom Scripts -->
    <script src="../../../js/main.js"></script>
    

    <!-- Scroll-to-Top Script -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const scrollToTopBtn = document.getElementById('scrollToTop');
            
            // Show/hide button on scroll
            window.addEventListener('scroll', function() {
                if (window.scrollY > 300) {
                    scrollToTopBtn.classList.add('show');
                } else {
                    scrollToTopBtn.classList.remove('show');
                }
            });
            
            // Smooth scroll to top on click
            scrollToTopBtn.addEventListener('click', function() {
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        });
    </script>
</body>
</html>
