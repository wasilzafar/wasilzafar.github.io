<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Complete beginner's guide to the Attention Is All You Need paper. Understand Transformers, self-attention, multi-head attention, and why this 2017 paper revolutionized AI." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="Attention Is All You Need, Transformer, Self-Attention, Multi-Head Attention, NLP, Deep Learning, Neural Networks, BERT, GPT, Encoder Decoder, Positional Encoding, Machine Translation" />
    <meta property="og:title" content="Attention Is All You Need: The Paper That Revolutionized AI - A Complete Beginner's Guide" />
    <meta property="og:description" content="Understand the Transformer architecture from scratch. Learn self-attention, multi-head attention, positional encoding, and why this paper changed everything in AI." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-02" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>Attention Is All You Need: The Paper That Revolutionized AI - Wasil Zafar</title>

    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />

    <!-- Custom Styles -->
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />

    <!-- Prism.js Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />

    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <!-- Google Consent Mode v2 -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        
        gtag('consent', 'default', {
            'ad_storage': 'denied',
            'ad_user_data': 'denied',
            'ad_personalization': 'denied',
            'analytics_storage': 'denied',
            'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE']
        });
        
        gtag('consent', 'default', {
            'ad_storage': 'granted',
            'ad_user_data': 'granted',
            'ad_personalization': 'granted',
            'analytics_storage': 'granted'
        });
        
        gtag('set', 'url_passthrough', true);
    </script>

    <!-- Google Tag Manager -->
    <script>
        (function(w, d, s, l, i) {
            w[l] = w[l] || [];
            w[l].push({
                'gtm.start': new Date().getTime(),
                event: 'gtm.js'
            });
            var f = d.getElementsByTagName(s)[0],
                j = d.createElement(s),
                dl = l != 'dataLayer' ? '&l=' + l : '';
            j.async = true;
            j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    <style>
        /* Blog Post Specific Styles */
        .blog-hero {
            background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%);
            color: white;
            padding: 80px 0;
        }

        .blog-header {
            margin-bottom: 2rem;
        }

        .blog-meta {
            font-size: 0.95rem;
            color: var(--color-teal);
            margin-bottom: 1rem;
            display: flex;
            align-items: center;
            flex-wrap: wrap;
            gap: 1rem;
        }

        .blog-meta span {
            margin-right: 0.5rem;
        }

        .print-btn {
            background: var(--color-teal);
            color: white;
            border: none;
            padding: 0.4rem 1rem;
            border-radius: 4px;
            font-size: 0.9rem;
            cursor: pointer;
            transition: all 0.3s ease;
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
        }

        .print-btn:hover {
            background: var(--color-crimson);
            transform: translateY(-1px);
        }

        @media print {
            /* Hide print button and navigation */
            .print-btn,
            nav,
            .navbar,
            footer,
            .back-link,
            .related-posts,
            .scroll-to-top,
            .toc-toggle-btn,
            .sidenav-toc,
            .sidenav-overlay { 
                display: none !important; 
            }
            
            /* Force color printing */
            * {
                -webkit-print-color-adjust: exact !important;
                print-color-adjust: exact !important;
                color-adjust: exact !important;
            }
            
            /* Preserve header colors */
            .blog-content h2 {
                color: var(--color-navy) !important;
                border-bottom: 3px solid var(--color-teal) !important;
                page-break-after: avoid;
            }
            
            .blog-content h3 {
                color: var(--color-blue) !important;
                page-break-after: avoid;
            }
            
            .blog-content h4 {
                color: var(--color-crimson) !important;
                page-break-after: avoid;
            }
            
            /* Preserve strong text color */
            .blog-content strong {
                color: var(--color-crimson) !important;
            }
            
            /* Preserve highlight boxes */
            .highlight-box {
                background: rgba(59, 151, 151, 0.1) !important;
                border-left: 4px solid var(--color-teal) !important;
                page-break-inside: avoid;
            }
            
            /* Preserve experiment cards */
            .experiment-card {
                border: 1px solid #ddd !important;
                page-break-inside: avoid;
            }
            
            .experiment-card h4 {
                color: var(--color-crimson) !important;
            }
            
            /* Preserve badges */
            .badge {
                -webkit-print-color-adjust: exact !important;
                print-color-adjust: exact !important;
            }
            
            .bg-teal {
                background-color: var(--color-teal) !important;
                color: white !important;
            }
            
            .bg-crimson {
                background-color: var(--color-crimson) !important;
                color: white !important;
            }
            
            /* Preserve TOC box */
            .toc-box {
                border: 2px solid var(--color-teal) !important;
                page-break-inside: avoid;
            }
            
            .toc-box h3 {
                color: var(--color-navy) !important;
            }
            
            .toc-box a {
                color: var(--color-blue) !important;
            }
            
            /* Code blocks */
            pre[class*="language-"] {
                page-break-inside: avoid;
                border: 1px solid #ddd !important;
            }
            
            /* Reading time badge */
            .reading-time {
                background: var(--color-crimson) !important;
                color: white !important;
            }
            
            /* Page breaks */
            .blog-content h2 {
                page-break-before: auto;
            }
            
            /* Ensure good spacing */
            body {
                font-size: 12pt;
                line-height: 1.6;
            }
        }

        .blog-content {
            font-size: 1.05rem;
            line-height: 1.8;
            color: #333;
            text-align: justify;
        }

        .blog-content h2 {
            font-size: 1.8rem;
            font-weight: 700;
            margin-top: 2.5rem;
            margin-bottom: 1.5rem;
            color: var(--color-navy);
            border-bottom: 3px solid var(--color-teal);
            padding-bottom: 0.5rem;
        }

        .blog-content h3 {
            font-size: 1.3rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: var(--color-blue);
        }

        .blog-content h4 {
            font-size: 1.1rem;
            font-weight: 600;
            margin-top: 1.5rem;
            margin-bottom: 0.8rem;
            color: var(--color-navy);
        }

        .blog-content p {
            margin-bottom: 1.2rem;
        }

        .blog-content strong {
            color: var(--color-crimson);
        }

        .highlight-box {
            background: rgba(59, 151, 151, 0.1);
            border-left: 4px solid var(--color-teal);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 4px;
        }

        .experiment-card {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: all 0.3s ease;
        }

        .experiment-card:hover {
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
            transform: translateY(-2px);
        }

        .experiment-card h4 {
            color: var(--color-crimson);
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        .card-meta {
            font-size: 0.9rem;
            color: var(--color-blue);
            margin-bottom: 1rem;
            font-style: italic;
        }

        .card-content {
            color: #333;
        }

        .card-tags {
            margin-top: 1rem;
        }

        .bias-tag {
            display: inline-block;
            background: var(--color-teal);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 20px;
            font-size: 0.85rem;
            margin-right: 0.5rem;
            margin-bottom: 0.5rem;
        }

        /* Side Navigation Table of Contents (Modern Overlay Style) */
        /* Toggle Button */
        .toc-toggle-btn {
            position: fixed;
            bottom: 2rem;
            left: 2rem;
            width: 50px;
            height: 50px;
            background: var(--color-teal);
            color: white;
            border: none;
            border-radius: 50%;
            font-size: 1.2rem;
            cursor: pointer;
            box-shadow: 0 4px 12px rgba(59, 151, 151, 0.4);
            transition: all 0.3s ease;
            z-index: 1049;
            display: flex;
            align-items: center;
            justify-content: center;
        }

        .toc-toggle-btn:hover {
            background: var(--color-crimson);
            transform: scale(1.1);
            box-shadow: 0 6px 16px rgba(191, 9, 47, 0.5);
        }

        .toc-toggle-btn:active {
            transform: scale(0.95);
        }

        /* Side Navigation Overlay */
        .sidenav-toc {
            height: calc(100% - 64px);
            width: 0;
            position: fixed;
            z-index: 1050;
            top: 64px;
            left: 0;
            background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%);
            overflow-x: hidden;
            overflow-y: auto;
            transition: width 0.4s ease;
            padding-top: 30px;
            box-shadow: 4px 0 15px rgba(0, 0, 0, 0.3);
        }

        .sidenav-toc.open {
            width: 350px;
        }

        /* Header row with close button and title */
        .sidenav-toc .toc-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 20px 30px;
            margin-bottom: 20px;
            border-bottom: 2px solid var(--color-teal);
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
        }

        .sidenav-toc.open .toc-header {
            opacity: 1;
            visibility: visible;
        }

        .sidenav-toc .closebtn {
            font-size: 32px;
            color: white;
            background: transparent;
            border: none;
            cursor: pointer;
            transition: all 0.3s ease;
            line-height: 1;
            padding: 0;
            margin: 0;
        }

        .sidenav-toc .closebtn:hover {
            color: var(--color-crimson);
            transform: rotate(90deg);
        }

        .sidenav-toc h3 {
            color: white;
            margin: 0;
            padding: 0;
            font-weight: 700;
            font-size: 1.3rem;
            flex-grow: 1;
        }

        .sidenav-toc ol {
            list-style: decimal;
            padding: 0;
            padding-left: 30px;
            margin: 0;
            color: rgba(255, 255, 255, 0.9);
        }

        .sidenav-toc ol li {
            margin: 0;
            margin-bottom: 8px;
        }

        .sidenav-toc ul {
            list-style-type: lower-alpha;
            padding-left: 30px;
            margin-top: 8px;
            margin-bottom: 8px;
        }

        .sidenav-toc ul li {
            margin-bottom: 6px;
        }

        .sidenav-toc a {
            padding: 12px 30px;
            text-decoration: none;
            font-size: 0.95rem;
            color: rgba(255, 255, 255, 0.85);
            display: block;
            transition: all 0.3s ease;
            border-left: 4px solid transparent;
            position: relative;
        }

        .sidenav-toc a:hover {
            color: white;
            background: rgba(59, 151, 151, 0.2);
            border-left-color: var(--color-teal);
            padding-left: 35px;
        }

        .sidenav-toc a.active {
            color: white;
            background: rgba(191, 9, 47, 0.3);
            border-left-color: var(--color-crimson);
            font-weight: 600;
        }

        .sidenav-toc a.active::before {
            content: '▶';
            position: absolute;
            left: 15px;
            font-size: 0.7rem;
            color: var(--color-crimson);
        }

        /* Overlay backdrop */
        .sidenav-overlay {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: 1049;
            transition: opacity 0.4s ease;
        }

        .sidenav-overlay.show {
            display: block;
        }

        /* Responsive adjustments */
        @media (max-width: 768px) {
            .sidenav-toc.open {
                width: 280px;
            }
            
            .toc-toggle-btn {
                width: 50px;
                height: 50px;
                font-size: 1.2rem;
                left: 15px;
            }
        }

        /* Smooth scroll behavior */
        html {
            scroll-behavior: smooth;
        }

        .reading-time {
            display: inline-block;
            background: var(--color-crimson);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 4px;
            font-size: 0.9rem;
            margin-left: 0.5rem;
        }

        .back-link {
            display: inline-block;
            color: white;
            text-decoration: none;
            transition: all 0.3s ease;
            margin-bottom: 1rem;
            opacity: 0.9;
        }

        .back-link:hover {
            color: var(--color-teal);
            opacity: 1;
            transform: translateX(-5px);
        }

        .related-posts {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 2rem;
            margin-top: 3rem;
        }

        .related-posts h3 {
            color: var(--color-navy);
            margin-bottom: 1.5rem;
        }

        .related-post-item {
            padding: 1rem;
            border-left: 3px solid var(--color-teal);
            margin-bottom: 1rem;
            transition: all 0.3s ease;
            background: white;
        }

        .related-post-item:hover {
            border-left-color: var(--color-crimson);
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .related-post-item h5 {
            color: var(--color-navy);
            margin-bottom: 0.5rem;
        }

        .related-post-item a {
            color: var(--color-blue);
            text-decoration: none;
            font-weight: 600;
        }

        .related-post-item a:hover {
            color: var(--color-crimson);
        }

        /* Code Block Styles */
        pre[class*="language-"] {
            position: relative;
            margin: 1.5rem 0;
            padding-top: 3rem;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
        }

        code[class*="language-"] {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        /* Inline code */
        .blog-content code:not([class*="language-"]) {
            background: rgba(59, 151, 151, 0.15);
            color: var(--color-navy);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-size: 0.9em;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
        }

        /* Math equations styling */
        .math-block {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            text-align: center;
            font-family: 'Times New Roman', serif;
            font-size: 1.2rem;
            overflow-x: auto;
        }

        /* Toolbar styling */
        div.code-toolbar > .toolbar {
            opacity: 1;
            display: flex;
            gap: 0.5rem;
        }

        div.code-toolbar > .toolbar > .toolbar-item > button {
            background: var(--color-teal);
            color: white;
            border: none;
            padding: 0.4rem 0.8rem;
            border-radius: 4px;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        div.code-toolbar > .toolbar > .toolbar-item > button:hover {
            background: var(--color-blue);
            transform: translateY(-1px);
        }

        div.code-toolbar > .toolbar > .toolbar-item > button:focus {
            outline: 2px solid var(--color-teal);
            outline-offset: 2px;
        }

        /* Theme switcher dropdown */
        div.code-toolbar > .toolbar > .toolbar-item > select {
            background: var(--color-navy);
            color: white;
            border: 1px solid var(--color-teal);
            padding: 0.4rem 0.8rem;
            border-radius: 4px;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.3s ease;
            outline: none;
        }

        div.code-toolbar > .toolbar > .toolbar-item > select:hover {
            background: var(--color-blue);
            border-color: var(--color-crimson);
        }

        div.code-toolbar > .toolbar > .toolbar-item > select:focus {
            outline: 2px solid var(--color-teal);
            outline-offset: 2px;
        }

        /* Style select options */
        div.code-toolbar > .toolbar > .toolbar-item > select option {
            background: var(--color-navy);
            color: white;
        }

        /* Scroll-to-Top Button */
        .scroll-to-top {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            background: var(--color-teal);
            color: white;
            border: none;
            border-radius: 50%;
            font-size: 1.2rem;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            box-shadow: 0 4px 12px rgba(59, 151, 151, 0.3);
            z-index: 999;
        }

        .scroll-to-top.show {
            opacity: 1;
            visibility: visible;
        }

        .scroll-to-top:hover {
            background: var(--color-crimson);
            transform: translateY(-3px);
            box-shadow: 0 6px 16px rgba(191, 9, 47, 0.4);
        }

        .scroll-to-top:active {
            transform: translateY(-1px);
        }

        /* Paper citation box */
        .paper-citation {
            background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 8px;
            margin: 2rem 0;
        }

        .paper-citation h4 {
            color: var(--color-teal);
            margin-bottom: 1rem;
        }

        .paper-citation p {
            margin-bottom: 0.5rem;
            font-size: 0.95rem;
        }

        .paper-citation a {
            color: var(--color-teal);
        }

        @media (max-width: 768px) {
            .scroll-to-top {
                bottom: 1rem;
                right: 1rem;
                width: 45px;
                height: 45px;
                font-size: 1rem;
            }
        }
        /* Category Indicator */
        .category-indicator {
            position: fixed;
            bottom: 2rem;
            right: 6.5rem;
            background: var(--color-navy);
            color: white;
            padding: 0.75rem 1.25rem;
            border-radius: 25px;
            font-size: 0.9rem;
            font-weight: 600;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            box-shadow: 0 4px 12px rgba(19, 36, 64, 0.3);
            z-index: 998;
            max-width: 150px;
            text-align: center;
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
        }

        .category-indicator.show {
            opacity: 1;
            visibility: visible;
        }

        .category-indicator i {
            margin-right: 0.5rem;
            color: var(--color-teal);
        }

        @media (max-width: 768px) {
            .category-indicator {
                display: none;
            }
        }
    </style>
</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript>
        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe>
    </noscript>

    <!-- GDPR Cookie Consent Banner -->
    <div id="cookieBanner" class="light display-bottom" style="display: none;">
        <div id="closeIcon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
                <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"></path>
            </svg>
        </div>
        
        <div class="content-wrap">
            <div class="msg-wrap">
                <div class="title-wrap">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="20" height="20">
                        <path fill="#3B9797" d="M510.52 255.82c-69.97-.85-126.47-57.69-126.47-127.86-70.17 0-127-56.49-127.86-126.45-27.26-4.14-55.13.3-79.72 12.82l-69.13 35.22a132.221 132.221 0 0 0-57.79 57.81l-35.1 68.88a132.645 132.645 0 0 0-12.82 80.95l12.08 76.27a132.521 132.521 0 0 0 37.16 70.37l54.64 54.64a132.036 132.036 0 0 0 70.37 37.16l76.27 12.15c27.51 4.36 55.7-.11 80.95-12.8l68.88-35.08a132.166 132.166 0 0 0 57.79-57.81l35.1-68.88c12.56-24.64 17.01-52.58 12.91-79.91zM176 368c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32zm32-160c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32zm160 128c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32z"></path>
                    </svg>
                    <h4 style="margin: 0; font-size: 18px; color: var(--color-navy); font-weight: 700;">Cookie Consent</h4>
                </div>
                <p style="font-size: 14px; line-height: 1.6; color: var(--color-navy); margin-bottom: 15px;">
                    We use cookies to enhance your browsing experience, serve personalized content, and analyze our traffic. 
                    By clicking "Accept All", you consent to our use of cookies. See our 
                    <a href="/privacy-policy.html" style="color: var(--color-teal); border-bottom: 1px dotted var(--color-teal);">Privacy Policy</a> 
                    for more information.
                </p>
                
                <div id="cookieSettings" style="display: none;">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="14" height="14">
                        <path fill="currentColor" d="M487.4 315.7l-42.6-24.6c4.3-23.2 4.3-47 0-70.2l42.6-24.6c4.9-2.8 7.1-8.6 5.5-14-11.1-35.6-30-67.8-54.7-94.6-3.8-4.1-10-5.1-14.8-2.3L380.8 110c-17.9-15.4-38.5-27.3-60.8-35.1V25.8c0-5.6-3.9-10.5-9.4-11.7-36.7-8.2-74.3-7.8-109.2 0-5.5 1.2-9.4 6.1-9.4 11.7V75c-22.2 7.9-42.8 19.8-60.8 35.1L88.7 85.5c-4.9-2.8-11-1.9-14.8 2.3-24.7 26.7-43.6 58.9-54.7 94.6-1.7 5.4.6 11.2 5.5 14L67.3 221c-4.3 23.2-4.3 47 0 70.2l-42.6 24.6c-4.9 2.8-7.1 8.6-5.5 14 11.1 35.6 30 67.8 54.7 94.6 3.8 4.1 10 5.1 14.8 2.3l42.6-24.6c17.9 15.4 38.5 27.3 60.8 35.1v49.2c0 5.6 3.9 10.5 9.4 11.7 36.7 8.2 74.3 7.8 109.2 0 5.5-1.2 9.4-6.1 9.4-11.7v-49.2c22.2-7.9 42.8-19.8 60.8-35.1l42.6 24.6c4.9 2.8 11 1.9 14.8-2.3 24.7-26.7 43.6-58.9 54.7-94.6 1.5-5.5-.7-11.3-5.6-14.1zM256 336c-44.1 0-80-35.9-80-80s35.9-80 80-80 80 35.9 80 80-35.9 80-80 80z"></path>
                    </svg>
                    <span style="margin-left: 5px; font-size: 12px; font-weight: 600; color: var(--color-navy);">Customize Settings</span>
                </div>
                
                <div id="cookieTypes" style="display: none; margin-top: 15px; padding-top: 15px; border-top: 1px solid rgba(59, 151, 151, 0.2);">
                    <h5 style="font-size: 12px; font-weight: 700; color: var(--color-navy); margin-bottom: 10px; text-transform: uppercase;">Cookie Preferences</h5>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" checked disabled style="margin-top: 2px; margin-right: 8px; cursor: not-allowed;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Essential Cookies (Required)</strong>
                                <span style="font-size: 12px; color: #666;">Necessary for the website to function properly.</span>
                            </div>
                        </label>
                    </div>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" id="analyticsCookies" checked style="margin-top: 2px; margin-right: 8px;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Analytics Cookies</strong>
                                <span style="font-size: 12px; color: #666;">Help us understand how you interact with the website.</span>
                            </div>
                        </label>
                    </div>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" id="marketingCookies" style="margin-top: 2px; margin-right: 8px;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Marketing Cookies</strong>
                                <span style="font-size: 12px; color: #666;">Used to deliver relevant advertisements.</span>
                            </div>
                        </label>
                    </div>
                </div>
            </div>
            
            <div class="btn-wrap">
                <button id="cookieAccept" style="background: var(--color-teal); color: white; font-weight: 600;">Accept All</button>
                <button id="cookieReject" style="background: transparent; color: var(--color-navy); border: 2px solid var(--color-teal); font-weight: 600;">Reject All</button>
                <button id="cookieSave" style="background: var(--color-blue); color: white; font-weight: 600; display: none;">Save Preferences</button>
            </div>
        </div>
    </div>

    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/">
                <span class="gradient-text">Wasil Zafar</span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="/">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#about">About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#skills">Skills</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#certifications">Certifications</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#interests">Interests</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Blog Hero Section -->
    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link">
                    <i class="fas fa-arrow-left me-2"></i>Back to Technology
                </a>
                <h1 class="display-4 fw-bold mb-3">
                    Attention Is All You Need: The Paper That Revolutionized AI
                </h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 2, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-2"></i>32 min read</span>
                    <button onclick="window.print()" class="print-btn" title="Print this article">
                        <i class="fas fa-print"></i> Print
                    </button>
                </div>
                <p class="lead">A complete beginner's guide to understanding the Transformer architecture. Learn how self-attention, multi-head attention, and positional encoding work—and why this 2017 paper changed everything in AI, from BERT and GPT to modern LLMs.</p>
            </div>
        </div>
    </section>

    <!-- Main Content -->
    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="blog-content">

                        <!-- Paper Citation Box -->
                        <div class="paper-citation">
                            <h4><i class="fas fa-file-alt me-2"></i>Original Paper</h4>
                            <p><strong>Title:</strong> Attention Is All You Need</p>
                            <p><strong>Authors:</strong> Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin</p>
                            <p><strong>Published:</strong> December 6, 2017 at NIPS 2017</p>
                            <p><strong>Link:</strong> <a href="https://arxiv.org/abs/1706.03762" target="_blank">arXiv:1706.03762</a></p>
                        </div>
                        
                        <!-- Table of Contents Toggle Button -->
                        <button class="toc-toggle-btn" onclick="openNav()" title="Table of Contents" aria-label="Open Table of Contents">
                            <i class="fas fa-list"></i>
                        </button>

                        <!-- Side Navigation Overlay -->
                        <div id="tocSidenav" class="sidenav-toc">
                            <div class="toc-header">
                                <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
                                <button class="closebtn" onclick="closeNav()" aria-label="Close Table of Contents">&times;</button>
                            </div>
                            <ol>
                                <li><a href="#introduction" onclick="closeNav()">Introduction: Why This Paper Matters</a></li>
                                <li><a href="#problem" onclick="closeNav()">The Problem: Limitations of RNNs and CNNs</a>
                                    <ul>
                                        <li><a href="#rnn-limitations" onclick="closeNav()">Why RNNs Struggle</a></li>
                                        <li><a href="#cnn-limitations" onclick="closeNav()">Why CNNs Aren't Ideal</a></li>
                                        <li><a href="#attention-solution" onclick="closeNav()">The Attention Solution</a></li>
                                    </ul>
                                </li>
                                <li><a href="#architecture" onclick="closeNav()">The Transformer Architecture</a>
                                    <ul>
                                        <li><a href="#encoder" onclick="closeNav()">The Encoder Stack</a></li>
                                        <li><a href="#decoder" onclick="closeNav()">The Decoder Stack</a></li>
                                    </ul>
                                </li>
                                <li><a href="#attention-mechanism" onclick="closeNav()">The Attention Mechanism Explained</a>
                                    <ul>
                                        <li><a href="#qkv" onclick="closeNav()">Query, Key, Value: The Core Concepts</a></li>
                                        <li><a href="#scaled-dot-product" onclick="closeNav()">Scaled Dot-Product Attention</a></li>
                                        <li><a href="#multi-head" onclick="closeNav()">Multi-Head Attention</a></li>
                                        <li><a href="#attention-types" onclick="closeNav()">Three Types of Attention in Transformers</a></li>
                                    </ul>
                                </li>
                                <li><a href="#ffn" onclick="closeNav()">Position-Wise Feed-Forward Networks</a></li>
                                <li><a href="#positional-encoding" onclick="closeNav()">Positional Encoding: Injecting Order</a></li>
                                <li><a href="#embeddings" onclick="closeNav()">Embeddings and Weight Sharing</a></li>
                                <li><a href="#why-better" onclick="closeNav()">Why Self-Attention Is Better</a></li>
                                <li><a href="#training" onclick="closeNav()">Training the Transformer</a></li>
                                <li><a href="#results" onclick="closeNav()">Results and Breakthroughs</a></li>
                                <li><a href="#ablation" onclick="closeNav()">Ablation Studies: What Really Matters</a></li>
                                <li><a href="#impact" onclick="closeNav()">The Impact: From BERT to GPT to Modern AI</a></li>
                                <li><a href="#implementing" onclick="closeNav()">Implementing Attention in Code</a></li>
                                <li><a href="#conclusion" onclick="closeNav()">Conclusion</a></li>
                            </ol>
                        </div>

                        <!-- Overlay Backdrop -->
                        <div id="tocOverlay" class="sidenav-overlay" onclick="closeNav()"></div>

                        <!-- Section 1: Introduction -->
                        <h2 id="introduction"><i class="fas fa-lightbulb me-2"></i>1. Introduction: Why This Paper Matters</h2>
                        
                        <p>In December 2017, a team of researchers from Google Brain and Google Research published a paper that would fundamentally change the trajectory of artificial intelligence. The paper, titled <strong>"Attention Is All You Need,"</strong> introduced the <strong>Transformer architecture</strong>—a new way of processing sequences that abandoned the recurrent and convolutional approaches that had dominated the field for years.</p>
                        
                        <p>The Transformer's impact cannot be overstated. It is the foundation for:</p>
                        
                        <ul>
                            <li><strong>GPT</strong> (Generative Pre-trained Transformer) — OpenAI's language models</li>
                            <li><strong>BERT</strong> (Bidirectional Encoder Representations from Transformers) — Google's breakthrough in NLP</li>
                            <li><strong>T5, LLaMA, Claude, Gemini</strong> — Modern large language models</li>
                            <li><strong>Vision Transformers (ViT)</strong> — Transformers applied to images</li>
                            <li><strong>DALL-E, Stable Diffusion</strong> — Text-to-image generation models</li>
                        </ul>
                        
                        <div class="highlight-box">
                            <h4><i class="fas fa-key me-2"></i>The Core Idea</h4>
                            <p>The paper's revolutionary insight was simple yet profound: <strong>attention mechanisms alone are sufficient</strong> to build powerful sequence-to-sequence models. You don't need recurrence. You don't need convolutions. Attention is all you need.</p>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-map-signs me-2"></i>Complete Series Navigation</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">11-Part Series</span>
                                <span class="badge bg-crimson">Data Science Mastery</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><a href="../../series/python-data-science/python-setup-notebooks-guide.html">Python Setup & Notebooks</a> - IDE setup, Jupyter, virtual environments</li>
                                    <li><a href="../../series/python-data-science/python-data-science-numpy-foundations.html">NumPy Foundations</a> - Arrays, broadcasting, linear algebra</li>
                                    <li><a href="../../series/python-data-science/python-data-science-pandas-analysis.html">Pandas Data Analysis</a> - DataFrames, cleaning, manipulation</li>
                                    <li><a href="../../series/python-data-science/python-data-science-visualization.html">Data Visualization</a> - Matplotlib, Seaborn, Plotly</li>
                                    <li><a href="../../series/python-data-science/python-data-science-machine-learning.html">Machine Learning with Scikit-learn</a> - Classification, regression, clustering</li>
                                    <li><a href="machine-learning-mathematics-statistics-foundations.html">ML Mathematics & Statistics</a> - Linear algebra, calculus, probability</li>
                                    <li><a href="artificial-neural-networks-guide.html">Artificial Neural Networks</a> - Perceptrons, backpropagation, architectures</li>
                                    <li><a href="computer-vision-fundamentals-guide.html">Computer Vision Fundamentals</a> - CNNs, image processing, object detection</li>
                                    <li><a href="pytorch-deep-learning-guide.html">PyTorch Deep Learning</a> - Tensors, autograd, model training</li>
                                    <li><a href="tensorflow-deep-learning-guide.html">TensorFlow & Keras</a> - Sequential models, callbacks, deployment</li>
                                    <li><strong>Transformers & Attention (This Guide)</strong> - Self-attention, BERT, GPT architecture</li>
                                </ol>
                            </div>
                        </div>
                        
                        <p>Let's start with a simple Python example to understand what we mean by "attention":</p>
                        
<pre><code class="language-python">import numpy as np

# Simple intuition: Attention as weighted relevance
# Imagine you're reading: "The cat sat on the mat because it was tired"
# When processing "it", which words should we pay attention to?

sentence = ["The", "cat", "sat", "on", "the", "mat", "because", "it", "was", "tired"]
query_word = "it"  # We want to understand what "it" refers to

# Attention scores (how relevant is each word to "it"?)
# Higher score = more attention
attention_scores = {
    "The": 0.02,
    "cat": 0.45,      # High! "it" likely refers to "cat"
    "sat": 0.05,
    "on": 0.01,
    "the": 0.02,
    "mat": 0.15,      # Some attention - could "it" refer to mat?
    "because": 0.03,
    "it": 0.05,
    "was": 0.02,
    "tired": 0.20     # "tired" helps us know "it" = "cat" (not "mat")
}

print("Attention distribution for 'it':")
for word, score in attention_scores.items():
    bar = "█" * int(score * 50)
    print(f"  {word:10} {score:.2f} {bar}")

# Result: The model learns "it" refers to "cat" by attending to context
</code></pre>
                        
                        <p>This is the essence of attention: <strong>dynamically focusing on the most relevant parts</strong> of the input when processing each element. The Transformer takes this idea and builds an entire architecture around it.</p>
                        
                        <h3>What Made This Paper Special?</h3>
                        
                        <p>Before the Transformer, the dominant approaches for sequence modeling were:</p>
                        
                        <div class="comparison-table">
                            <table class="table table-bordered">
                                <thead>
                                    <tr>
                                        <th>Architecture</th>
                                        <th>How It Processes Sequences</th>
                                        <th>Main Limitation</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>RNNs/LSTMs</strong></td>
                                        <td>One token at a time, left to right</td>
                                        <td>Slow, forgets long-range context</td>
                                    </tr>
                                    <tr>
                                        <td><strong>CNNs</strong></td>
                                        <td>Fixed-size windows (kernels)</td>
                                        <td>Many layers needed for long-range</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Transformer</strong></td>
                                        <td>All tokens simultaneously via attention</td>
                                        <td>Memory scales with sequence length²</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        
                        <p>The Transformer achieved <strong>state-of-the-art results</strong> on machine translation while being <strong>significantly faster to train</strong> than RNN-based models. Let's understand why the old approaches had problems.</p>

                        <!-- Section 2: The Problem -->
                        <h2 id="problem"><i class="fas fa-exclamation-triangle me-2"></i>2. The Problem: Limitations of RNNs and CNNs</h2>
                        
                        <p>To appreciate what the Transformer solved, we need to understand the fundamental limitations of the architectures that came before it.</p>
                        
                        <h3 id="rnn-limitations">Why RNNs Struggle</h3>
                        
                        <p><strong>Recurrent Neural Networks (RNNs)</strong> process sequences one element at a time, maintaining a "hidden state" that carries information forward:</p>
                        
<pre><code class="language-python">import numpy as np

def simple_rnn_forward(sequence, hidden_size=64):
    """
    Simulates how an RNN processes a sequence.
    Key insight: Each step depends on the previous step.
    """
    seq_length = len(sequence)
    hidden_state = np.zeros(hidden_size)  # Start with zero state
    
    # RNNs process one token at a time - THIS IS THE BOTTLENECK
    processing_order = []
    for t in range(seq_length):
        token = sequence[t]
        processing_order.append(f"Step {t+1}: Process '{token}' using hidden state from step {t}")
        
        # In real RNN: hidden_state = tanh(W_h @ hidden_state + W_x @ token + b)
        # The new hidden state depends on the OLD hidden state
        
    return processing_order

sentence = ["The", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"]
steps = simple_rnn_forward(sentence)

print("RNN Sequential Processing (Cannot Be Parallelized!):")
print("-" * 60)
for step in steps:
    print(step)

print("\n⚠️ Problem: Step 9 must wait for steps 1-8 to complete!")
print("   This makes RNNs very slow on modern parallel hardware (GPUs).")
</code></pre>
                        
                        <div class="experiment-card">
                            <h4><i class="fas fa-flask me-2"></i>RNN's Vanishing Gradient Problem</h4>
                            <div class="card-meta">A fundamental challenge in learning long-range dependencies</div>
                            <div class="card-content">
                                <p>When an RNN tries to learn that a word at position 1 affects meaning at position 100, gradients must flow through 99 sequential steps during backpropagation. At each step, gradients get multiplied by weight matrices, causing them to either:</p>
                                <ul>
                                    <li><strong>Vanish</strong> (approach zero) — the model can't learn long-range patterns</li>
                                    <li><strong>Explode</strong> (grow huge) — training becomes unstable</li>
                                </ul>
                                <p>LSTMs and GRUs partially address this with gating mechanisms, but the sequential bottleneck remains.</p>
                            </div>
                            <div class="card-tags">
                                <span class="bias-tag">Sequential Processing</span>
                                <span class="bias-tag">Gradient Flow</span>
                                <span class="bias-tag">Training Bottleneck</span>
                            </div>
                        </div>
                        
<pre><code class="language-python">import numpy as np

def demonstrate_vanishing_gradient(sequence_length, weight_factor=0.9):
    """
    Shows how gradients vanish over long sequences.
    """
    # Simulate gradient flowing backward through time
    gradient = 1.0  # Start with gradient of 1
    gradients_over_time = [gradient]
    
    for t in range(sequence_length - 1):
        # At each step, gradient gets multiplied by weights
        # If weights < 1, gradient shrinks; if > 1, it explodes
        gradient = gradient * weight_factor
        gradients_over_time.append(gradient)
    
    return gradients_over_time

# Simulate 100-step sequence
gradients = demonstrate_vanishing_gradient(100, weight_factor=0.95)

print("Gradient Magnitude Over 100 Time Steps:")
print("-" * 50)
for t in [0, 10, 25, 50, 75, 99]:
    print(f"  Step {t:3d}: Gradient = {gradients[t]:.6f}")

print(f"\n📉 After 100 steps, gradient is only {gradients[-1]:.6f}")
print("   Almost zero! The model can't learn what happened 100 steps ago.")
</code></pre>
                        
                        <h3 id="cnn-limitations">Why CNNs Aren't Ideal for Sequences</h3>
                        
                        <p><strong>Convolutional Neural Networks (CNNs)</strong> process sequences using fixed-size windows (kernels). While they can parallelize better than RNNs, they have their own limitations:</p>
                        
<pre><code class="language-python">import numpy as np

def cnn_receptive_field(kernel_size=3, num_layers=1):
    """
    Calculate the receptive field (how far the model can "see") 
    based on CNN architecture.
    """
    # Receptive field grows linearly with layers for kernel_size=3
    # Position i can see positions [i - receptive_field, i + receptive_field]
    receptive_field = 1 + (kernel_size - 1) * num_layers
    return receptive_field

# How many CNN layers needed to connect word 1 to word 100?
sentence_length = 100
kernel_size = 3

print("CNN Layers Required for Full Sequence Coverage:")
print("-" * 50)

for target_coverage in [10, 25, 50, 100]:
    layers_needed = (target_coverage - 1) // (kernel_size - 1)
    print(f"  To connect words {target_coverage} positions apart: {layers_needed} layers")

print(f"\n⚠️ Problem: Need ~50 CNN layers for 100-word sentences!")
print("   Transformers connect ANY two positions in just 1 layer.")
</code></pre>
                        
                        <h3 id="attention-solution">The Attention Solution</h3>
                        
                        <p>The Transformer's key innovation is replacing sequential/local processing with <strong>global attention</strong>:</p>
                        
<pre><code class="language-python">import numpy as np

def compare_architectures(sequence_length):
    """
    Compare path lengths and parallelization across architectures.
    """
    results = {
        "RNN/LSTM": {
            "path_length": sequence_length,  # Must traverse entire sequence
            "parallel_ops": sequence_length,  # Process one at a time
            "description": "Sequential: O(n) steps, cannot parallelize"
        },
        "CNN (kernel=3)": {
            "path_length": np.log2(sequence_length),  # With dilated convolutions
            "parallel_ops": np.log2(sequence_length),  # Layers must be sequential
            "description": "Local windows: O(log n) layers needed"
        },
        "Transformer": {
            "path_length": 1,  # Direct attention between any two positions
            "parallel_ops": 1,  # All positions computed simultaneously
            "description": "Global attention: O(1) path length, fully parallel"
        }
    }
    
    return results

seq_len = 100
comparisons = compare_architectures(seq_len)

print(f"Architecture Comparison for Sequence Length = {seq_len}")
print("=" * 70)

for arch, stats in comparisons.items():
    print(f"\n{arch}:")
    print(f"  Path length (word 1 → word 100): {stats['path_length']:.1f}")
    print(f"  {stats['description']}")

print("\n🚀 Key Insight: Transformers have O(1) path length!")
print("   Any word can directly attend to any other word in a single step.")
</code></pre>
                        
                        <div class="highlight-box">
                            <h4><i class="fas fa-bolt me-2"></i>Why This Matters</h4>
                            <p>With constant path length, the Transformer can:</p>
                            <ul>
                                <li><strong>Learn long-range dependencies easily</strong> — no vanishing gradients over distance</li>
                                <li><strong>Train in parallel on GPUs</strong> — process all positions simultaneously</li>
                                <li><strong>Scale to very long sequences</strong> — just needs enough memory for attention matrix</li>
                            </ul>
                        </div>

                        <!-- Section 3: The Transformer Architecture -->
                        <h2 id="architecture"><i class="fas fa-cubes me-2"></i>3. The Transformer Architecture</h2>
                        
                        <p>The Transformer follows an <strong>encoder-decoder</strong> structure, common in sequence-to-sequence tasks like machine translation. Let's build up an understanding of its components.</p>
                        
<pre><code class="language-python">import numpy as np

# Transformer Architecture Overview
transformer_structure = """
┌─────────────────────────────────────────────────────────────────┐
│                    TRANSFORMER ARCHITECTURE                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│    INPUT SEQUENCE                        OUTPUT SEQUENCE         │
│    "The cat sat"                         "Le chat assis"         │
│         ↓                                      ↑                 │
│  ┌──────────────┐                    ┌──────────────┐           │
│  │   Embedding  │                    │   Embedding  │           │
│  │ + Positional │                    │ + Positional │           │
│  │   Encoding   │                    │   Encoding   │           │
│  └──────────────┘                    └──────────────┘           │
│         ↓                                      ↑                 │
│  ┌──────────────┐                    ┌──────────────┐           │
│  │   ENCODER    │ ──────────────────→│   DECODER    │           │
│  │   (6 layers) │    Cross-Attention │   (6 layers) │           │
│  └──────────────┘                    └──────────────┘           │
│         ↓                                      ↑                 │
│   Encoder Output                     ┌──────────────┐           │
│   (Contextual                        │    Linear    │           │
│    Representations)                  │   + Softmax  │           │
│                                      └──────────────┘           │
│                                            ↑                     │
│                                      Output Probabilities        │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
"""
print(transformer_structure)

# Key hyperparameters from the original paper
config = {
    "d_model": 512,       # Embedding dimension (model width)
    "n_heads": 8,         # Number of attention heads
    "d_k": 64,            # Key/Query dimension (512 / 8 = 64)
    "d_v": 64,            # Value dimension (512 / 8 = 64)
    "d_ff": 2048,         # Feed-forward hidden dimension
    "n_layers": 6,        # Number of encoder AND decoder layers
    "dropout": 0.1,       # Dropout rate
    "max_seq_len": 512    # Maximum sequence length
}

print("\nTransformer-Base Hyperparameters:")
print("-" * 40)
for param, value in config.items():
    print(f"  {param}: {value}")
</code></pre>
                        
                        <h3 id="encoder">The Encoder Stack</h3>
                        
                        <p>The encoder transforms the input sequence into a rich contextual representation. It consists of <strong>6 identical layers</strong>, each containing two sub-layers:</p>
                        
                        <ol>
                            <li><strong>Multi-Head Self-Attention</strong> — each position attends to all positions</li>
                            <li><strong>Position-wise Feed-Forward Network</strong> — applied to each position independently</li>
                        </ol>
                        
                        <p>Both sub-layers use <strong>residual connections</strong> and <strong>layer normalization</strong>:</p>
                        
<pre><code class="language-python">import numpy as np

def layer_norm(x, eps=1e-6):
    """
    Layer Normalization: Normalize across features (not batch).
    This stabilizes training and helps gradients flow.
    """
    mean = np.mean(x, axis=-1, keepdims=True)
    std = np.std(x, axis=-1, keepdims=True)
    return (x - mean) / (std + eps)

def encoder_sublayer(x, sublayer_fn, dropout_rate=0.1):
    """
    Each encoder sublayer follows this pattern:
    output = LayerNorm(x + Dropout(Sublayer(x)))
    
    This is the "Add & Norm" block you see in diagrams.
    """
    # 1. Apply the sublayer (attention or feed-forward)
    sublayer_output = sublayer_fn(x)
    
    # 2. Apply dropout (for regularization)
    # In practice: sublayer_output = dropout(sublayer_output, p=dropout_rate)
    
    # 3. Add residual connection (skip connection)
    residual_output = x + sublayer_output
    
    # 4. Apply layer normalization
    normalized_output = layer_norm(residual_output)
    
    return normalized_output

# Visualize encoder layer structure
encoder_layer_structure = """
┌───────────────────────────────────────────┐
│              ENCODER LAYER                 │
│                                            │
│   Input (seq_len × d_model)               │
│         ↓                                  │
│   ┌─────────────────────────────────────┐ │
│   │     Multi-Head Self-Attention       │ │
│   └─────────────────────────────────────┘ │
│         ↓                                  │
│      Dropout                               │
│         ↓                                  │
│   ────────────────────────────────────────│──→ + (Residual)
│         ↓                                  │
│   ┌─────────────────────────────────────┐ │
│   │        Layer Normalization          │ │
│   └─────────────────────────────────────┘ │
│         ↓                                  │
│   ┌─────────────────────────────────────┐ │
│   │   Position-wise Feed-Forward Net    │ │
│   └─────────────────────────────────────┘ │
│         ↓                                  │
│      Dropout                               │
│         ↓                                  │
│   ────────────────────────────────────────│──→ + (Residual)
│         ↓                                  │
│   ┌─────────────────────────────────────┐ │
│   │        Layer Normalization          │ │
│   └─────────────────────────────────────┘ │
│         ↓                                  │
│   Output (seq_len × d_model)              │
│                                            │
└───────────────────────────────────────────┘
"""
print(encoder_layer_structure)

print("Key Properties:")
print("  ✓ Input and output have the same shape: (seq_len, d_model)")
print("  ✓ Residual connections allow gradients to flow directly")
print("  ✓ Layer norm stabilizes training")
print("  ✓ 6 of these layers are stacked in the encoder")
</code></pre>
                        
                        <h3 id="decoder">The Decoder Stack</h3>
                        
                        <p>The decoder is similar to the encoder but has an extra attention layer. Each of its <strong>6 layers</strong> contains three sub-layers:</p>
                        
                        <ol>
                            <li><strong>Masked Multi-Head Self-Attention</strong> — attends only to previous positions (no peeking!)</li>
                            <li><strong>Multi-Head Cross-Attention</strong> — attends to the encoder's output</li>
                            <li><strong>Position-wise Feed-Forward Network</strong> — same as encoder</li>
                        </ol>
                        
<pre><code class="language-python">import numpy as np

def create_causal_mask(seq_len):
    """
    Creates a mask to prevent attending to future positions.
    This is crucial for autoregressive generation.
    
    Example for seq_len=5:
    [[1, 0, 0, 0, 0],   <- Position 0 can only see position 0
     [1, 1, 0, 0, 0],   <- Position 1 can see positions 0-1
     [1, 1, 1, 0, 0],   <- Position 2 can see positions 0-2
     [1, 1, 1, 1, 0],   <- Position 3 can see positions 0-3
     [1, 1, 1, 1, 1]]   <- Position 4 can see positions 0-4
    """
    # Create lower triangular matrix
    mask = np.tril(np.ones((seq_len, seq_len)))
    return mask

# Demonstrate the causal mask
seq_len = 5
mask = create_causal_mask(seq_len)

print("Causal (Look-Ahead) Mask:")
print("This prevents the decoder from 'cheating' by seeing future tokens.")
print("-" * 50)
print(mask.astype(int))
print()
print("Interpretation:")
print("  1 = CAN attend to this position")
print("  0 = CANNOT attend (future position, masked out)")

# Visualize decoder structure
decoder_layer_structure = """
┌───────────────────────────────────────────┐
│              DECODER LAYER                 │
│                                            │
│   Target Input (shifted right)            │
│         ↓                                  │
│   ┌─────────────────────────────────────┐ │
│   │   MASKED Multi-Head Self-Attention  │ │  ← Causal mask!
│   └─────────────────────────────────────┘ │
│         ↓ + Add & Norm                     │
│                                            │
│   ┌─────────────────────────────────────┐ │
│   │   Multi-Head Cross-Attention        │ │  ← Attends to ENCODER output
│   │   (Q from decoder, K/V from encoder)│ │
│   └─────────────────────────────────────┘ │
│         ↓ + Add & Norm                     │
│                                            │
│   ┌─────────────────────────────────────┐ │
│   │   Position-wise Feed-Forward Net    │ │
│   └─────────────────────────────────────┘ │
│         ↓ + Add & Norm                     │
│                                            │
│   Output (seq_len × d_model)              │
│                                            │
└───────────────────────────────────────────┘
"""
print(decoder_layer_structure)
</code></pre>
                        
                        <div class="experiment-card">
                            <h4><i class="fas fa-question-circle me-2"></i>Why Mask Future Positions?</h4>
                            <div class="card-meta">Understanding autoregressive generation</div>
                            <div class="card-content">
                                <p>During translation, the decoder generates output <strong>one token at a time</strong>:</p>
                                <ol>
                                    <li>Start with [START] token</li>
                                    <li>Predict first word (e.g., "Le")</li>
                                    <li>Predict second word given "Le" (e.g., "chat")</li>
                                    <li>Continue until [END] token is predicted</li>
                                </ol>
                                <p>If the decoder could see future tokens during training, it would simply copy the answer instead of learning to predict. The causal mask ensures <strong>training matches inference conditions</strong>.</p>
                            </div>
                            <div class="card-tags">
                                <span class="bias-tag">Autoregressive</span>
                                <span class="bias-tag">Teacher Forcing</span>
                                <span class="bias-tag">Causal Masking</span>
                            </div>
                        </div>

                        <!-- Section 4: Attention Mechanism Explained -->
                        <h2 id="attention-mechanism"><i class="fas fa-eye me-2"></i>4. The Attention Mechanism Explained</h2>
                        
                        <p>The attention mechanism is the heart of the Transformer. Let's break it down step by step, from intuition to implementation.</p>
                        
                        <h3 id="qkv">Query, Key, Value: The Core Concepts</h3>
                        
                        <p>Attention uses three components: <strong>Query (Q)</strong>, <strong>Key (K)</strong>, and <strong>Value (V)</strong>. Think of it like a database lookup:</p>
                        
<pre><code class="language-python">import numpy as np

# Intuition: Attention as a soft database lookup
database_analogy = """
┌─────────────────────────────────────────────────────────────────┐
│                  ATTENTION AS DATABASE LOOKUP                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Traditional Database:                                           │
│    SELECT value FROM table WHERE key = query                     │
│    → Returns ONE exact match                                     │
│                                                                  │
│  Attention Mechanism:                                            │
│    For each query, compute similarity to ALL keys               │
│    → Returns WEIGHTED COMBINATION of all values                  │
│    → Weights = softmax(similarity scores)                        │
│                                                                  │
│  Example (translating "The cat sat"):                            │
│    Query: Representation of current word being processed        │
│    Keys:  Representations of all input words                    │
│    Values: Information to aggregate from input words            │
│                                                                  │
│    The model learns: "What should I pay attention to?"          │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
"""
print(database_analogy)

# Simple example with actual vectors
np.random.seed(42)

# Embedding dimension (simplified)
d_model = 4

# Input: 3 words, each represented as a d_model-dimensional vector
# In reality, these come from embedding + positional encoding
words = ["cat", "sat", "mat"]
X = np.random.randn(3, d_model)

print("Input embeddings (3 words × 4 dimensions):")
for i, word in enumerate(words):
    print(f"  '{word}': {X[i].round(2)}")

# In self-attention, Q, K, V all come from the same input
# They're created by linear projections: Q = X @ W_Q, K = X @ W_K, V = X @ W_V
W_Q = np.random.randn(d_model, d_model) * 0.1
W_K = np.random.randn(d_model, d_model) * 0.1
W_V = np.random.randn(d_model, d_model) * 0.1

Q = X @ W_Q  # Queries: "What am I looking for?"
K = X @ W_K  # Keys: "What do I contain?"
V = X @ W_V  # Values: "What information do I provide?"

print("\nQ, K, V are learned transformations of the input:")
print(f"  Q shape: {Q.shape} (3 queries, one per word)")
print(f"  K shape: {K.shape} (3 keys, one per word)")
print(f"  V shape: {V.shape} (3 values, one per word)")
</code></pre>
                        
                        <h3 id="scaled-dot-product">Scaled Dot-Product Attention</h3>
                        
                        <p>This is the core attention computation. The formula from the paper is:</p>
                        
                        <div class="formula-box" style="background: #f8f9fa; padding: 1.5rem; border-radius: 8px; text-align: center; margin: 2rem 0; font-size: 1.3rem;">
                            <strong>Attention(Q, K, V) = softmax(QK<sup>T</sup> / √d<sub>k</sub>) V</strong>
                        </div>
                        
                        <p>Let's implement it step by step:</p>
                        
<pre><code class="language-python">import numpy as np

def softmax(x, axis=-1):
    """Numerically stable softmax."""
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Computes scaled dot-product attention.
    
    Args:
        Q: Queries of shape (seq_len, d_k)
        K: Keys of shape (seq_len, d_k)  
        V: Values of shape (seq_len, d_v)
        mask: Optional mask to prevent attending to certain positions
        
    Returns:
        attention_output: Weighted sum of values
        attention_weights: The attention distribution
    """
    d_k = K.shape[-1]
    
    # Step 1: Compute dot products between all queries and keys
    # Shape: (seq_len, seq_len) - each query's similarity to each key
    scores = Q @ K.T
    print(f"Step 1: Raw attention scores (Q @ K.T):\n{scores.round(2)}\n")
    
    # Step 2: Scale by sqrt(d_k) to prevent extremely large values
    # Large values → softmax becomes very peaked → gradients vanish
    scaled_scores = scores / np.sqrt(d_k)
    print(f"Step 2: Scaled scores (÷ √{d_k} = {np.sqrt(d_k):.2f}):\n{scaled_scores.round(2)}\n")
    
    # Step 3: Apply mask (optional - used in decoder)
    if mask is not None:
        # Set masked positions to -infinity so softmax gives 0
        scaled_scores = np.where(mask == 0, -1e9, scaled_scores)
        print(f"Step 3: After masking:\n{scaled_scores.round(2)}\n")
    
    # Step 4: Apply softmax to get attention weights (probabilities)
    attention_weights = softmax(scaled_scores, axis=-1)
    print(f"Step 4: Attention weights (softmax):\n{attention_weights.round(3)}\n")
    
    # Step 5: Weighted sum of values
    output = attention_weights @ V
    print(f"Step 5: Output (attention_weights @ V):\n{output.round(3)}")
    
    return output, attention_weights

# Example with 3 words
np.random.seed(42)
seq_len, d_k, d_v = 3, 4, 4

Q = np.random.randn(seq_len, d_k)
K = np.random.randn(seq_len, d_k)
V = np.random.randn(seq_len, d_v)

print("=" * 60)
print("SCALED DOT-PRODUCT ATTENTION - Step by Step")
print("=" * 60)
print(f"\nInput shapes: Q={Q.shape}, K={K.shape}, V={V.shape}\n")

output, weights = scaled_dot_product_attention(Q, K, V)

print("\n" + "=" * 60)
print("Interpretation of attention weights:")
print("-" * 40)
words = ["Word 0", "Word 1", "Word 2"]
for i in range(len(words)):
    print(f"\n{words[i]} attends to:")
    for j in range(len(words)):
        bar = "█" * int(weights[i, j] * 30)
        print(f"  {words[j]}: {weights[i, j]:.3f} {bar}")
</code></pre>
                        
                        <div class="highlight-box">
                            <h4><i class="fas fa-info-circle me-2"></i>Why Scale by √d<sub>k</sub>?</h4>
                            <p>When d<sub>k</sub> is large, the dot products Q·K can become very large in magnitude. Large values pushed through softmax result in extremely small gradients (the softmax output is either ~0 or ~1). Dividing by √d<sub>k</sub> keeps the variance of the dot products at a reasonable level, ensuring effective gradient flow during training.</p>
                        </div>
                        
                        <h3 id="multi-head">Multi-Head Attention</h3>
                        
                        <p>Instead of performing attention once with d<sub>model</sub>-dimensional keys, queries, and values, the Transformer performs it <strong>h times in parallel</strong> with different learned projections. This is called <strong>Multi-Head Attention</strong>.</p>
                        
<pre><code class="language-python">import numpy as np

def softmax(x, axis=-1):
    """Numerically stable softmax."""
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

def multi_head_attention(X, n_heads=8, d_model=512):
    """
    Multi-Head Attention mechanism.
    
    Instead of one attention function with d_model dimensions,
    use h attention heads with d_k = d_model/h dimensions each.
    
    Args:
        X: Input of shape (seq_len, d_model)
        n_heads: Number of attention heads (h)
        d_model: Model dimension
        
    Returns:
        output: (seq_len, d_model)
    """
    seq_len = X.shape[0]
    d_k = d_model // n_heads  # Each head gets d_model/h dimensions
    d_v = d_model // n_heads
    
    print(f"Multi-Head Attention Configuration:")
    print(f"  d_model = {d_model}")
    print(f"  n_heads = {n_heads}")
    print(f"  d_k = d_v = {d_model} / {n_heads} = {d_k}")
    print()
    
    # Initialize weights for all heads (in practice, learned during training)
    np.random.seed(42)
    W_Q = np.random.randn(n_heads, d_model, d_k) * 0.1  # (h, d_model, d_k)
    W_K = np.random.randn(n_heads, d_model, d_k) * 0.1
    W_V = np.random.randn(n_heads, d_model, d_v) * 0.1
    W_O = np.random.randn(n_heads * d_v, d_model) * 0.1  # Output projection
    
    head_outputs = []
    
    for head in range(n_heads):
        # Project input to Q, K, V for this head
        Q = X @ W_Q[head]  # (seq_len, d_k)
        K = X @ W_K[head]  # (seq_len, d_k)
        V = X @ W_V[head]  # (seq_len, d_v)
        
        # Scaled dot-product attention
        scores = (Q @ K.T) / np.sqrt(d_k)
        weights = softmax(scores, axis=-1)
        head_output = weights @ V  # (seq_len, d_v)
        
        head_outputs.append(head_output)
        
        if head < 2:  # Show first 2 heads as example
            print(f"Head {head + 1}:")
            print(f"  Q, K, V shapes: ({seq_len}, {d_k})")
            print(f"  Attention weights sample (word 0 → all):")
            print(f"    {weights[0].round(3)}")
    
    # Concatenate all heads: (seq_len, h * d_v) = (seq_len, d_model)
    concat = np.concatenate(head_outputs, axis=-1)
    print(f"\nConcatenated heads shape: {concat.shape}")
    
    # Final linear projection back to d_model
    output = concat @ W_O
    print(f"Output shape (after W_O): {output.shape}")
    
    return output

# Example
np.random.seed(42)
seq_len = 4
d_model = 512
n_heads = 8

X = np.random.randn(seq_len, d_model)
print("=" * 60)
print("MULTI-HEAD ATTENTION")
print("=" * 60)
print(f"Input shape: ({seq_len}, {d_model})\n")

output = multi_head_attention(X, n_heads=n_heads, d_model=d_model)

print("\n" + "=" * 60)
print("Why Multiple Heads?")
print("-" * 40)
print("""
Each head can learn to attend to different types of information:
  • Head 1: Might focus on syntactic relationships
  • Head 2: Might focus on semantic similarity  
  • Head 3: Might track coreference (pronouns → nouns)
  • Head 4: Might capture positional patterns
  ... and so on.

The concatenation combines all these perspectives!
""")
</code></pre>
                        
                        <h3 id="attention-types">Three Types of Attention in Transformers</h3>
                        
                        <p>The Transformer uses attention in three different ways:</p>
                        
<pre><code class="language-python">import numpy as np

attention_types = """
┌─────────────────────────────────────────────────────────────────┐
│           THREE TYPES OF ATTENTION IN TRANSFORMERS               │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  1. ENCODER SELF-ATTENTION                                       │
│     ─────────────────────                                        │
│     Location: Encoder layers                                     │
│     Q, K, V source: All from encoder input                      │
│     Purpose: Each input word attends to all input words         │
│     Masking: None (bidirectional)                               │
│                                                                  │
│     "The cat sat on the mat"                                     │
│      ↑↓  ↑↓  ↑↓  ↑↓ ↑↓  ↑↓                                      │
│      (every word sees every other word)                         │
│                                                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  2. MASKED DECODER SELF-ATTENTION                                │
│     ─────────────────────────────                                │
│     Location: First attention layer in decoder                  │
│     Q, K, V source: All from decoder input                      │
│     Purpose: Each output word attends to previous outputs       │
│     Masking: Causal (can't see future tokens)                   │
│                                                                  │
│     "Le chat" → predicting next word                            │
│      ↑↓  ↑↓                                                      │
│       ←──┘ (can only look left)                                 │
│                                                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  3. ENCODER-DECODER CROSS-ATTENTION                              │
│     ───────────────────────────────                              │
│     Location: Second attention layer in decoder                 │
│     Q source: Decoder (what am I looking for?)                  │
│     K, V source: Encoder output (what's available?)             │
│     Purpose: Output words attend to input words                 │
│     Masking: None (can attend to all encoder positions)         │
│                                                                  │
│     Decoder: "Le" → looking for context in "The cat sat..."    │
│              Queries come from "Le"                             │
│              Keys/Values come from encoded input                │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
"""
print(attention_types)

# Demonstrate cross-attention dimensions
def cross_attention_example():
    """Show how dimensions work in cross-attention."""
    # Encoder output (from encoding "The cat sat on the mat")
    encoder_seq_len = 6
    d_model = 512
    
    # Decoder current state (generating "Le chat assis")
    decoder_seq_len = 3
    
    print("Cross-Attention Dimensions:")
    print("-" * 40)
    print(f"Encoder output: ({encoder_seq_len}, {d_model})")
    print(f"Decoder state:  ({decoder_seq_len}, {d_model})")
    print()
    print("Projections:")
    print(f"  Q (from decoder): ({decoder_seq_len}, d_k)")
    print(f"  K (from encoder): ({encoder_seq_len}, d_k)")
    print(f"  V (from encoder): ({encoder_seq_len}, d_v)")
    print()
    print("Attention scores Q @ K.T:")
    print(f"  Shape: ({decoder_seq_len}, {encoder_seq_len})")
    print("  Each decoder position attends to all encoder positions!")
    print()
    print("Output after attention:")
    print(f"  Shape: ({decoder_seq_len}, d_v)")
    print("  Same as decoder sequence length")

cross_attention_example()
</code></pre>
                        
                        <div class="experiment-card">
                            <h4><i class="fas fa-lightbulb me-2"></i>Visualizing Attention Patterns</h4>
                            <div class="card-meta">What attention heads learn to focus on</div>
                            <div class="card-content">
                                <p>Researchers have visualized attention weights in trained Transformers and found that different heads learn different patterns:</p>
                                <ul>
                                    <li><strong>Position heads:</strong> Attend to adjacent words (next/previous)</li>
                                    <li><strong>Syntax heads:</strong> Connect verbs to subjects, adjectives to nouns</li>
                                    <li><strong>Coreference heads:</strong> Link pronouns to their referents</li>
                                    <li><strong>Rare word heads:</strong> Pay attention to uncommon but important tokens</li>
                                </ul>
                                <p>This division of labor emerges naturally during training!</p>
                            </div>
                            <div class="card-tags">
                                <span class="bias-tag">Interpretability</span>
                                <span class="bias-tag">Attention Visualization</span>
                                <span class="bias-tag">Emergent Behavior</span>
                            </div>
                        </div>

                        <!-- Section 5: Position-Wise Feed-Forward Networks -->
                        <h2 id="ffn"><i class="fas fa-layer-group me-2"></i>5. Position-Wise Feed-Forward Networks</h2>
                        
                        <p>After the attention mechanism in each encoder and decoder layer, there's a <strong>position-wise feed-forward network (FFN)</strong>. This is a simple two-layer neural network applied <strong>independently to each position</strong>.</p>
                        
                        <p>The formula from the paper:</p>
                        
                        <div class="formula-box" style="background: #f8f9fa; padding: 1.5rem; border-radius: 8px; text-align: center; margin: 2rem 0; font-size: 1.2rem;">
                            <strong>FFN(x) = max(0, xW<sub>1</sub> + b<sub>1</sub>)W<sub>2</sub> + b<sub>2</sub></strong>
                            <p style="font-size: 0.9rem; margin-top: 0.5rem; color: #666;">ReLU activation in the hidden layer</p>
                        </div>
                        
<pre><code class="language-python">import numpy as np

def relu(x):
    """ReLU activation: max(0, x)"""
    return np.maximum(0, x)

def position_wise_ffn(x, d_model=512, d_ff=2048):
    """
    Position-wise Feed-Forward Network.
    
    Applied to each position INDEPENDENTLY - no interaction between positions.
    This is where individual token representations get transformed.
    
    Args:
        x: Input of shape (seq_len, d_model)
        d_model: Model dimension (512 in base model)
        d_ff: Hidden layer dimension (2048 in base model = 4 * d_model)
    
    Returns:
        output: Same shape as input (seq_len, d_model)
    """
    seq_len = x.shape[0]
    
    # Initialize weights (in practice, learned during training)
    np.random.seed(42)
    W1 = np.random.randn(d_model, d_ff) * 0.02   # Expand: 512 → 2048
    b1 = np.zeros(d_ff)
    W2 = np.random.randn(d_ff, d_model) * 0.02   # Contract: 2048 → 512
    b2 = np.zeros(d_model)
    
    print(f"FFN Architecture:")
    print(f"  Input:  ({seq_len}, {d_model})")
    print(f"  Hidden: ({seq_len}, {d_ff})  ← Expand by 4x, apply ReLU")
    print(f"  Output: ({seq_len}, {d_model})  ← Contract back")
    print()
    
    # Forward pass
    # Step 1: Linear transformation + ReLU
    hidden = relu(x @ W1 + b1)
    
    # Step 2: Linear transformation (no activation)
    output = hidden @ W2 + b2
    
    # Count parameters
    params = d_model * d_ff + d_ff + d_ff * d_model + d_model
    print(f"Parameters in FFN: {params:,}")
    print(f"  W1: {d_model} × {d_ff} = {d_model * d_ff:,}")
    print(f"  b1: {d_ff:,}")
    print(f"  W2: {d_ff} × {d_model} = {d_ff * d_model:,}")
    print(f"  b2: {d_model:,}")
    
    return output

# Example
x = np.random.randn(10, 512)  # 10 tokens, 512 dimensions each

print("=" * 60)
print("POSITION-WISE FEED-FORWARD NETWORK")
print("=" * 60)
print()

output = position_wise_ffn(x, d_model=512, d_ff=2048)

print()
print("Key Insight:")
print("-" * 40)
print("""
The FFN is applied to each position SEPARATELY:
  • Position 0 gets its own FFN pass
  • Position 1 gets its own FFN pass
  • ... and so on

There's NO interaction between positions in the FFN.
All cross-position communication happens in ATTENTION.

Think of it as:
  • Attention: "What should I pay attention to?"
  • FFN: "How should I process what I gathered?"
""")
</code></pre>
                        
                        <div class="highlight-box">
                            <h4><i class="fas fa-question-circle me-2"></i>Why Expand to 4× the Dimension?</h4>
                            <p>The expansion from d<sub>model</sub>=512 to d<sub>ff</sub>=2048 creates a "bottleneck" architecture that:</p>
                            <ul>
                                <li><strong>Increases capacity:</strong> More parameters mean more expressive power</li>
                                <li><strong>Enables sparse activation:</strong> ReLU zeros out many hidden units, creating sparsity</li>
                                <li><strong>Balances compute:</strong> FFN accounts for ~2/3 of parameters but processes quickly</li>
                            </ul>
                        </div>
                        
<pre><code class="language-python">import numpy as np

def analyze_transformer_parameters():
    """
    Calculate parameter distribution in a Transformer layer.
    """
    # Transformer-Base configuration
    d_model = 512
    d_ff = 2048
    n_heads = 8
    d_k = d_v = 64  # d_model / n_heads
    
    # Multi-Head Attention parameters
    # Q, K, V projections: 3 matrices of (d_model × d_model) each
    attn_qkv = 3 * d_model * d_model
    # Output projection: (d_model × d_model)
    attn_out = d_model * d_model
    attn_total = attn_qkv + attn_out
    
    # Feed-Forward Network parameters
    ffn_w1 = d_model * d_ff
    ffn_w2 = d_ff * d_model
    ffn_total = ffn_w1 + ffn_w2
    
    # Layer normalization (2 per encoder layer)
    # Each has 2 × d_model parameters (scale and shift)
    layer_norm = 2 * (2 * d_model)
    
    total = attn_total + ffn_total + layer_norm
    
    print("Parameter Distribution in One Encoder Layer:")
    print("=" * 50)
    print(f"  Multi-Head Attention:  {attn_total:>10,} ({100*attn_total/total:.1f}%)")
    print(f"    - Q, K, V projections: {attn_qkv:,}")
    print(f"    - Output projection:   {attn_out:,}")
    print(f"  Feed-Forward Network:  {ffn_total:>10,} ({100*ffn_total/total:.1f}%)")
    print(f"    - W1 (expand):         {ffn_w1:,}")
    print(f"    - W2 (contract):       {ffn_w2:,}")
    print(f"  Layer Normalization:   {layer_norm:>10,} ({100*layer_norm/total:.1f}%)")
    print("-" * 50)
    print(f"  TOTAL per layer:       {total:>10,}")
    print(f"  TOTAL for 6 layers:    {6*total:>10,}")
    
analyze_transformer_parameters()
</code></pre>

                        <!-- Section 6: Positional Encoding -->
                        <h2 id="positional-encoding"><i class="fas fa-map-marker-alt me-2"></i>6. Positional Encoding: Injecting Order</h2>
                        
                        <p>Here's a critical question: <strong>How does the Transformer know word order?</strong></p>
                        
                        <p>Unlike RNNs which process words sequentially (inherently knowing order), or CNNs which have positional locality in their kernels, the Transformer's attention mechanism is <strong>permutation-invariant</strong>. Without additional information, it would treat "dog bites man" and "man bites dog" identically!</p>
                        
<pre><code class="language-python">import numpy as np

# The problem: Attention is permutation-invariant!
def attention_without_position():
    """
    Demonstrate that attention doesn't inherently know position.
    """
    np.random.seed(42)
    
    # Two sentences with same words, different order
    sentence1 = ["dog", "bites", "man"]
    sentence2 = ["man", "bites", "dog"]
    
    # Imagine these are word embeddings (same for same words)
    embeddings = {
        "dog": np.array([1.0, 0.2, -0.5]),
        "bites": np.array([0.3, 0.8, 0.1]),
        "man": np.array([0.9, 0.3, -0.4])
    }
    
    # Create embedding matrices
    X1 = np.array([embeddings[w] for w in sentence1])
    X2 = np.array([embeddings[w] for w in sentence2])
    
    print("Sentence 1:", " ".join(sentence1))
    print("Sentence 2:", " ".join(sentence2))
    print()
    print("Without positional encoding:")
    print("  X1 and X2 have the SAME attention patterns!")
    print("  The model can't distinguish between them.")
    print()
    print("  'Dog bites man' = 'Man bites dog' ← WRONG!")
    print()
    print("Solution: Add position information to embeddings!")

attention_without_position()
</code></pre>
                        
                        <p>The solution is <strong>positional encoding</strong>—adding position information to the input embeddings. The paper uses <strong>sinusoidal positional encodings</strong>:</p>
                        
                        <div class="formula-box" style="background: #f8f9fa; padding: 1.5rem; border-radius: 8px; text-align: center; margin: 2rem 0;">
                            <p style="margin-bottom: 0.5rem;"><strong>PE<sub>(pos, 2i)</sub> = sin(pos / 10000<sup>2i/d<sub>model</sub></sup>)</strong></p>
                            <p style="margin-bottom: 0;"><strong>PE<sub>(pos, 2i+1)</sub> = cos(pos / 10000<sup>2i/d<sub>model</sub></sup>)</strong></p>
                            <p style="font-size: 0.85rem; margin-top: 1rem; color: #666;">
                                pos = position in sequence (0, 1, 2, ...)<br>
                                i = dimension index (0, 1, ..., d<sub>model</sub>/2)
                            </p>
                        </div>
                        
<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt

def get_positional_encoding(max_seq_len, d_model):
    """
    Generate sinusoidal positional encodings.
    
    Args:
        max_seq_len: Maximum sequence length
        d_model: Embedding dimension
    
    Returns:
        PE: Positional encoding matrix of shape (max_seq_len, d_model)
    """
    PE = np.zeros((max_seq_len, d_model))
    
    for pos in range(max_seq_len):
        for i in range(0, d_model, 2):
            # Calculate the denominator: 10000^(2i/d_model)
            denominator = 10000 ** (i / d_model)
            
            # Even dimensions get sine
            PE[pos, i] = np.sin(pos / denominator)
            
            # Odd dimensions get cosine
            if i + 1 < d_model:
                PE[pos, i + 1] = np.cos(pos / denominator)
    
    return PE

# Generate positional encodings
max_seq_len = 100
d_model = 512

PE = get_positional_encoding(max_seq_len, d_model)

print("Positional Encoding Shape:", PE.shape)
print()
print("Sample encodings (first 8 dimensions):")
print("-" * 60)
for pos in [0, 1, 2, 10, 50]:
    encoding = PE[pos, :8].round(3)
    print(f"Position {pos:2d}: {encoding}")

print()
print("Key Properties:")
print("-" * 40)
print("1. Each position gets a unique encoding")
print("2. Encodings are bounded between -1 and 1")
print("3. Nearby positions have similar encodings")
print("4. The model can learn to attend to relative positions")
</code></pre>
                        
                        <div class="experiment-card">
                            <h4><i class="fas fa-wave-square me-2"></i>Why Sinusoidal Encodings?</h4>
                            <div class="card-meta">The elegant mathematics behind position encoding</div>
                            <div class="card-content">
                                <p>The authors chose sinusoidal functions for several clever reasons:</p>
                                <ul>
                                    <li><strong>Unique per position:</strong> Each position gets a distinct encoding vector</li>
                                    <li><strong>Bounded values:</strong> Sin and cos are always in [-1, 1], same scale as embeddings</li>
                                    <li><strong>Relative positions:</strong> PE<sub>pos+k</sub> can be represented as a linear function of PE<sub>pos</sub>, allowing the model to learn relative positioning</li>
                                    <li><strong>Extrapolation:</strong> Can handle sequences longer than seen during training</li>
                                </ul>
                                <p>Modern transformers often use <strong>learned positional embeddings</strong> instead, but sinusoidal encodings remain effective and require no training.</p>
                            </div>
                            <div class="card-tags">
                                <span class="bias-tag">Sinusoidal</span>
                                <span class="bias-tag">Relative Position</span>
                                <span class="bias-tag">Extrapolation</span>
                            </div>
                        </div>
                        
<pre><code class="language-python">import numpy as np

def demonstrate_relative_positions():
    """
    Show how sinusoidal encodings enable learning relative positions.
    
    Key insight: PE[pos+k] can be computed from PE[pos] using a linear transformation!
    This means the model can learn to attend to "3 positions back" regardless of absolute position.
    """
    d_model = 4  # Simplified for demonstration
    
    # For a fixed offset k, there exists a matrix M_k such that:
    # PE[pos + k] = PE[pos] @ M_k
    
    # This works because of trigonometric identities:
    # sin(a + b) = sin(a)cos(b) + cos(a)sin(b)
    # cos(a + b) = cos(a)cos(b) - sin(a)sin(b)
    
    print("Relative Position Property:")
    print("=" * 50)
    print()
    print("For any offset k, there exists a matrix M such that:")
    print("    PE[pos + k] = PE[pos] @ M")
    print()
    print("This means:")
    print("  • The model can learn 'look 3 words back'")
    print("  • This pattern works at ANY absolute position")
    print("  • Position 0 → 3 uses the same transformation as 10 → 13")
    print()
    
    # Demonstrate with actual values
    def get_pe(pos, d_model):
        pe = np.zeros(d_model)
        for i in range(0, d_model, 2):
            denom = 10000 ** (i / d_model)
            pe[i] = np.sin(pos / denom)
            if i + 1 < d_model:
                pe[i + 1] = np.cos(pos / denom)
        return pe
    
    # Compare distance between positions
    pe_0 = get_pe(0, 16)
    pe_1 = get_pe(1, 16)
    pe_10 = get_pe(10, 16)
    pe_11 = get_pe(11, 16)
    
    dist_0_to_1 = np.linalg.norm(pe_1 - pe_0)
    dist_10_to_11 = np.linalg.norm(pe_11 - pe_10)
    
    print("Distance Consistency:")
    print(f"  ||PE[1] - PE[0]|| = {dist_0_to_1:.4f}")
    print(f"  ||PE[11] - PE[10]|| = {dist_10_to_11:.4f}")
    print(f"  These are similar! The 'step' is consistent.")

demonstrate_relative_positions()
</code></pre>
                        
                        <h3>Adding Positional Encoding to Embeddings</h3>
                        
                        <p>The positional encoding is simply <strong>added</strong> to the word embeddings:</p>
                        
<pre><code class="language-python">import numpy as np

def create_transformer_input(tokens, vocab_embeddings, max_seq_len=512, d_model=512):
    """
    Create the input to the Transformer by combining word embeddings
    with positional encodings.
    
    Input = Embedding(token) + PositionalEncoding(position)
    """
    seq_len = len(tokens)
    
    # Step 1: Look up word embeddings
    word_embeddings = np.array([vocab_embeddings[token] for token in tokens])
    print(f"Word embeddings shape: {word_embeddings.shape}")
    
    # Step 2: Generate positional encodings
    PE = np.zeros((seq_len, d_model))
    for pos in range(seq_len):
        for i in range(0, d_model, 2):
            denom = 10000 ** (i / d_model)
            PE[pos, i] = np.sin(pos / denom)
            if i + 1 < d_model:
                PE[pos, i + 1] = np.cos(pos / denom)
    print(f"Positional encodings shape: {PE.shape}")
    
    # Step 3: Add them together (element-wise addition)
    transformer_input = word_embeddings + PE
    print(f"Combined input shape: {transformer_input.shape}")
    
    # Step 4: Scale embeddings (optional but common)
    # The paper multiplies embeddings by sqrt(d_model) before adding PE
    # This balances the magnitude of embeddings and positional encodings
    scaled_input = word_embeddings * np.sqrt(d_model) + PE
    
    return scaled_input

# Example with dummy embeddings
np.random.seed(42)
d_model = 8  # Simplified

# Pretend we have a vocabulary with learned embeddings
vocab = {"the": 0, "cat": 1, "sat": 2, "on": 3, "mat": 4}
embeddings = np.random.randn(len(vocab), d_model) * 0.1

# Map tokens to embeddings
tokens = ["the", "cat", "sat", "on", "the", "mat"]
vocab_embeddings = {word: embeddings[idx] for word, idx in vocab.items()}

print("Creating Transformer Input:")
print("=" * 50)
print(f"Tokens: {tokens}")
print(f"d_model: {d_model}")
print()

transformer_input = create_transformer_input(tokens, vocab_embeddings, d_model=d_model)

print()
print("Result: Each token now knows its position!")
print("The model can distinguish 'the' at position 0 from 'the' at position 4.")
</code></pre>

                        <!-- Section 7: Embeddings and Weight Sharing -->
                        <h2 id="embeddings"><i class="fas fa-link me-2"></i>7. Embeddings and Weight Sharing</h2>
                        
                        <p>The Transformer uses embeddings in three places:</p>
                        
                        <ol>
                            <li><strong>Input embedding:</strong> Convert source tokens to vectors (encoder input)</li>
                            <li><strong>Output embedding:</strong> Convert target tokens to vectors (decoder input)</li>
                            <li><strong>Pre-softmax linear:</strong> Convert decoder output to vocabulary logits</li>
                        </ol>
                        
                        <p>A key insight from the paper: <strong>these can share weights!</strong></p>
                        
<pre><code class="language-python">import numpy as np

def demonstrate_weight_sharing():
    """
    Explain the three embedding matrices and weight sharing.
    """
    structure = """
    ┌───────────────────────────────────────────────────────────────┐
    │                  EMBEDDING WEIGHT SHARING                      │
    ├───────────────────────────────────────────────────────────────┤
    │                                                                │
    │  Source Tokens ──→ [Input Embedding] ──→ Encoder               │
    │                          ↑                                     │
    │                          │ SHARED (optional)                   │
    │                          ↓                                     │
    │  Target Tokens ──→ [Output Embedding] ──→ Decoder              │
    │                          ↑                                     │
    │                          │ SHARED (transposed)                 │
    │                          ↓                                     │
    │  Decoder Output ──→ [Pre-Softmax Linear] ──→ Vocabulary Probs  │
    │                                                                │
    │  In the paper:                                                 │
    │    • Input/Output embeddings: SHARED (same matrix)             │
    │    • Output embed & Pre-softmax: SHARED (transposed)           │
    │                                                                │
    │  Benefits:                                                     │
    │    • Fewer parameters to train                                 │
    │    • Better generalization                                     │
    │    • Semantic consistency across encode/decode                 │
    │                                                                │
    └───────────────────────────────────────────────────────────────┘
    """
    print(structure)

demonstrate_weight_sharing()

def embedding_layer_example():
    """
    Show how embedding and output projection are related.
    """
    np.random.seed(42)
    
    vocab_size = 10000
    d_model = 512
    
    # The embedding matrix: (vocab_size, d_model)
    # Each row is the embedding vector for one token
    embedding_matrix = np.random.randn(vocab_size, d_model) * 0.02
    
    print("Embedding Matrix Shape:", embedding_matrix.shape)
    print()
    
    # Forward: token_id → embedding vector
    token_id = 42
    embedding = embedding_matrix[token_id]  # Look up row 42
    print(f"Forward (embed token {token_id}):")
    print(f"  Shape: {embedding.shape}")
    
    # Reverse: decoder_output → vocabulary logits
    # Use the SAME matrix, transposed!
    decoder_output = np.random.randn(d_model)  # Output from decoder
    
    # Multiply by embedding matrix transposed
    logits = decoder_output @ embedding_matrix.T  # (d_model,) @ (d_model, vocab) = (vocab,)
    
    print(f"\nReverse (output → logits):")
    print(f"  Decoder output shape: {decoder_output.shape}")
    print(f"  Logits shape: {logits.shape}")
    print(f"  This gives a score for each word in vocabulary!")
    
    # The word with highest logit is the prediction
    predicted_token = np.argmax(logits)
    print(f"\n  Predicted token ID: {predicted_token}")
    print(f"  (Apply softmax to get probabilities)")

print("Embedding Layer Example:")
print("=" * 50)
embedding_layer_example()
</code></pre>
                        
                        <div class="highlight-box">
                            <h4><i class="fas fa-compress-arrows-alt me-2"></i>Parameter Efficiency Through Sharing</h4>
                            <p>Weight sharing between embeddings significantly reduces parameters:</p>
                            <ul>
                                <li><strong>Without sharing:</strong> 3 × vocab_size × d_model parameters</li>
                                <li><strong>With sharing:</strong> 1 × vocab_size × d_model parameters</li>
                            </ul>
                            <p>For a 32K vocabulary and d_model=512: saving of ~33 million parameters!</p>
                        </div>

                        <!-- Section 8: Why Self-Attention Is Better -->
                        <h2 id="why-better"><i class="fas fa-trophy me-2"></i>8. Why Self-Attention Is Better</h2>
                        
                        <p>The paper dedicates an entire section to comparing self-attention with recurrent and convolutional layers. Let's understand the three key metrics they analyze:</p>
                        
                        <div class="comparison-table">
                            <table class="table table-bordered">
                                <thead>
                                    <tr>
                                        <th>Layer Type</th>
                                        <th>Complexity per Layer</th>
                                        <th>Sequential Operations</th>
                                        <th>Max Path Length</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Self-Attention</strong></td>
                                        <td>O(n² · d)</td>
                                        <td>O(1)</td>
                                        <td><span style="color: var(--color-teal); font-weight: bold;">O(1)</span></td>
                                    </tr>
                                    <tr>
                                        <td><strong>Recurrent (RNN)</strong></td>
                                        <td>O(n · d²)</td>
                                        <td>O(n)</td>
                                        <td>O(n)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Convolutional</strong></td>
                                        <td>O(k · n · d²)</td>
                                        <td>O(1)</td>
                                        <td>O(log<sub>k</sub>(n))</td>
                                    </tr>
                                </tbody>
                            </table>
                            <p class="small text-muted">n = sequence length, d = model dimension, k = kernel size</p>
                        </div>
                        
<pre><code class="language-python">import numpy as np

def compare_layer_properties(n, d, k=3):
    """
    Compare computational properties of different layer types.
    
    Args:
        n: Sequence length
        d: Model dimension  
        k: Kernel size (for CNN)
    """
    print(f"Layer Comparison for n={n}, d={d}, k={k}")
    print("=" * 70)
    
    # Complexity per layer (total operations)
    self_attn_complexity = n * n * d
    rnn_complexity = n * d * d
    cnn_complexity = k * n * d * d
    
    print("\n1. COMPUTATIONAL COMPLEXITY (operations per layer):")
    print("-" * 50)
    print(f"   Self-Attention: O(n²·d) = {self_attn_complexity:,}")
    print(f"   RNN:            O(n·d²) = {rnn_complexity:,}")
    print(f"   CNN:            O(k·n·d²) = {cnn_complexity:,}")
    
    if n < d:
        print(f"\n   When n < d (n={n}, d={d}): Self-attention is FASTER!")
    else:
        print(f"\n   When n > d (n={n}, d={d}): Self-attention is SLOWER per layer")
        print("   But wait... see parallelization and path length!")
    
    # Sequential operations (cannot be parallelized)
    print("\n2. SEQUENTIAL OPERATIONS (parallelization barrier):")
    print("-" * 50)
    print(f"   Self-Attention: O(1) → Fully parallelizable!")
    print(f"   RNN:            O(n) = {n} sequential steps")
    print(f"   CNN:            O(1) → Parallelizable within layer")
    
    print(f"\n   RNN must wait {n} steps. Transformers process all at once!")
    
    # Maximum path length (for learning long-range dependencies)
    print("\n3. MAXIMUM PATH LENGTH (for gradient flow):")
    print("-" * 50)
    print(f"   Self-Attention: O(1) = 1 step between ANY two positions!")
    print(f"   RNN:            O(n) = {n} steps worst case")
    print(f"   CNN:            O(log_k(n)) ≈ {int(np.ceil(np.log(n)/np.log(k)))} layers needed")
    
    print(f"\n   Gradients in RNN must flow through {n} time steps!")
    print("   In Transformers: direct connection, easy gradient flow.")

# Typical NLP scenario
compare_layer_properties(n=100, d=512, k=3)

print("\n" + "=" * 70)
print("KEY TAKEAWAY:")
print("-" * 50)
print("""
Self-Attention wins on:
  ✓ Parallelization: O(1) sequential ops vs O(n) for RNN
  ✓ Path length: O(1) direct connection vs O(n) for RNN  
  ✓ Learning long-range dependencies: gradients flow directly

Self-Attention's cost:
  ✗ Memory: O(n²) attention matrix can be large for long sequences
  
This is why modern LLMs use techniques like:
  • Sparse attention (only attend to some positions)
  • Linear attention (approximate full attention)
  • Sliding window attention (local + global tokens)
""")
</code></pre>
                        
                        <div class="experiment-card">
                            <h4><i class="fas fa-clock me-2"></i>Training Speed in Practice</h4>
                            <div class="card-meta">Real-world performance comparison from the paper</div>
                            <div class="card-content">
                                <p>The paper reports training times on 8 NVIDIA P100 GPUs:</p>
                                <table class="table table-sm">
                                    <tr>
                                        <td><strong>Transformer Base:</strong></td>
                                        <td>12 hours (100K steps)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Transformer Big:</strong></td>
                                        <td>3.5 days (300K steps)</td>
                                    </tr>
                                </table>
                                <p>Compared to RNN-based models that took <strong>weeks</strong> to train to similar quality, Transformers were a massive speedup—enabling rapid experimentation and iteration.</p>
                            </div>
                            <div class="card-tags">
                                <span class="bias-tag">Training Speed</span>
                                <span class="bias-tag">GPU Utilization</span>
                                <span class="bias-tag">Parallelization</span>
                            </div>
                        </div>
                        
<pre><code class="language-python">import numpy as np

def attention_as_interpretable_feature():
    """
    Another advantage: attention weights are interpretable!
    """
    print("BONUS: Self-Attention is Interpretable!")
    print("=" * 50)
    print()
    print("Unlike RNN hidden states, attention weights show")
    print("exactly what the model is 'looking at'.")
    print()
    
    # Simulated attention weights for translation
    source = ["The", "cat", "sat", "on", "the", "mat"]
    target = ["Le", "chat", "assis", "sur", "le", "tapis"]
    
    # Mock attention weights (what a trained model might produce)
    attention = np.array([
        [0.95, 0.02, 0.01, 0.01, 0.01, 0.00],  # "Le" → "The"
        [0.02, 0.90, 0.03, 0.02, 0.02, 0.01],  # "chat" → "cat"
        [0.01, 0.05, 0.85, 0.04, 0.03, 0.02],  # "assis" → "sat"
        [0.01, 0.02, 0.02, 0.90, 0.03, 0.02],  # "sur" → "on"
        [0.01, 0.01, 0.02, 0.03, 0.90, 0.03],  # "le" → "the"
        [0.01, 0.02, 0.01, 0.02, 0.04, 0.90],  # "tapis" → "mat"
    ])
    
    print("Cross-Attention Weights (Translation Example):")
    print("-" * 60)
    print(f"{'Target':<10} {'→ Source words (attention weights)'}")
    print("-" * 60)
    
    for i, target_word in enumerate(target):
        weights_str = "  ".join([f"{source[j]}:{attention[i,j]:.2f}" 
                                  for j in range(len(source))])
        max_idx = np.argmax(attention[i])
        print(f"{target_word:<10} → {source[max_idx]} ({attention[i, max_idx]:.0%})")
    
    print()
    print("We can SEE that 'chat' attends to 'cat', 'tapis' to 'mat', etc.")
    print("This interpretability is valuable for debugging and trust!")

attention_as_interpretable_feature()
</code></pre>

                        <!-- Section 9: Training the Transformer -->
                        <h2 id="training"><i class="fas fa-dumbbell me-2"></i>9. Training the Transformer</h2>
                        
                        <p>The paper provides detailed training procedures that were crucial for achieving state-of-the-art results. Let's examine the key components.</p>
                        
                        <h3>Training Data and Batching</h3>
                        
<pre><code class="language-python">import numpy as np

def training_configuration():
    """
    Training setup from the paper.
    """
    config = {
        "dataset_en_de": "WMT 2014 English-German (4.5M sentence pairs)",
        "dataset_en_fr": "WMT 2014 English-French (36M sentence pairs)", 
        "tokenization": "Byte-Pair Encoding (BPE)",
        "vocab_size_en_de": "37,000 shared tokens",
        "vocab_size_en_fr": "32,000 shared tokens",
        "batch_size": "~25,000 source + target tokens per batch",
        "hardware": "8 NVIDIA P100 GPUs",
        "training_steps_base": "100,000 steps (12 hours)",
        "training_steps_big": "300,000 steps (3.5 days)"
    }
    
    print("Training Configuration:")
    print("=" * 60)
    for key, value in config.items():
        print(f"  {key}: {value}")

training_configuration()
</code></pre>
                        
                        <h3>The Adam Optimizer with Warmup</h3>
                        
                        <p>One of the most important training innovations was the <strong>learning rate schedule with warmup</strong>:</p>
                        
                        <div class="formula-box" style="background: #f8f9fa; padding: 1.5rem; border-radius: 8px; text-align: center; margin: 2rem 0;">
                            <strong>lr = d<sub>model</sub><sup>-0.5</sup> · min(step<sup>-0.5</sup>, step · warmup_steps<sup>-1.5</sup>)</strong>
                            <p style="font-size: 0.85rem; margin-top: 1rem; color: #666;">
                                Linearly increase during warmup, then decay proportionally to 1/√step
                            </p>
                        </div>
                        
<pre><code class="language-python">import numpy as np

def transformer_learning_rate(step, d_model=512, warmup_steps=4000):
    """
    Calculate learning rate using the Transformer schedule.
    
    Two phases:
    1. Warmup (steps 1 to warmup_steps): Linear increase
    2. Decay (steps > warmup_steps): Proportional to 1/sqrt(step)
    """
    # The formula from the paper
    lr = (d_model ** -0.5) * min(step ** -0.5, step * (warmup_steps ** -1.5))
    return lr

# Visualize the schedule
steps = list(range(1, 50001))
lrs = [transformer_learning_rate(s) for s in steps]

print("Transformer Learning Rate Schedule:")
print("=" * 60)
print()

# Key points
peak_step = 4000
peak_lr = transformer_learning_rate(peak_step)
final_lr = transformer_learning_rate(50000)

print(f"Configuration: d_model=512, warmup_steps=4000")
print()
print("Learning rate at key points:")
print(f"  Step 1:      {transformer_learning_rate(1):.6f}")
print(f"  Step 1000:   {transformer_learning_rate(1000):.6f}")
print(f"  Step 4000:   {transformer_learning_rate(4000):.6f}  ← PEAK (end of warmup)")
print(f"  Step 10000:  {transformer_learning_rate(10000):.6f}")
print(f"  Step 50000:  {transformer_learning_rate(50000):.6f}")
print()
print("Phase 1 (Warmup): LR increases linearly from ~0 to peak")
print("Phase 2 (Decay):  LR decays as 1/√step")
print()
print("Why warmup?")
print("  • Early training: gradients are unstable, large LR causes divergence")
print("  • Warmup lets the model 'settle' before aggressive updates")
print("  • Essential for Transformer training stability!")
</code></pre>
                        
                        <div class="experiment-card">
                            <h4><i class="fas fa-fire me-2"></i>Why Warmup Is Critical</h4>
                            <div class="card-meta">Training stability in large models</div>
                            <div class="card-content">
                                <p>At the start of training:</p>
                                <ul>
                                    <li>Weights are randomly initialized</li>
                                    <li>Attention patterns are essentially random</li>
                                    <li>Gradients can be very large and noisy</li>
                                </ul>
                                <p>A high learning rate at this stage causes the model to <strong>diverge</strong> (loss explodes to infinity). Warmup keeps updates small until the model has learned reasonable attention patterns.</p>
                            </div>
                            <div class="card-tags">
                                <span class="bias-tag">Training Stability</span>
                                <span class="bias-tag">Gradient Variance</span>
                                <span class="bias-tag">Learning Rate</span>
                            </div>
                        </div>
                        
                        <h3>Regularization Techniques</h3>
                        
                        <p>The paper uses several regularization techniques:</p>
                        
<pre><code class="language-python">import numpy as np

def regularization_techniques():
    """
    Regularization methods used in the Transformer.
    """
    techniques = {
        "Dropout": {
            "rate": 0.1,
            "applied_to": [
                "After each sub-layer (attention, FFN)",
                "To embeddings + positional encodings",
                "Attention weights (in some implementations)"
            ],
            "purpose": "Prevent overfitting by randomly dropping activations"
        },
        "Label Smoothing": {
            "rate": 0.1,
            "description": "Instead of hard targets (0 or 1), use soft targets",
            "example": "True label: [0, 0, 1, 0] → Smoothed: [0.033, 0.033, 0.9, 0.033]",
            "purpose": "Prevents overconfident predictions, improves generalization"
        },
        "Residual Dropout": {
            "description": "Dropout applied before adding residual connection",
            "purpose": "Regularizes the skip connection pathway"
        }
    }
    
    print("Regularization Techniques in the Transformer:")
    print("=" * 60)
    
    for name, details in techniques.items():
        print(f"\n{name}:")
        for key, value in details.items():
            if isinstance(value, list):
                print(f"  {key}:")
                for item in value:
                    print(f"    • {item}")
            else:
                print(f"  {key}: {value}")

regularization_techniques()

def label_smoothing_example():
    """
    Demonstrate label smoothing.
    """
    print("\n" + "=" * 60)
    print("Label Smoothing Example:")
    print("-" * 40)
    
    vocab_size = 5
    true_label = 2  # The correct token is at index 2
    smoothing = 0.1
    
    # Hard target (standard one-hot)
    hard_target = np.zeros(vocab_size)
    hard_target[true_label] = 1.0
    
    # Soft target (label smoothing)
    soft_target = np.ones(vocab_size) * (smoothing / vocab_size)
    soft_target[true_label] = 1.0 - smoothing + (smoothing / vocab_size)
    
    print(f"Vocabulary size: {vocab_size}")
    print(f"True label index: {true_label}")
    print(f"Smoothing factor: {smoothing}")
    print()
    print(f"Hard target: {hard_target}")
    print(f"Soft target: {soft_target.round(3)}")
    print()
    print("The model learns to be less 'certain', improving calibration.")

label_smoothing_example()
</code></pre>

                        <!-- Section 10: Results and Breakthroughs -->
                        <h2 id="results"><i class="fas fa-chart-line me-2"></i>10. Results and Breakthroughs</h2>
                        
                        <p>The Transformer achieved <strong>state-of-the-art results</strong> on machine translation benchmarks while being dramatically faster to train.</p>
                        
                        <h3>BLEU Scores on Translation</h3>
                        
                        <div class="comparison-table">
                            <table class="table table-bordered">
                                <thead>
                                    <tr>
                                        <th>Model</th>
                                        <th>EN-DE BLEU</th>
                                        <th>EN-FR BLEU</th>
                                        <th>Training Cost</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td>Previous SOTA (GNMT)</td>
                                        <td>24.6</td>
                                        <td>39.9</td>
                                        <td>Higher</td>
                                    </tr>
                                    <tr>
                                        <td>Transformer Base</td>
                                        <td>27.3</td>
                                        <td>38.1</td>
                                        <td>~$50 (12 hours)</td>
                                    </tr>
                                    <tr style="background: rgba(59, 151, 151, 0.1);">
                                        <td><strong>Transformer Big</strong></td>
                                        <td><strong>28.4</strong></td>
                                        <td><strong>41.0</strong></td>
                                        <td>~$150 (3.5 days)</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>
                        
<pre><code class="language-python">import numpy as np

def analyze_results():
    """
    Analyze the paper's results on machine translation.
    """
    results = {
        "English-German (WMT 2014)": {
            "Previous SOTA": 24.6,
            "Transformer Base": 27.3,
            "Transformer Big": 28.4,
            "Improvement": "+3.8 BLEU"
        },
        "English-French (WMT 2014)": {
            "Previous SOTA": 39.9,
            "Transformer Base": 38.1,
            "Transformer Big": 41.0,
            "Improvement": "+1.1 BLEU"
        }
    }
    
    print("Machine Translation Results:")
    print("=" * 60)
    
    for task, scores in results.items():
        print(f"\n{task}:")
        for model, score in scores.items():
            print(f"  {model}: {score}")
    
    print()
    print("Key Observations:")
    print("-" * 40)
    print("1. Transformer Big achieved new SOTA on both benchmarks")
    print("2. Even Transformer Base was competitive with previous SOTA")
    print("3. Training cost was a FRACTION of RNN-based models")
    
analyze_results()

def training_efficiency():
    """
    Compare training efficiency.
    """
    print("\n" + "=" * 60)
    print("Training Efficiency Comparison:")
    print("-" * 40)
    
    models = {
        "GNMT (Google's RNN)": {
            "training_time": "6 days on 96 GPUs",
            "approx_gpu_hours": 6 * 24 * 96,  # ~13,824 GPU-hours
        },
        "ConvS2S (Facebook's CNN)": {
            "training_time": "Long training",
            "approx_gpu_hours": "~10,000 GPU-hours",
        },
        "Transformer Base": {
            "training_time": "12 hours on 8 GPUs",
            "approx_gpu_hours": 12 * 8,  # 96 GPU-hours
        },
        "Transformer Big": {
            "training_time": "3.5 days on 8 GPUs",
            "approx_gpu_hours": 3.5 * 24 * 8,  # 672 GPU-hours
        }
    }
    
    for model, details in models.items():
        print(f"\n{model}:")
        for key, value in details.items():
            print(f"  {key}: {value}")
    
    print()
    print("Transformer Base: ~100x more efficient than GNMT!")
    print("This enabled rapid experimentation and iteration.")

training_efficiency()
</code></pre>
                        
                        <div class="highlight-box">
                            <h4><i class="fas fa-tachometer-alt me-2"></i>The Efficiency Revolution</h4>
                            <p>The Transformer wasn't just better—it was <strong>dramatically more efficient</strong>:</p>
                            <ul>
                                <li><strong>100x fewer GPU-hours</strong> than previous SOTA</li>
                                <li><strong>Same quality</strong> with 1/100th the compute</li>
                                <li>Enabled <strong>rapid experimentation</strong> that led to BERT, GPT, etc.</li>
                            </ul>
                            <p>This efficiency gain was arguably as important as the quality improvements!</p>
                        </div>

                        <!-- Section 11: Ablation Studies -->
                        <h2 id="ablation"><i class="fas fa-flask me-2"></i>11. Ablation Studies: What Really Matters</h2>
                        
                        <p>The paper includes thorough ablation studies showing which components matter most. This is invaluable for understanding the architecture.</p>
                        
<pre><code class="language-python">import numpy as np

def ablation_studies():
    """
    Key findings from the paper's ablation studies.
    """
    print("Ablation Study Results:")
    print("=" * 60)
    
    ablations = [
        {
            "change": "Vary number of attention heads (h)",
            "findings": [
                "h=1: 24.9 BLEU (single head is worse)",
                "h=8: 25.4 BLEU (base model)",
                "h=16: 25.3 BLEU (diminishing returns)",
                "h=32: 25.0 BLEU (too many heads hurt)",
            ],
            "insight": "8 heads is optimal; too many or too few hurts"
        },
        {
            "change": "Vary attention key dimension (d_k)",
            "findings": [
                "d_k=32: 25.0 BLEU",
                "d_k=64: 25.4 BLEU (base)",
                "d_k=128: 25.3 BLEU",
            ],
            "insight": "d_k=64 is the sweet spot for base model"
        },
        {
            "change": "Model size (d_model)",
            "findings": [
                "Bigger is generally better",
                "But must balance with compute budget",
            ],
            "insight": "d_model=512 (base) vs 1024 (big)"
        },
        {
            "change": "Remove positional encoding",
            "findings": [
                "Significant performance drop",
                "Model cannot learn word order",
            ],
            "insight": "Positional encoding is ESSENTIAL"
        },
        {
            "change": "Replace learned embeddings with sinusoidal",
            "findings": [
                "Nearly identical performance",
            ],
            "insight": "Learned vs. sinusoidal: minimal difference"
        },
    ]
    
    for i, ablation in enumerate(ablations, 1):
        print(f"\n{i}. {ablation['change']}:")
        for finding in ablation['findings']:
            print(f"   • {finding}")
        print(f"   → Insight: {ablation['insight']}")

ablation_studies()
</code></pre>
                        
                        <div class="experiment-card">
                            <h4><i class="fas fa-search me-2"></i>Attention Head Specialization</h4>
                            <div class="card-meta">What different heads learn to do</div>
                            <div class="card-content">
                                <p>The ablation studies revealed that different attention heads in the same layer learn to focus on different things:</p>
                                <ul>
                                    <li><strong>Some heads:</strong> Track syntactic dependencies (subject-verb agreement)</li>
                                    <li><strong>Some heads:</strong> Attend to previous/next positions</li>
                                    <li><strong>Some heads:</strong> Focus on rare or semantically important words</li>
                                    <li><strong>Some heads:</strong> Handle long-range dependencies</li>
                                </ul>
                                <p>This <strong>emergent specialization</strong> is why multiple heads outperform a single large head.</p>
                            </div>
                            <div class="card-tags">
                                <span class="bias-tag">Emergent Behavior</span>
                                <span class="bias-tag">Specialization</span>
                                <span class="bias-tag">Multi-Head</span>
                            </div>
                        </div>
                        
<pre><code class="language-python">import numpy as np

def model_size_comparison():
    """
    Compare Transformer Base vs Big configurations.
    """
    base = {
        "d_model": 512,
        "d_ff": 2048,
        "n_heads": 8,
        "n_layers": 6,
        "d_k": 64,
        "dropout": 0.1,
        "params": "65M"
    }
    
    big = {
        "d_model": 1024,
        "d_ff": 4096,
        "n_heads": 16,
        "n_layers": 6,
        "d_k": 64,
        "dropout": 0.3,  # Higher dropout for bigger model
        "params": "213M"
    }
    
    print("Transformer Base vs Big:")
    print("=" * 60)
    print(f"{'Parameter':<15} {'Base':<15} {'Big':<15}")
    print("-" * 45)
    
    for key in base.keys():
        print(f"{key:<15} {str(base[key]):<15} {str(big[key]):<15}")
    
    print()
    print("Key differences:")
    print("  • Big has 2x model dimension")
    print("  • Big has 2x feed-forward dimension")
    print("  • Big has 2x attention heads (but same d_k!)")
    print("  • Big uses higher dropout (0.3 vs 0.1)")
    print("  • Big has ~3x the parameters")

model_size_comparison()
</code></pre>

                        <!-- Section 12: The Impact -->
                        <h2 id="impact"><i class="fas fa-rocket me-2"></i>12. The Impact: From BERT to GPT to Modern AI</h2>
                        
                        <p>The Transformer paper has become one of the most influential papers in AI history. Let's trace its impact.</p>
                        
<pre><code class="language-python">import numpy as np

def transformer_timeline():
    """
    Timeline of major developments following the Transformer.
    """
    timeline = """
    ┌─────────────────────────────────────────────────────────────────────┐
    │                    TRANSFORMER IMPACT TIMELINE                       │
    ├─────────────────────────────────────────────────────────────────────┤
    │                                                                      │
    │  2017 │ "Attention Is All You Need" published (Dec)                 │
    │       │                                                              │
    │  2018 │ GPT-1: Decoder-only Transformer for generation (Jun)        │
    │       │ BERT: Encoder-only for understanding (Oct)                  │
    │       │ → BERT revolutionizes NLP benchmarks                        │
    │       │                                                              │
    │  2019 │ GPT-2: Scaling up language models (Feb)                     │
    │       │ T5: Encoder-decoder for everything (Oct)                    │
    │       │ Transformer-XL: Long sequences                              │
    │       │                                                              │
    │  2020 │ GPT-3: 175B parameters, few-shot learning (May)             │
    │       │ Vision Transformer (ViT): Transformers for images (Oct)     │
    │       │ DALL-E: Text-to-image generation                            │
    │       │                                                              │
    │  2021 │ Codex: Code generation (GPT for code)                       │
    │       │ CLIP: Connecting vision and language                        │
    │       │                                                              │
    │  2022 │ ChatGPT: Conversational AI breakthrough (Nov)               │
    │       │ Stable Diffusion: Open-source image generation              │
    │       │                                                              │
    │  2023 │ GPT-4: Multimodal capabilities                              │
    │       │ LLaMA: Open-source LLMs                                     │
    │       │ Claude, Gemini: Competing frontier models                   │
    │       │                                                              │
    │  2024 │ Claude 3, GPT-4o, Gemini 1.5                                │
    │       │ Transformers dominate AI across ALL domains                 │
    │       │                                                              │
    │  2025+│ Multimodal reasoning, embodied AI, scientific discovery    │
    │       │                                                              │
    └─────────────────────────────────────────────────────────────────────┘
    """
    print(timeline)

transformer_timeline()
</code></pre>
                        
                        <h3>Three Flavors of Transformers</h3>
                        
<pre><code class="language-python">import numpy as np

def transformer_variants():
    """
    The three main ways to use Transformers.
    """
    variants = """
    ┌─────────────────────────────────────────────────────────────────────┐
    │               THREE TRANSFORMER ARCHITECTURES                        │
    ├─────────────────────────────────────────────────────────────────────┤
    │                                                                      │
    │  1. ENCODER-ONLY (BERT-style)                                       │
    │     ────────────────────────                                         │
    │     Uses: Only the encoder stack                                    │
    │     Attention: Bidirectional (see all tokens)                       │
    │     Best for: Understanding tasks                                   │
    │       • Text classification                                         │
    │       • Named entity recognition                                    │
    │       • Question answering (extractive)                             │
    │     Examples: BERT, RoBERTa, ALBERT, DeBERTa                        │
    │                                                                      │
    │  2. DECODER-ONLY (GPT-style)                                        │
    │     ────────────────────────                                         │
    │     Uses: Only the decoder stack                                    │
    │     Attention: Causal (see only past tokens)                        │
    │     Best for: Generation tasks                                      │
    │       • Text generation                                             │
    │       • Code completion                                             │
    │       • Conversational AI                                           │
    │     Examples: GPT-1/2/3/4, LLaMA, Claude, Gemini                    │
    │                                                                      │
    │  3. ENCODER-DECODER (Original Transformer)                          │
    │     ────────────────────────────────────────                         │
    │     Uses: Both encoder and decoder                                  │
    │     Attention: Bidirectional + Causal + Cross                       │
    │     Best for: Sequence-to-sequence tasks                            │
    │       • Translation                                                  │
    │       • Summarization                                               │
    │       • Question answering (generative)                             │
    │     Examples: T5, BART, mBART, MarianMT                             │
    │                                                                      │
    └─────────────────────────────────────────────────────────────────────┘
    """
    print(variants)

transformer_variants()
</code></pre>
                        
                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>Why GPT Uses Decoder-Only</h4>
                            <p>Modern LLMs like GPT-4 and Claude use only the decoder stack because:</p>
                            <ul>
                                <li><strong>Simplicity:</strong> One unified architecture for all tasks</li>
                                <li><strong>Flexibility:</strong> Can frame any task as text generation</li>
                                <li><strong>Scaling:</strong> Easier to scale a single stack</li>
                                <li><strong>In-context learning:</strong> Works naturally with prompts</li>
                            </ul>
                            <p>The encoder-decoder architecture is better for specific seq2seq tasks but less general.</p>
                        </div>
                        
                        <h3>Beyond NLP: Transformers Everywhere</h3>
                        
<pre><code class="language-python">import numpy as np

def transformers_beyond_nlp():
    """
    Applications of Transformers beyond text.
    """
    domains = {
        "Computer Vision": [
            "ViT (Vision Transformer): Image classification",
            "DETR: Object detection",
            "Swin Transformer: Hierarchical vision",
            "SAM: Segment Anything Model"
        ],
        "Audio & Speech": [
            "Whisper: Speech recognition",
            "MusicLM/Suno: Music generation",
            "Wav2Vec: Audio understanding"
        ],
        "Multimodal": [
            "CLIP: Image-text understanding",
            "DALL-E, Stable Diffusion: Text-to-image",
            "GPT-4V, Gemini: Vision + Language"
        ],
        "Biology & Science": [
            "AlphaFold 2: Protein structure prediction",
            "GenSLMs: Genomic language models",
            "Drug discovery models"
        ],
        "Robotics & Control": [
            "Decision Transformer: RL as sequence modeling",
            "RT-2: Robot reasoning",
            "Embodied AI agents"
        ],
        "Code & Math": [
            "Codex/GitHub Copilot: Code generation",
            "AlphaCode: Competitive programming",
            "Minerva: Math reasoning"
        ]
    }
    
    print("Transformers Beyond NLP:")
    print("=" * 60)
    
    for domain, examples in domains.items():
        print(f"\n{domain}:")
        for example in examples:
            print(f"  • {example}")
    
    print()
    print("The attention mechanism is remarkably general!")
    print("It works on any data that can be represented as sequences.")

transformers_beyond_nlp()
</code></pre>

                        <!-- Section 13: Implementing Attention in Code -->
                        <h2 id="implementing"><i class="fas fa-code me-2"></i>13. Implementing Attention in Code</h2>
                        
                        <p>Let's implement a complete, working Transformer encoder layer in Python. This code is designed to be clear and educational.</p>
                        
<pre><code class="language-python">import numpy as np

def softmax(x, axis=-1):
    """Numerically stable softmax."""
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

def layer_norm(x, gamma, beta, eps=1e-6):
    """Layer normalization."""
    mean = np.mean(x, axis=-1, keepdims=True)
    std = np.std(x, axis=-1, keepdims=True)
    return gamma * (x - mean) / (std + eps) + beta

class MultiHeadAttention:
    """
    Multi-Head Attention implementation.
    """
    def __init__(self, d_model, n_heads):
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # Initialize weights (normally learned)
        np.random.seed(42)
        scale = 0.02
        self.W_Q = np.random.randn(d_model, d_model) * scale
        self.W_K = np.random.randn(d_model, d_model) * scale
        self.W_V = np.random.randn(d_model, d_model) * scale
        self.W_O = np.random.randn(d_model, d_model) * scale
    
    def split_heads(self, x):
        """Reshape (batch, seq_len, d_model) → (batch, n_heads, seq_len, d_k)"""
        batch_size, seq_len, _ = x.shape
        x = x.reshape(batch_size, seq_len, self.n_heads, self.d_k)
        return x.transpose(0, 2, 1, 3)  # (batch, n_heads, seq_len, d_k)
    
    def combine_heads(self, x):
        """Reshape (batch, n_heads, seq_len, d_k) → (batch, seq_len, d_model)"""
        batch_size, _, seq_len, _ = x.shape
        x = x.transpose(0, 2, 1, 3)  # (batch, seq_len, n_heads, d_k)
        return x.reshape(batch_size, seq_len, self.d_model)
    
    def forward(self, Q, K, V, mask=None):
        """
        Compute multi-head attention.
        Q, K, V: (batch, seq_len, d_model)
        """
        batch_size = Q.shape[0]
        
        # Linear projections
        Q = Q @ self.W_Q  # (batch, seq_len, d_model)
        K = K @ self.W_K
        V = V @ self.W_V
        
        # Split into multiple heads
        Q = self.split_heads(Q)  # (batch, n_heads, seq_len, d_k)
        K = self.split_heads(K)
        V = self.split_heads(V)
        
        # Scaled dot-product attention
        scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(self.d_k)
        
        if mask is not None:
            scores = np.where(mask == 0, -1e9, scores)
        
        attention_weights = softmax(scores, axis=-1)
        context = attention_weights @ V  # (batch, n_heads, seq_len, d_k)
        
        # Combine heads
        context = self.combine_heads(context)  # (batch, seq_len, d_model)
        
        # Final linear projection
        output = context @ self.W_O
        
        return output, attention_weights

# Test the implementation
print("Testing Multi-Head Attention:")
print("=" * 50)

batch_size, seq_len, d_model, n_heads = 2, 10, 64, 8
x = np.random.randn(batch_size, seq_len, d_model)

mha = MultiHeadAttention(d_model, n_heads)
output, weights = mha.forward(x, x, x)  # Self-attention

print(f"Input shape:  {x.shape}")
print(f"Output shape: {output.shape}")
print(f"Attention weights shape: {weights.shape}")
print(f"  → (batch={batch_size}, heads={n_heads}, seq={seq_len}, seq={seq_len})")
</code></pre>
                        
                        <h3>Complete Encoder Layer</h3>
                        
<pre><code class="language-python">import numpy as np

def softmax(x, axis=-1):
    """Numerically stable softmax."""
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)

def relu(x):
    """ReLU activation."""
    return np.maximum(0, x)

def layer_norm(x, gamma, beta, eps=1e-6):
    """Layer normalization."""
    mean = np.mean(x, axis=-1, keepdims=True)
    std = np.std(x, axis=-1, keepdims=True)
    return gamma * (x - mean) / (std + eps) + beta

class TransformerEncoderLayer:
    """
    A single Transformer encoder layer.
    
    Structure:
        x → MultiHeadAttention → Add & Norm → FFN → Add & Norm → output
    """
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_ff = d_ff
        self.dropout = dropout
        
        # Initialize weights
        np.random.seed(42)
        scale = 0.02
        
        # Multi-head attention weights
        self.W_Q = np.random.randn(d_model, d_model) * scale
        self.W_K = np.random.randn(d_model, d_model) * scale
        self.W_V = np.random.randn(d_model, d_model) * scale
        self.W_O = np.random.randn(d_model, d_model) * scale
        
        # Feed-forward weights
        self.W1 = np.random.randn(d_model, d_ff) * scale
        self.b1 = np.zeros(d_ff)
        self.W2 = np.random.randn(d_ff, d_model) * scale
        self.b2 = np.zeros(d_model)
        
        # Layer norm parameters
        self.gamma1 = np.ones(d_model)
        self.beta1 = np.zeros(d_model)
        self.gamma2 = np.ones(d_model)
        self.beta2 = np.zeros(d_model)
    
    def multi_head_attention(self, x):
        """Self-attention sublayer."""
        batch_size, seq_len, _ = x.shape
        d_k = self.d_model // self.n_heads
        
        Q = x @ self.W_Q
        K = x @ self.W_K
        V = x @ self.W_V
        
        # Reshape for multi-head
        def split_heads(tensor):
            return tensor.reshape(batch_size, seq_len, self.n_heads, d_k).transpose(0, 2, 1, 3)
        
        Q, K, V = split_heads(Q), split_heads(K), split_heads(V)
        
        # Attention
        scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(d_k)
        weights = softmax(scores, axis=-1)
        context = weights @ V
        
        # Combine heads
        context = context.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, self.d_model)
        return context @ self.W_O
    
    def feed_forward(self, x):
        """Position-wise feed-forward sublayer."""
        hidden = relu(x @ self.W1 + self.b1)
        return hidden @ self.W2 + self.b2
    
    def forward(self, x):
        """Forward pass through the encoder layer."""
        # Self-attention with residual and norm
        attn_output = self.multi_head_attention(x)
        x = layer_norm(x + attn_output, self.gamma1, self.beta1)
        
        # Feed-forward with residual and norm
        ff_output = self.feed_forward(x)
        x = layer_norm(x + ff_output, self.gamma2, self.beta2)
        
        return x

# Test complete encoder layer
print("Testing Transformer Encoder Layer:")
print("=" * 50)

batch_size, seq_len, d_model = 2, 10, 64
n_heads, d_ff = 8, 256

x = np.random.randn(batch_size, seq_len, d_model)
encoder = TransformerEncoderLayer(d_model, n_heads, d_ff)

output = encoder.forward(x)
print(f"Input shape:  {x.shape}")
print(f"Output shape: {output.shape}")
print(f"✓ Same shape (residual connections preserve dimensions)")
</code></pre>
                        
                        <h3>Using PyTorch's Built-in Implementation</h3>
                        
                        <p>In practice, use framework implementations for efficiency and GPU support:</p>
                        
<pre><code class="language-python">import torch
import torch.nn as nn

# PyTorch provides built-in Transformer layers
d_model = 512
n_heads = 8
d_ff = 2048
n_layers = 6

# Single encoder layer
encoder_layer = nn.TransformerEncoderLayer(
    d_model=d_model,
    nhead=n_heads,
    dim_feedforward=d_ff,
    dropout=0.1,
    batch_first=True  # (batch, seq, features)
)

# Stack of encoder layers
encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)

# Example forward pass
batch_size, seq_len = 32, 100
x = torch.randn(batch_size, seq_len, d_model)

output = encoder(x)
print(f"Input: {x.shape}")
print(f"Output: {output.shape}")

# Full Transformer (encoder + decoder)
transformer = nn.Transformer(
    d_model=d_model,
    nhead=n_heads,
    num_encoder_layers=n_layers,
    num_decoder_layers=n_layers,
    dim_feedforward=d_ff,
    dropout=0.1,
    batch_first=True
)

# For translation: source and target sequences
src = torch.randn(batch_size, 50, d_model)   # Source: 50 tokens
tgt = torch.randn(batch_size, 40, d_model)   # Target: 40 tokens

output = transformer(src, tgt)
print(f"\nTranslation example:")
print(f"Source: {src.shape}")
print(f"Target: {tgt.shape}")
print(f"Output: {output.shape}")
</code></pre>
                        
                        <div class="experiment-card">
                            <h4><i class="fas fa-graduation-cap me-2"></i>Libraries for Working with Transformers</h4>
                            <div class="card-meta">Practical tools for real projects</div>
                            <div class="card-content">
                                <table class="table table-sm">
                                    <tr>
                                        <td><strong>Hugging Face Transformers</strong></td>
                                        <td>Pre-trained models, tokenizers, training utilities</td>
                                    </tr>
                                    <tr>
                                        <td><strong>PyTorch</strong></td>
                                        <td>nn.Transformer, low-level control</td>
                                    </tr>
                                    <tr>
                                        <td><strong>TensorFlow</strong></td>
                                        <td>keras.layers.MultiHeadAttention</td>
                                    </tr>
                                    <tr>
                                        <td><strong>JAX/Flax</strong></td>
                                        <td>High-performance research implementations</td>
                                    </tr>
                                </table>
                                <p>For most applications, use <strong>Hugging Face</strong>—it provides thousands of pre-trained models ready to use!</p>
                            </div>
                            <div class="card-tags">
                                <span class="bias-tag">Hugging Face</span>
                                <span class="bias-tag">PyTorch</span>
                                <span class="bias-tag">Pre-trained Models</span>
                            </div>
                        </div>

                        <!-- Section 14: Conclusion -->
                        <h2 id="conclusion"><i class="fas fa-flag-checkered me-2"></i>14. Conclusion</h2>
                        
                        <p>"Attention Is All You Need" is more than a research paper—it's a <strong>paradigm shift</strong> that redefined how we build AI systems. Let's summarize the key takeaways:</p>
                        
                        <div class="highlight-box">
                            <h4><i class="fas fa-key me-2"></i>Key Takeaways</h4>
                            <ol>
                                <li><strong>Attention replaces recurrence:</strong> Self-attention can model any dependencies without sequential processing</li>
                                <li><strong>Parallelization is key:</strong> O(1) sequential operations enable massive GPU parallelism</li>
                                <li><strong>Multi-head attention is powerful:</strong> Different heads learn different relationship types</li>
                                <li><strong>Position needs explicit encoding:</strong> Sinusoidal or learned positional embeddings are essential</li>
                                <li><strong>Simple components, powerful combinations:</strong> Attention + FFN + LayerNorm + Residuals</li>
                                <li><strong>Scaling works:</strong> Bigger models consistently perform better (given enough data)</li>
                            </ol>
                        </div>
                        
<pre><code class="language-python">import numpy as np

def the_big_picture():
    """
    The Transformer's lasting impact.
    """
    summary = """
    ┌─────────────────────────────────────────────────────────────────────┐
    │                    THE TRANSFORMER'S LEGACY                          │
    ├─────────────────────────────────────────────────────────────────────┤
    │                                                                      │
    │  BEFORE (2017)                    AFTER (2017+)                     │
    │  ─────────────                    ────────────                      │
    │  RNNs/LSTMs for sequences         Transformers everywhere           │
    │  Slow sequential training         Massively parallel training       │
    │  Struggled with long sequences    Handle thousands of tokens        │
    │  Task-specific architectures      General-purpose foundation        │
    │  Limited model sizes              Billions of parameters            │
    │                                                                      │
    │  The paper's citation count: 100,000+ (one of most cited ever)     │
    │                                                                      │
    │  Models built on Transformers:                                      │
    │    • GPT-4, Claude, Gemini (LLMs)                                   │
    │    • BERT, RoBERTa (NLU)                                            │
    │    • DALL-E, Stable Diffusion (Image generation)                    │
    │    • Whisper (Speech)                                               │
    │    • AlphaFold (Protein structure)                                  │
    │    • And countless more...                                          │
    │                                                                      │
    │  The insight "attention is all you need" turned out to be          │
    │  remarkably prescient—attention mechanisms are indeed all           │
    │  you need for a huge range of AI applications.                      │
    │                                                                      │
    └─────────────────────────────────────────────────────────────────────┘
    """
    print(summary)

the_big_picture()
</code></pre>
                        
                        <h3>Where to Go From Here</h3>
                        
                        <div class="experiment-card">
                            <h4><i class="fas fa-road me-2"></i>Recommended Learning Path</h4>
                            <div class="card-meta">Continue your Transformer journey</div>
                            <div class="card-content">
                                <ol>
                                    <li><strong>Implement from scratch:</strong> Code a Transformer yourself (we did the encoder above!)</li>
                                    <li><strong>Study BERT:</strong> Understand encoder-only masked language modeling</li>
                                    <li><strong>Study GPT:</strong> Understand decoder-only causal language modeling</li>
                                    <li><strong>Use Hugging Face:</strong> Work with pre-trained models on real tasks</li>
                                    <li><strong>Explore Vision Transformers:</strong> See how attention applies to images</li>
                                    <li><strong>Read follow-up papers:</strong> BERT, GPT-2, T5, ViT, etc.</li>
                                </ol>
                            </div>
                            <div class="card-tags">
                                <span class="bias-tag">Next Steps</span>
                                <span class="bias-tag">Self-Study</span>
                                <span class="bias-tag">Projects</span>
                            </div>
                        </div>
                        
                        <p>The Transformer architecture will likely remain foundational for years to come. Understanding it deeply gives you the conceptual tools to understand <strong>virtually all modern AI systems</strong>.</p>
                        
                        <p>Welcome to the post-Transformer era of AI. The journey is just beginning!</p>

                        <!-- Related Posts -->
                        <div class="related-posts">
                            <h3><i class="fas fa-book me-2"></i>Related Articles</h3>
                            <div class="related-post-item">
                                <h5 class="mb-2">PyTorch Deep Learning: Complete Beginner's Guide to Building Neural Networks</h5>
                                <p class="text-muted small mb-2">Master PyTorch from scratch. Learn tensors, autograd, neural networks, CNNs, RNNs, transfer learning, and deployment with hands-on examples.</p>
                                <a href="pytorch-deep-learning-guide.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">TensorFlow Deep Learning: Complete Beginner's Guide to Building Neural Networks</h5>
                                <p class="text-muted small mb-2">Master TensorFlow 2 and Keras for deep learning. Learn neural network architectures, training workflows, and deployment techniques.</p>
                                <a href="tensorflow-deep-learning-guide.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Computer Vision Fundamentals: A Complete Beginner's Guide</h5>
                                <p class="text-muted small mb-2">Master computer vision from scratch. Learn object detection, segmentation, and generative models with YOLO, U-Net, and diffusion models.</p>
                                <a href="computer-vision-fundamentals-guide.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">
                        I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.
                    </p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook">
                            <i class="fab fa-facebook-f"></i>
                        </a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter">
                            <i class="fab fa-twitter"></i>
                        </a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn">
                            <i class="fab fa-linkedin-in"></i>
                        </a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube">
                            <i class="fab fa-youtube"></i>
                        </a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram">
                            <i class="fab fa-instagram"></i>
                        </a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest">
                            <i class="fab fa-pinterest-p"></i>
                        </a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email">
                            <i class="fas fa-envelope"></i>
                        </a>
                    </div>
                </div>
            </div>

            <hr class="bg-secondary">

            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small">
                        <i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a>
                    </p>
                    <p class="small mt-3">
                        <a href="/" class="text-light text-decoration-none">Home</a> | 
                        <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | 
                        <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a>
                    </p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">
                        Enjoying this content? ☕ <a href="https://buymeacoffee.com/itswzee" target="_blank" class="text-light" style="text-decoration: underline;">Keep me caffeinated</a> to keep the pixels flowing!
                    </p>
                </div>
            </div>
        </div>
    </footer>

    <!-- Scroll-to-Top Button -->
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <!-- Category Indicator -->
    <div id="categoryIndicator" class="category-indicator" title="Current Section">
        <i class="fas fa-tag"></i><span id="categoryText">Technology</span>
    </div>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    
    <!-- Cookie Consent JS -->
    <script src="../../../js/cookie-consent.js"></script>
    
    <!-- Main JS -->
    <script src="../../../js/main.js"></script>
    
    <!-- Prism.js for Syntax Highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <!-- Scroll-to-Top Script -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const scrollToTopBtn = document.getElementById('scrollToTop');
            const categoryIndicator = document.getElementById('categoryIndicator');
            
            // Show/hide button and category indicator on scroll
            window.addEventListener('scroll', function() {
                if (window.scrollY > 300) {
                    scrollToTopBtn.classList.add('show');
                    if (categoryIndicator) categoryIndicator.classList.add('show');
                } else {
                    scrollToTopBtn.classList.remove('show');
                    if (categoryIndicator) categoryIndicator.classList.remove('show');
                }
            });
            
            // Smooth scroll to top on click
            scrollToTopBtn.addEventListener('click', function() {
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        });
    </script>

    <!-- Prism Theme Switcher -->
    <script>
        // Available themes with display names
        const themes = {
            'prism-theme': 'Tomorrow Night',
            'prism-default': 'Default',
            'prism-dark': 'Dark',
            'prism-twilight': 'Twilight',
            'prism-okaidia': 'Okaidia',
            'prism-solarizedlight': 'Solarized Light'
        };

        // Load saved theme from localStorage or use default
        const savedTheme = localStorage.getItem('prism-theme') || 'prism-theme';

        // Function to switch theme
        function switchTheme(themeId) {
            // Disable all themes
            Object.keys(themes).forEach(id => {
                const link = document.getElementById(id);
                if (link) {
                    link.disabled = true;
                }
            });
            
            // Enable selected theme
            const selectedLink = document.getElementById(themeId);
            if (selectedLink) {
                selectedLink.disabled = false;
                localStorage.setItem('prism-theme', themeId);
            }

            // Update all dropdowns on the page to match selected theme
            document.querySelectorAll('div.code-toolbar select').forEach(dropdown => {
                dropdown.value = themeId;
            });

            // Re-apply syntax highlighting with new theme
            setTimeout(() => {
                Prism.highlightAll();
            }, 10);
        }

        // Apply saved theme on page load
        document.addEventListener('DOMContentLoaded', function() {
            switchTheme(savedTheme);
        });

        // Add theme switcher to Prism toolbar
        Prism.plugins.toolbar.registerButton('theme-switcher', function(env) {
            const select = document.createElement('select');
            select.setAttribute('aria-label', 'Select code theme');
            select.className = 'prism-theme-selector';
            
            // Populate dropdown with themes
            Object.keys(themes).forEach(themeId => {
                const option = document.createElement('option');
                option.value = themeId;
                option.textContent = themes[themeId];
                if (themeId === savedTheme) {
                    option.selected = true;
                }
                select.appendChild(option);
            });
            
            // Handle theme change
            select.addEventListener('change', function(e) {
                switchTheme(e.target.value);
            });
            
            return select;
        });
    </script>

    <!-- Side Navigation TOC Script -->
    <script>
        // Open side navigation
        function openNav() {
            document.getElementById('tocSidenav').classList.add('open');
            document.getElementById('tocOverlay').classList.add('show');
            document.body.style.overflow = 'hidden'; // Prevent background scroll
        }

        // Close side navigation
        function closeNav() {
            document.getElementById('tocSidenav').classList.remove('open');
            document.getElementById('tocOverlay').classList.remove('show');
            document.body.style.overflow = 'auto';
        }

        // Close on ESC key
        document.addEventListener('keydown', function(e) {
            if (e.key === 'Escape') {
                closeNav();
            }
        });

        // Highlight active section in TOC based on scroll position
        document.addEventListener('DOMContentLoaded', function() {
            const sections = document.querySelectorAll('[id]');
            const tocLinks = document.querySelectorAll('.sidenav-toc a');
            
            function highlightActiveSection() {
                let currentSection = '';
                
                sections.forEach(section => {
                    const sectionTop = section.offsetTop;
                    const sectionHeight = section.clientHeight;
                    
                    if (window.scrollY >= sectionTop - 200) {
                        currentSection = section.getAttribute('id');
                    }
                });
                
                tocLinks.forEach(link => {
                    link.classList.remove('active');
                    if (link.getAttribute('href') === '#' + currentSection) {
                        link.classList.add('active');
                    }
                });
            }
            
            // Highlight on scroll
            window.addEventListener('scroll', highlightActiveSection);
            
            // Initial highlight
            highlightActiveSection();
            
            // Smooth scroll for TOC links
            tocLinks.forEach(link => {
                link.addEventListener('click', function(e) {
                    e.preventDefault();
                    const targetId = this.getAttribute('href');
                    const targetSection = document.querySelector(targetId);
                    
                    if (targetSection) {
                        const offsetTop = targetSection.offsetTop - 80; // Account for fixed navbar
                        window.scrollTo({
                            top: offsetTop,
                            behavior: 'smooth'
                        });
                    }
                    
                    // Close nav after clicking
                    setTimeout(closeNav, 300);
                });
            });
        });
    </script>

            <!-- Scroll-to-Top and Category Indicator Script -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const scrollToTopBtn = document.getElementById('scrollToTop');
            const categoryIndicator = document.getElementById('categoryIndicator');
            const categoryText = document.getElementById('categoryText');
            
            // Auto-detect H2 sections in the article (works with or without id)
            const h2Elements = document.querySelectorAll('.blog-content h2');
            const sections = [];
            h2Elements.forEach(function(h2, index) {
                // Get text without icon
                let text = h2.textContent.trim().replace(/^\d+\.\s*/, '');
                // Truncate to 25 chars
                if (text.length > 25) text = text.substring(0, 22) + '...';
                sections.push({ element: h2, name: text });
            });
            
            // Fallback to article category if no sections found
            const articleCategory = categoryText ? categoryText.textContent : 'Article';
            
            // Show/hide button on scroll and update section
            window.addEventListener('scroll', function() {
                if (window.scrollY > 300) {
                    if (scrollToTopBtn) scrollToTopBtn.classList.add('show');
                    if (categoryIndicator) categoryIndicator.classList.add('show');
                } else {
                    if (scrollToTopBtn) scrollToTopBtn.classList.remove('show');
                    if (categoryIndicator) categoryIndicator.classList.remove('show');
                }
                
                // Update current section
                updateCurrentSection();
            });
            
            // Update section based on viewport position
            function updateCurrentSection() {
                if (!categoryText || sections.length === 0) return;
                
                let currentSection = articleCategory;
                
                for (let section of sections) {
                    const rect = section.element.getBoundingClientRect();
                    if (rect.top <= window.innerHeight / 2) {
                        currentSection = section.name;
                    }
                }
                
                categoryText.textContent = currentSection;
            }
            
            // Smooth scroll to top on click
            if (scrollToTopBtn) {
                scrollToTopBtn.addEventListener('click', function() {
                    window.scrollTo({ top: 0, behavior: 'smooth' });
                });
            }
        });
    </script>
</body>
</html>
