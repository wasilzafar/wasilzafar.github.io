<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Part 3 of the Complete NLP Series: Master text representation techniques—Bag of Words, TF-IDF, N-grams, one-hot encoding, and feature engineering for NLP models." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="NLP, Text Representation, Bag of Words, TF-IDF, N-grams, Feature Engineering, One-Hot Encoding, Document Vectors, Vectorization" />
    <meta property="og:title" content="Text Representation & Feature Engineering - Complete NLP Series Part 3" />
    <meta property="og:description" content="Turn text into numbers: Bag of Words, TF-IDF, N-grams, and feature engineering techniques for NLP." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-27" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>Text Representation & Feature Engineering - Complete NLP Series Part 3 - Wasil Zafar</title>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('consent', 'default', { 'ad_storage': 'denied', 'ad_user_data': 'denied', 'ad_personalization': 'denied', 'analytics_storage': 'denied', 'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE'] });
        gtag('consent', 'default', { 'ad_storage': 'granted', 'ad_user_data': 'granted', 'ad_personalization': 'granted', 'analytics_storage': 'granted' });
        gtag('set', 'url_passthrough', true);
    </script>
    <script>
        (function(w, d, s, l, i) { w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' }); var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f); })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    </head>
<body>
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/"><span class="gradient-text">Wasil Zafar</span></a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link" href="/">Home</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#about">About</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#certifications">Certifications</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#interests">Interests</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link"><i class="fas fa-arrow-left me-2"></i>Back to Technology</a>
                <h1 class="display-4 fw-bold mb-3">Text Representation & Feature Engineering</h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 27, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-1"></i>28 min read</span>
                    <button onclick="window.print()" class="print-btn" title="Print this article"><i class="fas fa-print"></i> Print</button>
                </div>
                <p class="lead">Part 3 of 16: Turn text into numbers—Bag of Words, TF-IDF, N-grams, and feature engineering for NLP models.</p>
            </div>
        </div>
    </section>

    <button class="toc-toggle-btn" onclick="openNav()" title="Table of Contents" aria-label="Open Table of Contents"><i class="fas fa-list"></i></button>

    <div id="tocSidenav" class="sidenav-toc">
        <div class="toc-header">
            <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
            <button class="closebtn" onclick="closeNav()" aria-label="Close">&times;</button>
        </div>
        <ol>
            <li><a href="#introduction" onclick="closeNav()">Introduction to Text Representation</a></li>
            <li><a href="#one-hot" onclick="closeNav()">One-Hot Encoding</a></li>
            <li><a href="#bow" onclick="closeNav()">Bag of Words (BoW)</a>
                <ul>
                    <li><a href="#bow-implementation" onclick="closeNav()">Implementation</a></li>
                    <li><a href="#bow-limitations" onclick="closeNav()">Limitations</a></li>
                </ul>
            </li>
            <li><a href="#tfidf" onclick="closeNav()">TF-IDF</a>
                <ul>
                    <li><a href="#tf" onclick="closeNav()">Term Frequency</a></li>
                    <li><a href="#idf" onclick="closeNav()">Inverse Document Frequency</a></li>
                    <li><a href="#tfidf-variants" onclick="closeNav()">TF-IDF Variants</a></li>
                </ul>
            </li>
            <li><a href="#ngrams" onclick="closeNav()">N-grams</a></li>
            <li><a href="#feature-engineering" onclick="closeNav()">Feature Engineering for NLP</a></li>
            <li><a href="#sparse-vs-dense" onclick="closeNav()">Sparse vs Dense Representations</a></li>
            <li><a href="#conclusion" onclick="closeNav()">Conclusion & Next Steps</a></li>
        </ol>
    </div>

    <div id="tocOverlay" class="sidenav-overlay" onclick="closeNav()"></div>

    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="blog-content">
                        
                        <h2 id="introduction"><i class="fas fa-vector-square me-2"></i>Introduction to Text Representation</h2>
                        
                        <p>Before applying machine learning to text, we need to convert words into numerical vectors. This third part covers classical text representation methods that laid the foundation for modern NLP.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-star me-2"></i>Key Insight</h4>
                            <p><strong>Classical text representations like TF-IDF are still valuable for many tasks and serve as strong baselines before deploying complex neural models.</strong></p>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-map-signs me-2"></i>Complete NLP Series Navigation</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">16-Part Series</span>
                                <span class="badge bg-crimson">NLP Mastery</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><a href="nlp-fundamentals-linguistic-basics.html">NLP Fundamentals & Linguistic Basics</a></li>
                                    <li><a href="nlp-tokenization-text-cleaning.html">Tokenization & Text Cleaning</a></li>
                                    <li><strong>Text Representation & Feature Engineering (This Guide)</strong></li>
                                    <li><a href="nlp-word-embeddings.html">Word Embeddings</a></li>
                                    <li><a href="nlp-statistical-language-models.html">Statistical Language Models & N-grams</a></li>
                                    <li><a href="nlp-neural-networks.html">Neural Networks for NLP</a></li>
                                    <li><a href="nlp-rnn-lstm-gru.html">RNNs, LSTMs & GRUs</a></li>
                                    <li><a href="nlp-transformers-attention.html">Transformers & Attention Mechanism</a></li>
                                    <li><a href="nlp-pretrained-models-transfer-learning.html">Pretrained Language Models & Transfer Learning</a></li>
                                    <li><a href="nlp-gpt-text-generation.html">GPT Models & Text Generation</a></li>
                                    <li><a href="nlp-core-tasks.html">Core NLP Tasks</a></li>
                                    <li><a href="nlp-advanced-tasks.html">Advanced NLP Tasks</a></li>
                                    <li><a href="nlp-multilingual-crosslingual.html">Multilingual & Cross-lingual NLP</a></li>
                                    <li><a href="nlp-evaluation-ethics.html">Evaluation, Ethics & Responsible NLP</a></li>
                                    <li><a href="nlp-systems-production.html">NLP Systems, Optimization & Production</a></li>
                                    <li><a href="nlp-cutting-edge-research.html">Cutting-Edge & Research Topics</a></li>
                                </ol>
                            </div>
                        </div>

                        <h2 id="one-hot"><i class="fas fa-toggle-on me-2"></i>One-Hot Encoding</h2>
                        
                        <p><strong>One-hot encoding</strong> is the simplest form of text representation where each word in the vocabulary is represented as a binary vector with exactly one element set to 1 and all others set to 0. The position of the "1" corresponds to the word's index in the vocabulary. For a vocabulary of size V, each word becomes a V-dimensional vector that uniquely identifies it.</p>

                        <p>Consider a vocabulary of ["cat", "dog", "bird"]. The word "cat" would be represented as [1, 0, 0], "dog" as [0, 1, 0], and "bird" as [0, 0, 1]. This representation treats each word as completely independent—there's no notion that "cat" and "dog" are both animals while "bird" is different. The orthogonality of vectors means cosine similarity between any two different words is always zero.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>Why One-Hot Encoding Matters</h4>
                            <p><strong>One-hot encoding provides a foundation for understanding more advanced representations.</strong> While impractical for large vocabularies, it's the conceptual basis for neural network input layers and helps illustrate why we need denser, more meaningful representations like word embeddings.</p>
                        </div>

<pre><code class="language-python">import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Sample vocabulary
vocabulary = ['apple', 'banana', 'cherry', 'date', 'elderberry']

# Create one-hot encoder
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(vocabulary)
print("Label Encoded:", integer_encoded)
# Output: [0 1 2 3 4]

# Reshape for one-hot encoding
integer_encoded = integer_encoded.reshape(-1, 1)
onehot_encoder = OneHotEncoder(sparse_output=False)
onehot_encoded = onehot_encoder.fit_transform(integer_encoded)

print("\nOne-Hot Encoded Vectors:")
for word, vector in zip(vocabulary, onehot_encoded):
    print(f"  {word:12} -> {vector}")
# apple        -> [1. 0. 0. 0. 0.]
# banana       -> [0. 1. 0. 0. 0.]
# cherry       -> [0. 0. 1. 0. 0.]
# date         -> [0. 0. 0. 1. 0.]
# elderberry   -> [0. 0. 0. 0. 1.]
</code></pre>

                        <p>The primary limitation of one-hot encoding is <strong>dimensionality explosion</strong>. A vocabulary of 50,000 words requires 50,000-dimensional vectors, making storage and computation extremely expensive. Additionally, these sparse, high-dimensional vectors don't capture any semantic relationships between words. The sentence "The cat sat on the mat" would require six 50,000-dimensional vectors just for representation.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-flask me-2"></i>One-Hot Encoding for Sentences</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Representation</span>
                                <span class="badge bg-crimson">Sparse Vectors</span>
                            </div>
                            <div class="content">
<pre><code class="language-python">import numpy as np

def one_hot_encode_sentence(sentence, vocab_to_idx):
    """Encode a sentence as sequence of one-hot vectors."""
    words = sentence.lower().split()
    vocab_size = len(vocab_to_idx)
    encoded = np.zeros((len(words), vocab_size))
    
    for i, word in enumerate(words):
        if word in vocab_to_idx:
            encoded[i, vocab_to_idx[word]] = 1
    return encoded

# Build vocabulary from corpus
corpus = ["the cat sat on the mat", "the dog ran in the park"]
all_words = set(' '.join(corpus).split())
vocab_to_idx = {word: idx for idx, word in enumerate(sorted(all_words))}

print("Vocabulary:", vocab_to_idx)
# {'cat': 0, 'dog': 1, 'in': 2, 'mat': 3, 'on': 4, 'park': 5, 'ran': 6, 'sat': 7, 'the': 8}

# Encode a sentence
sentence = "the cat sat"
encoded = one_hot_encode_sentence(sentence, vocab_to_idx)
print(f"\nSentence: '{sentence}'")
print("Shape:", encoded.shape)  # (3, 9)
print("Encoded:")
print(encoded)
</code></pre>
                            </div>
                        </div>

                        <h2 id="bow"><i class="fas fa-shopping-bag me-2"></i>Bag of Words (BoW)</h2>
                        
                        <p>The <strong>Bag of Words (BoW)</strong> model represents a document as an unordered collection of words, disregarding grammar and word order while keeping track of word frequency. Each document becomes a vector where each dimension corresponds to a vocabulary term, and the value represents how many times that term appears in the document. This simple yet powerful representation enables quantitative comparison between documents.</p>

                        <p>The name "bag of words" comes from the metaphor of throwing all words from a document into a bag—you know what words are present and how many of each, but you lose all information about their original order. Despite this limitation, BoW has proven remarkably effective for tasks like document classification, sentiment analysis, and information retrieval.</p>

                        <h3 id="bow-implementation">Implementation</h3>

                        <p>Scikit-learn's <code>CountVectorizer</code> provides an efficient implementation of BoW that handles tokenization, vocabulary building, and document encoding in a single pipeline. By default, it converts text to lowercase and removes single-character tokens, but these behaviors are fully configurable.</p>

<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Sample documents
documents = [
    "Machine learning is fascinating and useful",
    "Deep learning is a subset of machine learning",
    "Natural language processing uses machine learning",
    "NLP is useful for text analysis"
]

# Create Bag of Words model
vectorizer = CountVectorizer()
bow_matrix = vectorizer.fit_transform(documents)

# Get feature names (vocabulary)
feature_names = vectorizer.get_feature_names_out()
print("Vocabulary:", list(feature_names))

# Convert to DataFrame for visualization
bow_df = pd.DataFrame(
    bow_matrix.toarray(),
    columns=feature_names,
    index=[f"Doc {i+1}" for i in range(len(documents))]
)
print("\nBag of Words Matrix:")
print(bow_df)

# Document similarity using dot product
from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity(bow_matrix)
print("\nDocument Similarity Matrix:")
print(pd.DataFrame(similarity, index=bow_df.index, columns=bow_df.index).round(3))
</code></pre>

                        <p>The BoW representation allows us to perform document-level operations like finding similar documents, clustering related content, and training classifiers. The <strong>cosine similarity</strong> between BoW vectors provides a natural measure of document similarity that's robust to document length differences.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-cog me-2"></i>CountVectorizer Configuration Options</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Sklearn</span>
                                <span class="badge bg-crimson">Customization</span>
                            </div>
                            <div class="content">
<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer

documents = [
    "The quick brown fox jumps over the lazy dog",
    "A quick brown dog outpaces a lazy fox",
    "Machine learning and deep learning are related"
]

# Basic vectorizer
basic_vec = CountVectorizer()
print("Basic vocabulary size:", len(basic_vec.fit(documents).vocabulary_))

# With stop words removed
no_stop_vec = CountVectorizer(stop_words='english')
print("Without stop words:", len(no_stop_vec.fit(documents).vocabulary_))

# With minimum document frequency
min_df_vec = CountVectorizer(min_df=2)  # Word must appear in at least 2 docs
print("min_df=2 vocabulary:", list(min_df_vec.fit(documents).vocabulary_.keys()))

# With n-gram range (unigrams and bigrams)
ngram_vec = CountVectorizer(ngram_range=(1, 2))
ngram_matrix = ngram_vec.fit_transform(documents)
print("Unigrams + Bigrams features:", ngram_matrix.shape[1])

# With maximum features
max_feat_vec = CountVectorizer(max_features=10)
print("Top 10 features:", list(max_feat_vec.fit(documents).vocabulary_.keys()))

# Binary BoW (presence/absence only)
binary_vec = CountVectorizer(binary=True)
binary_matrix = binary_vec.fit_transform(documents)
print("\nBinary BoW (first doc):", binary_matrix.toarray()[0][:10])
</code></pre>
                            </div>
                        </div>

                        <h3 id="bow-limitations">Limitations</h3>

                        <p>While BoW is simple and effective for many tasks, it has significant limitations. <strong>Loss of word order</strong> means "dog bites man" and "man bites dog" have identical representations. <strong>Loss of context</strong> prevents distinguishing between different meanings of the same word. <strong>Vocabulary scaling</strong> creates increasingly sparse vectors as vocabulary grows. <strong>No semantic similarity</strong> means synonyms like "happy" and "joyful" are treated as completely unrelated.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-exclamation-triangle me-2"></i>BoW Limitations Summary</h4>
                            <table class="table table-bordered mt-3">
                                <thead class="table-dark">
                                    <tr>
                                        <th>Limitation</th>
                                        <th>Impact</th>
                                        <th>Mitigation</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>No word order</strong></td>
                                        <td>"not good" = "good not"</td>
                                        <td>Use N-grams</td>
                                    </tr>
                                    <tr>
                                        <td><strong>High dimensionality</strong></td>
                                        <td>Sparse, memory-intensive</td>
                                        <td>Dimensionality reduction</td>
                                    </tr>
                                    <tr>
                                        <td><strong>No semantics</strong></td>
                                        <td>Synonyms unrelated</td>
                                        <td>Word embeddings</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Equal word importance</strong></td>
                                        <td>Common words dominate</td>
                                        <td>TF-IDF weighting</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <h2 id="tfidf"><i class="fas fa-balance-scale me-2"></i>TF-IDF</h2>

                        <p><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong> is a numerical statistic that reflects how important a word is to a document within a collection. Unlike raw word counts, TF-IDF balances term frequency against document frequency, giving higher weight to words that are frequent in a specific document but rare across the corpus. This makes it invaluable for document retrieval, keyword extraction, and text classification.</p>

                        <p>The intuition behind TF-IDF is straightforward: words that appear frequently in a document are likely relevant to its content, but words that appear in many documents (like "the" or "is") carry less discriminative information. By multiplying term frequency by inverse document frequency, we amplify distinctive terms while dampening common ones.</p>

                        <h3 id="tf">Term Frequency</h3>

                        <p><strong>Term Frequency (TF)</strong> measures how often a term appears in a document. The simplest form is the raw count, but this favors longer documents. Common normalization approaches include dividing by document length or taking the logarithm to reduce the impact of very frequent terms.</p>

<pre><code class="language-python">import numpy as np
from collections import Counter

def compute_tf_variants(document):
    """Compute different TF variants for a document."""
    words = document.lower().split()
    word_counts = Counter(words)
    total_words = len(words)
    max_count = max(word_counts.values())
    
    results = {}
    for word, count in word_counts.items():
        results[word] = {
            'raw_count': count,
            'term_frequency': count / total_words,
            'log_normalization': 1 + np.log(count) if count > 0 else 0,
            'double_normalization': 0.5 + 0.5 * (count / max_count),
            'binary': 1 if count > 0 else 0
        }
    return results

# Example document
document = "machine learning is great machine learning is the future of machine intelligence"
tf_results = compute_tf_variants(document)

print("TF Variants for selected words:")
print(f"{'Word':<15} {'Raw':<6} {'TF':<8} {'Log':<8} {'Double':<8} {'Binary':<6}")
print("-" * 55)
for word in ['machine', 'learning', 'great', 'future']:
    r = tf_results[word]
    print(f"{word:<15} {r['raw_count']:<6} {r['term_frequency']:<8.3f} "
          f"{r['log_normalization']:<8.3f} {r['double_normalization']:<8.3f} {r['binary']:<6}")
</code></pre>

                        <h3 id="idf">Inverse Document Frequency</h3>

                        <p><strong>Inverse Document Frequency (IDF)</strong> measures how much information a term provides—whether it's common or rare across documents. Terms appearing in many documents get lower IDF scores, while rare terms get higher scores. The standard formula is IDF(t) = log(N / df(t)), where N is the total number of documents and df(t) is the number of documents containing term t.</p>

<pre><code class="language-python">import numpy as np
from collections import Counter

def compute_idf(corpus):
    """Compute IDF for all terms in a corpus."""
    # Get all unique terms
    all_terms = set()
    doc_term_sets = []
    for doc in corpus:
        terms = set(doc.lower().split())
        doc_term_sets.append(terms)
        all_terms.update(terms)
    
    N = len(corpus)
    idf_scores = {}
    
    for term in all_terms:
        # Count documents containing this term
        df = sum(1 for doc_terms in doc_term_sets if term in doc_terms)
        # Standard IDF with smoothing to avoid division by zero
        idf_scores[term] = np.log((N + 1) / (df + 1)) + 1
    
    return idf_scores

# Example corpus
corpus = [
    "the cat sat on the mat",
    "the dog ran in the park",
    "machine learning is fascinating",
    "deep learning uses neural networks",
    "the future of machine learning is bright"
]

idf_scores = compute_idf(corpus)

# Sort by IDF (highest = most discriminative)
sorted_idf = sorted(idf_scores.items(), key=lambda x: x[1], reverse=True)
print("IDF Scores (highest = most distinctive):")
print(f"{'Term':<15} {'IDF Score':<10} {'Interpretation'}")
print("-" * 50)
for term, score in sorted_idf[:10]:
    interp = "Common" if score < 1.5 else "Moderate" if score < 2.0 else "Rare/Distinctive"
    print(f"{term:<15} {score:<10.3f} {interp}")
</code></pre>

                        <p>Notice how common words like "the" have low IDF scores, while domain-specific terms like "neural" or "networks" have higher scores. This discrimination ability is what makes TF-IDF so effective for document retrieval and classification.</p>

                        <h3 id="tfidf-variants">TF-IDF Variants</h3>

                        <p>Scikit-learn's <code>TfidfVectorizer</code> provides a production-ready implementation with configurable TF and IDF formulations. The default uses sublinear TF scaling (1 + log(tf)) and smooth IDF (log((1 + n) / (1 + df)) + 1), along with L2 normalization to produce unit-length vectors.</p>

<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np

# Sample corpus for TF-IDF demonstration
corpus = [
    "Machine learning algorithms learn from data",
    "Deep learning is a subset of machine learning",
    "Natural language processing analyzes text data",
    "Computer vision processes image and video data",
    "Reinforcement learning learns through rewards"
]

# Standard TF-IDF
tfidf_standard = TfidfVectorizer()
tfidf_matrix = tfidf_standard.fit_transform(corpus)

# Display results
feature_names = tfidf_standard.get_feature_names_out()
tfidf_df = pd.DataFrame(
    tfidf_matrix.toarray().round(3),
    columns=feature_names,
    index=[f"Doc {i+1}" for i in range(len(corpus))]
)

print("TF-IDF Matrix (selected features):")
selected_features = ['learning', 'machine', 'data', 'deep', 'natural']
print(tfidf_df[selected_features])

# Compare with different configurations
tfidf_binary = TfidfVectorizer(binary=True)  # Binary TF
tfidf_no_smooth = TfidfVectorizer(smooth_idf=False)  # No IDF smoothing
tfidf_no_norm = TfidfVectorizer(norm=None)  # No L2 normalization

configs = {
    'Standard': tfidf_standard,
    'Binary TF': tfidf_binary,
    'No IDF smooth': tfidf_no_smooth,
    'No L2 norm': tfidf_no_norm
}

print("\nTF-IDF value for 'learning' in Doc 1 across configurations:")
for name, vec in configs.items():
    matrix = vec.fit_transform(corpus)
    features = vec.get_feature_names_out()
    idx = np.where(features == 'learning')[0][0]
    print(f"  {name:<15}: {matrix.toarray()[0, idx]:.4f}")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-search me-2"></i>TF-IDF for Document Retrieval</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Search</span>
                                <span class="badge bg-crimson">Ranking</span>
                            </div>
                            <div class="content">
<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Document corpus (simulating a search index)
documents = [
    "Python is a popular programming language for data science",
    "JavaScript is used for web development and frontend",
    "Machine learning models can predict outcomes from data",
    "Data analysis requires statistical knowledge",
    "Python libraries like pandas help with data manipulation",
    "Neural networks are used in deep learning applications"
]

# Build TF-IDF index
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(documents)

def search(query, top_k=3):
    """Search documents using TF-IDF similarity."""
    query_vec = vectorizer.transform([query])
    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()
    
    # Get top-k results
    top_indices = similarities.argsort()[-top_k:][::-1]
    
    print(f"Query: '{query}'")
    print("-" * 50)
    for rank, idx in enumerate(top_indices, 1):
        print(f"{rank}. (score: {similarities[idx]:.3f}) {documents[idx]}")
    print()

# Test searches
search("Python data science")
search("machine learning prediction")
search("web development JavaScript")
</code></pre>
                            </div>
                        </div>

                        <h2 id="ngrams"><i class="fas fa-link me-2"></i>N-grams</h2>

                        <p><strong>N-grams</strong> are contiguous sequences of N items (usually words or characters) from a text. While unigrams (single words) ignore word order, bigrams (2 words), trigrams (3 words), and higher-order n-grams capture local context and word co-occurrence patterns. This partial preservation of word order addresses a key limitation of simple BoW models.</p>

                        <p>For example, the sentence "I love machine learning" produces these n-grams: <strong>Unigrams</strong>: ["I", "love", "machine", "learning"]; <strong>Bigrams</strong>: ["I love", "love machine", "machine learning"]; <strong>Trigrams</strong>: ["I love machine", "love machine learning"]. Bigrams like "machine learning" capture meaningful phrases that individual words miss.</p>

<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

text = "Natural language processing is a fascinating field of artificial intelligence"

# Generate different n-gram representations
unigram_vec = CountVectorizer(ngram_range=(1, 1))
bigram_vec = CountVectorizer(ngram_range=(2, 2))
trigram_vec = CountVectorizer(ngram_range=(3, 3))
combined_vec = CountVectorizer(ngram_range=(1, 3))

print("N-gram Analysis:")
print("=" * 60)

for name, vec in [("Unigrams (1,1)", unigram_vec), 
                  ("Bigrams (2,2)", bigram_vec),
                  ("Trigrams (3,3)", trigram_vec),
                  ("Combined (1,3)", combined_vec)]:
    vec.fit([text])
    features = vec.get_feature_names_out()
    print(f"\n{name} - {len(features)} features:")
    print(", ".join(features[:10]))
    if len(features) > 10:
        print(f"  ... and {len(features) - 10} more")
</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-info-circle me-2"></i>Character vs Word N-grams</h4>
                            <p><strong>Character n-grams</strong> are useful for handling typos, morphological variations, and languages without clear word boundaries. Word n-grams capture phrases and collocations. Many modern systems use both for robust text representation.</p>
                        </div>

<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer

text = "The quick brown fox jumps"

# Character-level n-grams
char_vec = CountVectorizer(analyzer='char', ngram_range=(2, 4))
char_vec.fit([text])
char_ngrams = char_vec.get_feature_names_out()

print("Character N-grams (2-4):")
print("Sample:", list(char_ngrams[:15]))
print(f"Total: {len(char_ngrams)} features")

# Character n-grams within word boundaries (avoids cross-word n-grams)
char_wb_vec = CountVectorizer(analyzer='char_wb', ngram_range=(2, 4))
char_wb_vec.fit([text])
char_wb_ngrams = char_wb_vec.get_feature_names_out()

print("\nCharacter N-grams (word boundaries):")
print("Sample:", list(char_wb_ngrams[:15]))
print(f"Total: {len(char_wb_ngrams)} features")

# Practical example: handling typos
words = ["machine", "machne", "machin", "learning", "leanring"]
char_vec_typo = CountVectorizer(analyzer='char', ngram_range=(3, 3))
char_matrix = char_vec_typo.fit_transform(words)

from sklearn.metrics.pairwise import cosine_similarity
similarity = cosine_similarity(char_matrix)

print("\nTypo Similarity (character trigrams):")
for i, w1 in enumerate(words):
    for j, w2 in enumerate(words):
        if i < j:
            print(f"  '{w1}' vs '{w2}': {similarity[i,j]:.3f}")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-chart-bar me-2"></i>N-gram Frequency Analysis</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Analysis</span>
                                <span class="badge bg-crimson">Collocations</span>
                            </div>
                            <div class="content">
<pre><code class="language-python">from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

# Sample corpus
corpus = [
    "machine learning is transforming artificial intelligence research",
    "deep learning neural networks achieve state of the art results",
    "natural language processing uses machine learning models",
    "computer vision and machine learning work together",
    "artificial intelligence includes machine learning and deep learning"
]

# Extract bigrams
bigram_vec = CountVectorizer(ngram_range=(2, 2), stop_words='english')
bigram_matrix = bigram_vec.fit_transform(corpus)
bigram_features = bigram_vec.get_feature_names_out()

# Sum frequencies across documents
bigram_freq = np.array(bigram_matrix.sum(axis=0)).flatten()

# Sort by frequency
sorted_indices = bigram_freq.argsort()[::-1]

print("Most Common Bigrams in Corpus:")
print("-" * 40)
for i in sorted_indices[:10]:
    print(f"  '{bigram_features[i]}': {bigram_freq[i]:.0f} occurrences")

# Extract trigrams
trigram_vec = CountVectorizer(ngram_range=(3, 3), stop_words='english')
trigram_matrix = trigram_vec.fit_transform(corpus)
trigram_features = trigram_vec.get_feature_names_out()
trigram_freq = np.array(trigram_matrix.sum(axis=0)).flatten()
sorted_tri = trigram_freq.argsort()[::-1]

print("\nMost Common Trigrams:")
print("-" * 40)
for i in sorted_tri[:5]:
    if trigram_freq[i] > 0:
        print(f"  '{trigram_features[i]}': {trigram_freq[i]:.0f}")
</code></pre>
                            </div>
                        </div>

                        <h2 id="feature-engineering"><i class="fas fa-cogs me-2"></i>Feature Engineering for NLP</h2>

                        <p><strong>Feature engineering</strong> in NLP goes beyond simple word representations to capture linguistic, structural, and domain-specific signals. While modern deep learning can learn features automatically, hand-crafted features remain valuable for improving model interpretability, reducing training data requirements, and combining with neural approaches in hybrid systems.</p>

                        <p>Effective NLP features fall into several categories: <strong>lexical features</strong> (word counts, vocabulary richness), <strong>syntactic features</strong> (POS tag distributions, parse tree depth), <strong>semantic features</strong> (named entities, sentiment words), and <strong>structural features</strong> (sentence length, punctuation patterns). The right feature set depends heavily on your task and domain.</p>

<pre><code class="language-python">import numpy as np
import re
from collections import Counter

def extract_text_features(text):
    """Extract comprehensive features from text."""
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    features = {}
    
    # Basic statistics
    features['char_count'] = len(text)
    features['word_count'] = len(words)
    features['sentence_count'] = len(sentences)
    features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0
    features['avg_sentence_length'] = len(words) / len(sentences) if sentences else 0
    
    # Vocabulary richness
    unique_words = set(w.lower() for w in words)
    features['unique_word_count'] = len(unique_words)
    features['lexical_diversity'] = len(unique_words) / len(words) if words else 0
    
    # Punctuation features
    features['exclamation_count'] = text.count('!')
    features['question_count'] = text.count('?')
    features['comma_count'] = text.count(',')
    
    # Case features
    features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if text else 0
    features['title_case_words'] = sum(1 for w in words if w.istitle())
    
    # Special patterns
    features['digit_count'] = sum(1 for c in text if c.isdigit())
    features['has_url'] = 1 if re.search(r'http[s]?://', text) else 0
    features['has_email'] = 1 if re.search(r'\S+@\S+', text) else 0
    
    return features

# Test on sample texts
texts = [
    "Machine learning is AMAZING! It's transforming every industry.",
    "The quick brown fox jumps over the lazy dog. Simple and short.",
    "Contact us at info@example.com or visit https://example.com for more info!"
]

print("Feature Extraction Results:")
print("=" * 70)
for i, text in enumerate(texts, 1):
    print(f"\nText {i}: '{text[:50]}...'")
    features = extract_text_features(text)
    for key, value in list(features.items())[:8]:
        print(f"  {key}: {value:.3f}" if isinstance(value, float) else f"  {key}: {value}")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-puzzle-piece me-2"></i>Combining Features for Classification</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">ML Pipeline</span>
                                <span class="badge bg-crimson">Feature Union</span>
                            </div>
                            <div class="content">
<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.base import BaseEstimator, TransformerMixin
import numpy as np

class TextStatisticsExtractor(BaseEstimator, TransformerMixin):
    """Extract statistical features from text."""
    
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        features = []
        for text in X:
            words = text.split()
            features.append([
                len(words),  # word count
                len(text),   # char count
                np.mean([len(w) for w in words]) if words else 0,  # avg word length
                len(set(words)) / len(words) if words else 0,  # lexical diversity
                sum(1 for c in text if c.isupper()) / len(text) if text else 0  # uppercase ratio
            ])
        return np.array(features)

# Create combined feature pipeline
combined_features = FeatureUnion([
    ('tfidf', TfidfVectorizer(max_features=100, stop_words='english')),
    ('stats', Pipeline([
        ('extract', TextStatisticsExtractor()),
        ('scale', StandardScaler())
    ]))
])

# Sample data
texts = [
    "Machine learning is revolutionizing data science!",
    "Simple text with few words.",
    "URGENT: This is a very important message with CAPITAL letters!"
]

# Transform
combined_matrix = combined_features.fit_transform(texts)
print(f"Combined feature shape: {combined_matrix.shape}")
print(f"  - TF-IDF features: 100")
print(f"  - Statistical features: 5")
print(f"  - Total: {combined_matrix.shape[1]}")
</code></pre>
                            </div>
                        </div>

                        <h2 id="sparse-vs-dense"><i class="fas fa-th me-2"></i>Sparse vs Dense Representations</h2>

                        <p><strong>Sparse representations</strong> (BoW, TF-IDF) have most values as zero because each document only contains a tiny fraction of the total vocabulary. A 50,000-word vocabulary means 50,000-dimensional vectors, but a typical document might only have 100-500 unique words. <strong>Dense representations</strong> (word embeddings, neural encodings) compress meaning into lower-dimensional vectors (typically 100-1000 dimensions) where most values are non-zero.</p>

                        <p>The choice between sparse and dense depends on your task, data size, and computational constraints. Sparse representations are interpretable and require no training but scale poorly. Dense representations capture semantic relationships and generalize better but require substantial training data and are less interpretable.</p>

<pre><code class="language-python">import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy import sparse

# Sample corpus
corpus = [
    "Machine learning is a subset of artificial intelligence",
    "Deep learning uses neural networks for complex patterns",
    "Natural language processing analyzes human language"
]

# Create sparse TF-IDF matrix
vectorizer = TfidfVectorizer()
sparse_matrix = vectorizer.fit_transform(corpus)

print("Sparse Representation Analysis")
print("=" * 50)
print(f"Matrix shape: {sparse_matrix.shape}")
print(f"Data type: {type(sparse_matrix)}")
print(f"Storage format: {sparse_matrix.format}")

# Analyze sparsity
total_elements = sparse_matrix.shape[0] * sparse_matrix.shape[1]
non_zero = sparse_matrix.nnz
sparsity = (total_elements - non_zero) / total_elements * 100

print(f"\nSparsity Statistics:")
print(f"  Total elements: {total_elements}")
print(f"  Non-zero elements: {non_zero}")
print(f"  Sparsity: {sparsity:.1f}%")

# Memory comparison
dense_matrix = sparse_matrix.toarray()
sparse_memory = sparse_matrix.data.nbytes + sparse_matrix.indices.nbytes + sparse_matrix.indptr.nbytes
dense_memory = dense_matrix.nbytes

print(f"\nMemory Usage:")
print(f"  Sparse format: {sparse_memory:,} bytes")
print(f"  Dense format: {dense_memory:,} bytes")
print(f"  Compression ratio: {dense_memory / sparse_memory:.1f}x")
</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-balance-scale me-2"></i>Sparse vs Dense: When to Use Each</h4>
                            <table class="table table-bordered mt-3">
                                <thead class="table-dark">
                                    <tr>
                                        <th>Aspect</th>
                                        <th>Sparse (BoW/TF-IDF)</th>
                                        <th>Dense (Embeddings)</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td><strong>Dimensionality</strong></td>
                                        <td>High (vocabulary size)</td>
                                        <td>Low (100-1000)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Semantics</strong></td>
                                        <td>No semantic similarity</td>
                                        <td>Captures meaning</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Training</strong></td>
                                        <td>No training needed</td>
                                        <td>Requires large corpus</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Interpretability</strong></td>
                                        <td>High (direct word mapping)</td>
                                        <td>Low (abstract dimensions)</td>
                                    </tr>
                                    <tr>
                                        <td><strong>OOV handling</strong></td>
                                        <td>Poor (unknown = zero)</td>
                                        <td>Subword methods help</td>
                                    </tr>
                                    <tr>
                                        <td><strong>Best for</strong></td>
                                        <td>Search, classification baseline</td>
                                        <td>Semantic tasks, deep learning</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-compress me-2"></i>Dimensionality Reduction for Sparse Vectors</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">SVD</span>
                                <span class="badge bg-crimson">LSA</span>
                            </div>
                            <div class="content">
<pre><code class="language-python">from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import numpy as np

# Larger corpus for meaningful dimensionality reduction
corpus = [
    "Machine learning algorithms learn patterns from data",
    "Deep learning is a type of machine learning",
    "Neural networks power deep learning systems",
    "Natural language processing analyzes text",
    "Computer vision processes images and videos",
    "Reinforcement learning uses rewards and penalties",
    "Supervised learning requires labeled training data",
    "Unsupervised learning finds hidden patterns",
    "Data science combines statistics and programming",
    "Artificial intelligence mimics human intelligence"
]

# Create TF-IDF matrix
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(corpus)

print(f"Original TF-IDF shape: {tfidf_matrix.shape}")

# Apply Truncated SVD (LSA - Latent Semantic Analysis)
n_components = 5
svd = TruncatedSVD(n_components=n_components, random_state=42)
dense_matrix = svd.fit_transform(tfidf_matrix)

print(f"Reduced LSA shape: {dense_matrix.shape}")
print(f"Explained variance ratio: {svd.explained_variance_ratio_.sum():.2%}")

# Compare similarities in both spaces
from sklearn.metrics.pairwise import cosine_similarity

sparse_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)
dense_sim = cosine_similarity(dense_matrix[0:1], dense_matrix)

print("\nSimilarity to Document 0:")
print(f"{'Document':<12} {'Sparse':<10} {'Dense':<10}")
print("-" * 32)
for i in range(min(5, len(corpus))):
    print(f"Doc {i:<8} {sparse_sim[0,i]:<10.3f} {dense_sim[0,i]:<10.3f}")
</code></pre>
                            </div>
                        </div>

                        <h2 id="conclusion"><i class="fas fa-flag-checkered me-2"></i>Conclusion & Next Steps</h2>

                        <p>Text representation is the bridge between human language and machine learning algorithms. We've explored the progression from simple <strong>one-hot encoding</strong> to <strong>Bag of Words</strong>, then to <strong>TF-IDF</strong> weighting that accounts for term importance, and finally to <strong>N-grams</strong> that capture local word context. Each method has its place: one-hot for categorical encoding, BoW for baselines, TF-IDF for search and classification, and N-grams for phrase-aware applications.</p>

                        <p><strong>Feature engineering</strong> extends these representations with hand-crafted signals that capture linguistic and domain-specific patterns. While deep learning has reduced the need for manual feature design, combining classical features with neural approaches often yields the best results. Understanding when to use <strong>sparse vs dense</strong> representations helps you make informed decisions about model architecture and computational trade-offs.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-rocket me-2"></i>Key Takeaways</h4>
                            <ul>
                                <li><strong>One-Hot Encoding</strong>: Simple but doesn't scale; foundation for understanding other methods</li>
                                <li><strong>Bag of Words</strong>: Effective baseline that ignores word order but captures term frequency</li>
                                <li><strong>TF-IDF</strong>: Weights terms by importance; excellent for search and classification</li>
                                <li><strong>N-grams</strong>: Preserves local word context; captures meaningful phrases</li>
                                <li><strong>Feature Engineering</strong>: Domain knowledge encoded as numerical features</li>
                                <li><strong>Sparse vs Dense</strong>: Choose based on interpretability, semantics, and scale requirements</li>
                            </ul>
                        </div>

                        <p>In <strong>Part 4: Word Embeddings</strong>, we'll move beyond sparse, count-based representations to dense vectors that capture semantic meaning. You'll learn how Word2Vec, GloVe, and FastText learn to represent words in continuous vector spaces where similar words have similar vectors—enabling mathematical operations on meaning itself.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-tasks me-2"></i>Practice Exercises</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Hands-On</span>
                                <span class="badge bg-crimson">Recommended</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><strong>Document Classifier</strong>: Build a TF-IDF + Naive Bayes classifier for the 20 Newsgroups dataset</li>
                                    <li><strong>Search Engine</strong>: Create a simple search engine using TF-IDF cosine similarity</li>
                                    <li><strong>N-gram Analysis</strong>: Extract the most common bigrams and trigrams from a book corpus</li>
                                    <li><strong>Feature Comparison</strong>: Compare classification accuracy with BoW, TF-IDF, and TF-IDF + statistical features</li>
                                    <li><strong>LSA Exploration</strong>: Apply TruncatedSVD to TF-IDF and visualize document clusters in 2D</li>
                                </ol>
                            </div>
                        </div>

                        <div class="related-posts">
                            <h3><i class="fas fa-book-reader me-2"></i>Continue the NLP Series</h3>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 2: Tokenization & Text Cleaning</h5>
                                <p class="text-muted small mb-2">Convert raw text into usable input for NLP models.</p>
                                <a href="nlp-tokenization-text-cleaning.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 4: Word Embeddings</h5>
                                <p class="text-muted small mb-2">Capture meaning and similarity with Word2Vec, GloVe, and FastText.</p>
                                <a href="nlp-word-embeddings.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 5: Statistical Language Models & N-grams</h5>
                                <p class="text-muted small mb-2">Understand probabilistic models of language and sequence prediction.</p>
                                <a href="nlp-statistical-language-models.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.</p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook"><i class="fab fa-facebook-f"></i></a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter"><i class="fab fa-twitter"></i></a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn"><i class="fab fa-linkedin-in"></i></a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube"><i class="fab fa-youtube"></i></a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram"><i class="fab fa-instagram"></i></a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest"><i class="fab fa-pinterest-p"></i></a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email"><i class="fas fa-envelope"></i></a>
                    </div>
                </div>
            </div>
            <hr class="bg-secondary">
            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small"><i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a></p>
                    <p class="small mt-3"><a href="/" class="text-light text-decoration-none">Home</a> | <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a></p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">Enjoying this content? ☕ <a href="https://buymeacoffee.com/itswzee" target="_blank" class="text-light" style="text-decoration: underline;">Keep me caffeinated</a> to keep the pixels flowing!</p>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top"><i class="fas fa-arrow-up"></i></button>
    <!-- Category Indicator -->
    <div id="categoryIndicator" class="category-indicator" title="Current Section">
        <i class="fas fa-tag"></i><span id="categoryText">Technology</span>
    </div>
    <script src="../../../js/cookie-consent.js"></script>
    <script src="../../../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <script>
        const themes = { 'prism-theme': 'Tomorrow Night', 'prism-default': 'Default', 'prism-dark': 'Dark', 'prism-twilight': 'Twilight', 'prism-okaidia': 'Okaidia', 'prism-solarizedlight': 'Solarized Light' };
        const savedTheme = localStorage.getItem('prism-theme') || 'prism-theme';
        function switchTheme(themeId) { Object.keys(themes).forEach(id => { const link = document.getElementById(id); if (link) link.disabled = true; }); const selectedLink = document.getElementById(themeId); if (selectedLink) { selectedLink.disabled = false; localStorage.setItem('prism-theme', themeId); } document.querySelectorAll('div.code-toolbar select').forEach(dropdown => { dropdown.value = themeId; }); setTimeout(() => Prism.highlightAll(), 10); }
        document.addEventListener('DOMContentLoaded', function() { switchTheme(savedTheme); });
        Prism.plugins.toolbar.registerButton('theme-switcher', function(env) { const select = document.createElement('select'); select.setAttribute('aria-label', 'Select code theme'); Object.keys(themes).forEach(themeId => { const option = document.createElement('option'); option.value = themeId; option.textContent = themes[themeId]; if (themeId === savedTheme) option.selected = true; select.appendChild(option); }); select.addEventListener('change', function(e) { switchTheme(e.target.value); }); return select; });
    </script>

    <script>
        document.addEventListener('DOMContentLoaded', function() { const scrollToTopBtn = document.getElementById('scrollToTop'); window.addEventListener('scroll', function() { if (window.scrollY > 300) { scrollToTopBtn.classList.add('show'); } else { scrollToTopBtn.classList.remove('show'); } }); scrollToTopBtn.addEventListener('click', function() { window.scrollTo({ top: 0, behavior: 'smooth' }); }); });
        function openNav() { document.getElementById('tocSidenav').classList.add('open'); document.getElementById('tocOverlay').classList.add('show'); document.body.style.overflow = 'hidden'; }
        function closeNav() { document.getElementById('tocSidenav').classList.remove('open'); document.getElementById('tocOverlay').classList.remove('show'); document.body.style.overflow = 'auto'; }
        document.addEventListener('keydown', function(e) { if (e.key === 'Escape') closeNav(); });
    </script>

            </body>
</html>
