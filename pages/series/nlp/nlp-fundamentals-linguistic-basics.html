<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Part 1 of the Complete NLP Series: Learn NLP fundamentals, linguistic basics, levels of language (morphology, syntax, semantics, pragmatics), NLP pipelines, and the evolution from rule-based to neural approaches." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="NLP, Natural Language Processing, Linguistics, Morphology, Syntax, Semantics, Pragmatics, NLP Pipelines, Text Processing, Machine Learning, Deep Learning" />
    <meta property="og:title" content="NLP Fundamentals & Linguistic Basics - Complete NLP Series Part 1" />
    <meta property="og:description" content="Master the foundations of NLP: understand what language is, how machines process it, and explore the complete NLP pipeline from text to insights." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-27" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>NLP Fundamentals & Linguistic Basics - Complete NLP Series Part 1 - Wasil Zafar</title>

    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Font Awesome Icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />

    <!-- Custom Styles -->
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />

    <!-- Prism.js Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />

    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <!-- Google Consent Mode v2 -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        
        gtag('consent', 'default', {
            'ad_storage': 'denied',
            'ad_user_data': 'denied',
            'ad_personalization': 'denied',
            'analytics_storage': 'denied',
            'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE']
        });
        
        gtag('consent', 'default', {
            'ad_storage': 'granted',
            'ad_user_data': 'granted',
            'ad_personalization': 'granted',
            'analytics_storage': 'granted'
        });
        
        gtag('set', 'url_passthrough', true);
    </script>

    <!-- Google Tag Manager -->
    <script>
        (function(w, d, s, l, i) {
            w[l] = w[l] || [];
            w[l].push({
                'gtm.start': new Date().getTime(),
                event: 'gtm.js'
            });
            var f = d.getElementsByTagName(s)[0],
                j = d.createElement(s),
                dl = l != 'dataLayer' ? '&l=' + l : '';
            j.async = true;
            j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    </head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript>
        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe>
    </noscript>

    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/">
                <span class="gradient-text">Wasil Zafar</span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link" href="/">Home</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#about">About</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#certifications">Certifications</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#interests">Interests</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link">
                    <i class="fas fa-arrow-left me-2"></i>Back to Technology
                </a>
                <h1 class="display-4 fw-bold mb-3">NLP Fundamentals & Linguistic Basics</h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 27, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-1"></i>35 min read</span>
                    <button onclick="window.print()" class="print-btn" title="Print this article">
                        <i class="fas fa-print"></i> Print
                    </button>
                </div>
                <p class="lead">Part 1 of 16: Understand what language is and how machines process it—from linguistic foundations to modern NLP pipelines.</p>
            </div>
        </div>
    </section>

    <!-- TOC Toggle Button -->
    <button class="toc-toggle-btn" onclick="openNav()" title="Table of Contents" aria-label="Open Table of Contents">
        <i class="fas fa-list"></i>
    </button>

    <!-- Side Navigation -->
    <div id="tocSidenav" class="sidenav-toc">
        <div class="toc-header">
            <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
            <button class="closebtn" onclick="closeNav()" aria-label="Close">&times;</button>
        </div>
        <ol>
            <li><a href="#introduction" onclick="closeNav()">Introduction to NLP</a></li>
            <li><a href="#what-is-nlp" onclick="closeNav()">What is NLP?</a>
                <ul>
                    <li><a href="#use-cases" onclick="closeNav()">Use Cases & Applications</a></li>
                    <li><a href="#limitations" onclick="closeNav()">Limitations & Challenges</a></li>
                </ul>
            </li>
            <li><a href="#levels-of-language" onclick="closeNav()">Levels of Language</a>
                <ul>
                    <li><a href="#phonetics" onclick="closeNav()">Phonetics & Phonology</a></li>
                    <li><a href="#morphology" onclick="closeNav()">Morphology</a></li>
                    <li><a href="#syntax" onclick="closeNav()">Syntax</a></li>
                    <li><a href="#semantics" onclick="closeNav()">Semantics</a></li>
                    <li><a href="#pragmatics" onclick="closeNav()">Pragmatics & Discourse</a></li>
                </ul>
            </li>
            <li><a href="#nlp-approaches" onclick="closeNav()">NLP Approaches</a>
                <ul>
                    <li><a href="#rule-based" onclick="closeNav()">Rule-Based NLP</a></li>
                    <li><a href="#statistical" onclick="closeNav()">Statistical NLP</a></li>
                    <li><a href="#neural" onclick="closeNav()">Neural NLP</a></li>
                </ul>
            </li>
            <li><a href="#text-vs-speech" onclick="closeNav()">Text vs Speech NLP</a></li>
            <li><a href="#pipelines" onclick="closeNav()">NLP Pipelines & Workflows</a></li>
            <li><a href="#datasets" onclick="closeNav()">Datasets, Corpora & Annotation</a></li>
            <li><a href="#conclusion" onclick="closeNav()">Conclusion & Next Steps</a></li>
        </ol>
    </div>

    <!-- Overlay -->
    <div id="tocOverlay" class="sidenav-overlay" onclick="closeNav()"></div>

    <!-- Main Content -->
    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="blog-content">
                        
                        <h2 id="introduction"><i class="fas fa-lightbulb me-2"></i>Introduction to NLP</h2>
                        
                        <p>Natural Language Processing (NLP) is the bridge between human communication and machine understanding. In this first part of our comprehensive 16-part series, we'll explore the fundamental concepts that underpin all NLP systems.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-star me-2"></i>Key Insight</h4>
                            <p><strong>NLP is fundamentally about enabling computers to understand, interpret, and generate human language in a way that is both meaningful and useful.</strong></p>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-map-signs me-2"></i>Complete NLP Series Navigation</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">16-Part Series</span>
                                <span class="badge bg-crimson">NLP Mastery</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><strong>NLP Fundamentals & Linguistic Basics (This Guide)</strong></li>
                                    <li><a href="nlp-tokenization-text-cleaning.html">Tokenization & Text Cleaning</a></li>
                                    <li><a href="nlp-text-representation-features.html">Text Representation & Feature Engineering</a></li>
                                    <li><a href="nlp-word-embeddings.html">Word Embeddings</a></li>
                                    <li><a href="nlp-statistical-language-models.html">Statistical Language Models & N-grams</a></li>
                                    <li><a href="nlp-neural-networks.html">Neural Networks for NLP</a></li>
                                    <li><a href="nlp-rnn-lstm-gru.html">RNNs, LSTMs & GRUs</a></li>
                                    <li><a href="nlp-transformers-attention.html">Transformers & Attention Mechanism</a></li>
                                    <li><a href="nlp-pretrained-models-transfer-learning.html">Pretrained Language Models & Transfer Learning</a></li>
                                    <li><a href="nlp-gpt-text-generation.html">GPT Models & Text Generation</a></li>
                                    <li><a href="nlp-core-tasks.html">Core NLP Tasks</a></li>
                                    <li><a href="nlp-advanced-tasks.html">Advanced NLP Tasks</a></li>
                                    <li><a href="nlp-multilingual-crosslingual.html">Multilingual & Cross-lingual NLP</a></li>
                                    <li><a href="nlp-evaluation-ethics.html">Evaluation, Ethics & Responsible NLP</a></li>
                                    <li><a href="nlp-systems-production.html">NLP Systems, Optimization & Production</a></li>
                                    <li><a href="nlp-cutting-edge-research.html">Cutting-Edge & Research Topics</a></li>
                                </ol>
                            </div>
                        </div>

                        <h2 id="what-is-nlp"><i class="fas fa-brain me-2"></i>What is NLP?</h2>
                        
                        <p>Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and human language. It combines computational linguistics—the rule-based modeling of language—with statistical, machine learning, and deep learning techniques to enable machines to process, understand, and generate human language in meaningful ways.</p>

                        <p>At its core, NLP addresses the fundamental challenge of <strong>bridging the gap between human communication and machine computation</strong>. While humans effortlessly understand language with all its nuances, ambiguities, and contextual dependencies, computers operate on discrete symbols and mathematical operations. NLP provides the algorithms and models that translate between these two worlds.</p>

                        <p>The field encompasses two primary directions:</p>
                        <ul>
                            <li><strong>Natural Language Understanding (NLU):</strong> Enabling machines to comprehend and interpret human language input—extracting meaning, intent, and relationships from text or speech.</li>
                            <li><strong>Natural Language Generation (NLG):</strong> Enabling machines to produce human-like language output—creating coherent text, responses, summaries, or translations.</li>
                        </ul>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>The NLP Challenge</h4>
                            <p>Human language is inherently <strong>ambiguous, contextual, and evolving</strong>. The sentence "I saw her duck" could mean observing someone's pet bird OR watching someone lower their head. NLP systems must navigate these complexities that humans resolve unconsciously through context and world knowledge.</p>
                        </div>

<pre><code class="language-python"># Introduction to NLP with Python
# Demonstrating basic text analysis concepts

import re
from collections import Counter

# Sample text for analysis
text = """Natural Language Processing enables computers to understand human language.
NLP combines linguistics and computer science to analyze text and speech.
Modern NLP uses deep learning for remarkable language understanding."""

# Basic text statistics
words = text.lower().split()
sentences = text.split('.')

print("=== Basic NLP Statistics ===")
print(f"Total characters: {len(text)}")
print(f"Total words: {len(words)}")
print(f"Total sentences: {len([s for s in sentences if s.strip()])}")
print(f"Unique words: {len(set(words))}")
print(f"Average word length: {sum(len(w) for w in words) / len(words):.2f}")

# Word frequency analysis
word_freq = Counter(words)
print(f"\nTop 5 most common words:")
for word, count in word_freq.most_common(5):
    print(f"  '{word}': {count} occurrences")
</code></pre>

                        <h3 id="use-cases">Use Cases & Applications</h3>
                        
                        <p>NLP powers countless applications that we interact with daily, often without realizing the sophisticated language processing happening behind the scenes. These applications span virtually every industry and have transformed how we interact with technology.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-search me-2"></i>Search & Information Retrieval</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Core Application</span>
                                <span class="badge bg-crimson">High Impact</span>
                            </div>
                            <div class="content">
                                <p>Search engines like Google use NLP to understand query intent, match relevant documents, and provide direct answers. Modern search goes far beyond keyword matching—it understands synonyms, context, and user intent to deliver relevant results.</p>
                                <ul>
                                    <li>Query understanding and expansion</li>
                                    <li>Document ranking and relevance scoring</li>
                                    <li>Featured snippets and direct answers</li>
                                    <li>Voice search and conversational queries</li>
                                </ul>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-robot me-2"></i>Virtual Assistants & Chatbots</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Conversational AI</span>
                                <span class="badge bg-crimson">Growing Fast</span>
                            </div>
                            <div class="content">
                                <p>Siri, Alexa, Google Assistant, and enterprise chatbots rely on NLP to interpret user requests, maintain conversation context, and generate appropriate responses.</p>
                                <ul>
                                    <li><strong>Intent classification:</strong> Understanding what the user wants</li>
                                    <li><strong>Entity extraction:</strong> Identifying key information (dates, names, locations)</li>
                                    <li><strong>Dialogue management:</strong> Maintaining conversation flow</li>
                                    <li><strong>Response generation:</strong> Producing natural, helpful replies</li>
                                </ul>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-globe me-2"></i>Machine Translation</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Cross-lingual</span>
                                <span class="badge bg-crimson">Transformer-based</span>
                            </div>
                            <div class="content">
                                <p>Services like Google Translate and DeepL use neural machine translation to convert text between languages while preserving meaning, tone, and context. Modern systems handle over 100 languages with near-human quality for common language pairs.</p>
                            </div>
                        </div>

                        <p>Other major applications include:</p>
                        <ul>
                            <li><strong>Sentiment Analysis:</strong> Understanding opinions in reviews, social media, and customer feedback</li>
                            <li><strong>Email Filtering:</strong> Spam detection and email categorization</li>
                            <li><strong>Content Recommendation:</strong> Suggesting articles, products, and media based on text analysis</li>
                            <li><strong>Healthcare:</strong> Clinical note analysis, medical literature mining, and patient communication</li>
                            <li><strong>Legal:</strong> Contract analysis, legal document review, and case law research</li>
                            <li><strong>Finance:</strong> News sentiment analysis, fraud detection, and automated reporting</li>
                        </ul>

                        <h3 id="limitations">Limitations & Challenges</h3>
                        
                        <p>Despite remarkable progress, NLP systems face significant challenges that highlight the complexity of human language. Understanding these limitations is crucial for building realistic expectations and designing effective solutions.</p>

                        <h4>Ambiguity</h4>
                        <p>Language is inherently ambiguous at multiple levels. Consider the sentence "The chicken is ready to eat"—does this mean the chicken is prepared as food, or that a live chicken is hungry? Humans resolve such ambiguities effortlessly using context and world knowledge, but machines struggle with these distinctions.</p>

<pre><code class="language-python"># Demonstrating linguistic ambiguity challenges

# Lexical ambiguity - same word, different meanings
lexical_examples = [
    ("bank", ["financial institution", "river edge", "to rely on"]),
    ("bat", ["flying mammal", "sports equipment", "to hit"]),
    ("light", ["illumination", "not heavy", "to ignite"]),
]

print("=== Lexical Ambiguity Examples ===")
for word, meanings in lexical_examples:
    print(f"\n'{word}' can mean:")
    for i, meaning in enumerate(meanings, 1):
        print(f"  {i}. {meaning}")

# Syntactic ambiguity - same sentence, different parses
syntactic_examples = [
    ("I saw the man with the telescope", 
     ["I used a telescope to see the man", 
      "I saw a man who had a telescope"]),
    ("Flying planes can be dangerous",
     ["Planes that fly are dangerous",
      "The act of piloting planes is dangerous"]),
]

print("\n=== Syntactic Ambiguity Examples ===")
for sentence, interpretations in syntactic_examples:
    print(f"\nSentence: '{sentence}'")
    print("Possible interpretations:")
    for i, interp in enumerate(interpretations, 1):
        print(f"  {i}. {interp}")
</code></pre>

                        <h4>Context Dependence</h4>
                        <p>The meaning of words and sentences often depends heavily on surrounding context, prior conversation, shared knowledge, and real-world situations. Pronouns like "it" and "they" require understanding what was previously mentioned. Sarcasm, irony, and humor depend on detecting mismatches between literal and intended meaning.</p>

                        <h4>Data Bias and Fairness</h4>
                        <p>NLP models learn from training data, which often contains societal biases related to gender, race, religion, and other sensitive attributes. Models can perpetuate and even amplify these biases, leading to unfair or harmful outputs. Addressing bias requires careful dataset curation, model auditing, and ongoing monitoring.</p>

                        <h4>Resource Requirements</h4>
                        <p>State-of-the-art NLP models require massive computational resources for training—GPT-3 reportedly cost millions of dollars to train. This creates barriers for smaller organizations and researchers, and raises environmental concerns about energy consumption.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-exclamation-triangle me-2"></i>Key Challenges Summary</h4>
                            <ul>
                                <li><strong>Ambiguity:</strong> Words and sentences have multiple interpretations</li>
                                <li><strong>Context:</strong> Meaning depends on surrounding information and world knowledge</li>
                                <li><strong>Common Sense:</strong> Machines lack intuitive understanding of how the world works</li>
                                <li><strong>Low-resource Languages:</strong> Most NLP research focuses on English; many languages lack data</li>
                                <li><strong>Domain Adaptation:</strong> Models trained on general text may fail in specialized domains</li>
                                <li><strong>Evaluation:</strong> Measuring true language understanding remains difficult</li>
                            </ul>
                        </div>

                        <h2 id="levels-of-language"><i class="fas fa-layer-group me-2"></i>Levels of Language</h2>
                        
                        <p>Linguistics—the scientific study of language—analyzes human communication at multiple hierarchical levels. Each level addresses different aspects of language structure, from individual sounds to extended discourse. Understanding these levels is fundamental for NLP practitioners, as different tasks operate at different linguistic levels.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-layer-group me-2"></i>The Linguistic Hierarchy</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Foundation</span>
                                <span class="badge bg-crimson">Core Concept</span>
                            </div>
                            <div class="content">
                                <p>Language can be analyzed at progressively higher levels of abstraction:</p>
                                <ol>
                                    <li><strong>Phonetics/Phonology:</strong> Sounds and sound patterns</li>
                                    <li><strong>Morphology:</strong> Word formation and structure</li>
                                    <li><strong>Syntax:</strong> Sentence structure and grammar</li>
                                    <li><strong>Semantics:</strong> Meaning of words and sentences</li>
                                    <li><strong>Pragmatics:</strong> Language use in context</li>
                                    <li><strong>Discourse:</strong> Extended text and conversation structure</li>
                                </ol>
                            </div>
                        </div>

                        <h3 id="phonetics">Phonetics & Phonology</h3>
                        
                        <p><strong>Phonetics</strong> studies the physical properties of speech sounds—how they are produced by the vocal tract, transmitted through the air, and perceived by the ear. <strong>Phonology</strong> studies the abstract sound patterns and rules that govern how sounds function in particular languages.</p>

                        <p>While phonetics and phonology are primarily relevant to speech processing, they also matter for text NLP in several ways:</p>
                        <ul>
                            <li><strong>Spelling correction:</strong> Many misspellings are phonetically motivated ("definately" for "definitely")</li>
                            <li><strong>Named entity normalization:</strong> Different spellings of names may represent the same phonetic form</li>
                            <li><strong>Text-to-speech:</strong> Converting text to phonetic representations for speech synthesis</li>
                            <li><strong>Rhyme detection:</strong> Poetry analysis and song lyric generation</li>
                        </ul>

                        <p>Key phonetic concepts include:</p>
                        <ul>
                            <li><strong>Phonemes:</strong> The smallest units of sound that distinguish meaning (e.g., /p/ vs /b/ in "pat" vs "bat")</li>
                            <li><strong>Allophones:</strong> Variant pronunciations of the same phoneme in different contexts</li>
                            <li><strong>International Phonetic Alphabet (IPA):</strong> A standardized notation system for representing speech sounds</li>
                        </ul>

<pre><code class="language-python"># Working with phonetic representations
# Using the pronouncing library for CMU Pronouncing Dictionary

# Note: Run 'pip install pronouncing' first
import pronouncing

# Get phonetic pronunciations
words = ["natural", "language", "processing", "computer"]

print("=== Phonetic Pronunciations (ARPAbet) ===")
for word in words:
    pronunciations = pronouncing.phones_for_word(word)
    if pronunciations:
        print(f"{word}: {pronunciations[0]}")
    else:
        print(f"{word}: (not found)")

# Find rhyming words
print("\n=== Words that Rhyme with 'processing' ===")
rhymes = pronouncing.rhymes("processing")
print(f"Found {len(rhymes)} rhymes: {rhymes[:10]}...")

# Count syllables
print("\n=== Syllable Counts ===")
for word in words:
    phones = pronouncing.phones_for_word(word)
    if phones:
        syllables = pronouncing.syllable_count(phones[0])
        print(f"{word}: {syllables} syllables")
</code></pre>

                        <h3 id="morphology">Morphology</h3>
                        
                        <p><strong>Morphology</strong> is the study of the internal structure of words. It analyzes how words are formed from smaller meaningful units called <strong>morphemes</strong>. Understanding morphology is essential for NLP tasks like stemming, lemmatization, and handling out-of-vocabulary words.</p>

                        <p>Words can be broken down into different types of morphemes:</p>
                        <ul>
                            <li><strong>Roots/Stems:</strong> The core meaning unit that carries the main semantic content (e.g., "play" in "playing")</li>
                            <li><strong>Prefixes:</strong> Morphemes added before the root (e.g., "un-" in "unhappy", "re-" in "rebuild")</li>
                            <li><strong>Suffixes:</strong> Morphemes added after the root (e.g., "-ing" in "playing", "-ness" in "happiness")</li>
                            <li><strong>Infixes:</strong> Morphemes inserted within a root (rare in English, common in other languages)</li>
                        </ul>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>Morpheme Types</h4>
                            <p><strong>Free morphemes</strong> can stand alone as words ("book", "happy"). <strong>Bound morphemes</strong> must attach to other morphemes ("-ing", "un-", "-ly"). Analyzing these patterns helps NLP systems understand word relationships and handle novel word forms.</p>
                        </div>

                        <p>Morphological processes in English include:</p>
                        <ul>
                            <li><strong>Inflection:</strong> Grammatical variations that don't change word class (walk → walks, walked, walking)</li>
                            <li><strong>Derivation:</strong> Creating new words, often changing word class (happy → unhappy, happiness, happily)</li>
                            <li><strong>Compounding:</strong> Combining free morphemes (bookshelf, blackboard, sunflower)</li>
                        </ul>

<pre><code class="language-python"># Morphological analysis with NLTK
import nltk
from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer

# Download required resources
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

# Initialize stemmers and lemmatizer
porter = PorterStemmer()
lancaster = LancasterStemmer()
lemmatizer = WordNetLemmatizer()

# Test words with various morphological forms
words = ["running", "ran", "runs", "runner", "happily", "happiness", 
         "unhappy", "studies", "studying", "better", "best"]

print("=== Stemming Comparison ===")
print(f"{'Word':<12} {'Porter':<12} {'Lancaster':<12}")
print("-" * 36)
for word in words:
    print(f"{word:<12} {porter.stem(word):<12} {lancaster.stem(word):<12}")

print("\n=== Lemmatization (with POS) ===")
# Lemmatization requires part-of-speech for best results
# 'v' = verb, 'n' = noun, 'a' = adjective, 'r' = adverb
test_cases = [
    ("running", "v"),
    ("runs", "v"),
    ("better", "a"),
    ("studies", "n"),
    ("studies", "v"),
]

for word, pos in test_cases:
    lemma = lemmatizer.lemmatize(word, pos=pos)
    print(f"{word} (as {pos}): {lemma}")
</code></pre>

                        <h3 id="syntax">Syntax</h3>
                        
                        <p><strong>Syntax</strong> studies the rules and principles governing sentence structure—how words combine to form phrases, clauses, and sentences. It defines the grammatical relationships between words and determines whether a sequence of words is well-formed in a language.</p>

                        <p>Key syntactic concepts include:</p>
                        <ul>
                            <li><strong>Parts of Speech (POS):</strong> Grammatical categories like nouns, verbs, adjectives, adverbs, prepositions</li>
                            <li><strong>Phrases:</strong> Groups of words functioning as a unit (noun phrases, verb phrases, prepositional phrases)</li>
                            <li><strong>Constituents:</strong> Structural units that can be replaced, moved, or coordinated as a whole</li>
                            <li><strong>Parse Trees:</strong> Hierarchical representations of sentence structure</li>
                            <li><strong>Dependencies:</strong> Directed relationships between words (subject-verb, modifier-noun)</li>
                        </ul>

<pre><code class="language-python"># Syntactic analysis with spaCy
import spacy

# Load English language model
nlp = spacy.load("en_core_web_sm")

# Analyze a sentence
sentence = "The quick brown fox jumps over the lazy dog."
doc = nlp(sentence)

# Part-of-Speech tagging
print("=== Part-of-Speech Tags ===")
print(f"{'Token':<10} {'POS':<8} {'Tag':<8} {'Explanation'}")
print("-" * 50)
for token in doc:
    print(f"{token.text:<10} {token.pos_:<8} {token.tag_:<8} {spacy.explain(token.tag_)}")

# Dependency parsing
print("\n=== Dependency Relations ===")
print(f"{'Token':<10} {'Dep':<12} {'Head':<10} {'Children'}")
print("-" * 50)
for token in doc:
    children = [child.text for child in token.children]
    print(f"{token.text:<10} {token.dep_:<12} {token.head.text:<10} {children}")

# Noun chunks (noun phrases)
print("\n=== Noun Phrases ===")
for chunk in doc.noun_chunks:
    print(f"  '{chunk.text}' (root: {chunk.root.text})")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-sitemap me-2"></i>Constituency vs Dependency Parsing</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Syntax</span>
                                <span class="badge bg-crimson">Key Distinction</span>
                            </div>
                            <div class="content">
                                <p><strong>Constituency parsing</strong> breaks sentences into nested constituents (phrases within phrases), producing tree structures that show hierarchical groupings. It answers "what are the parts?"</p>
                                <p><strong>Dependency parsing</strong> identifies direct grammatical relationships between words, showing which word modifies or is governed by which other word. It answers "how are words related?"</p>
                                <p>Modern NLP often prefers dependency parsing because it's simpler, produces more consistent cross-lingual annotations, and captures semantic relationships more directly.</p>
                            </div>
                        </div>

                        <h3 id="semantics">Semantics</h3>
                        
                        <p><strong>Semantics</strong> is the study of meaning in language. While syntax tells us whether a sentence is grammatically well-formed, semantics tells us what it means. This is perhaps the most challenging level for NLP, as meaning is abstract, context-dependent, and deeply connected to human knowledge and experience.</p>

                        <p>Semantics operates at multiple levels:</p>
                        <ul>
                            <li><strong>Lexical Semantics:</strong> The meaning of individual words (word senses, synonymy, antonymy, hyponymy)</li>
                            <li><strong>Compositional Semantics:</strong> How word meanings combine to form phrase and sentence meanings</li>
                            <li><strong>Distributional Semantics:</strong> Meaning derived from patterns of word co-occurrence ("you shall know a word by the company it keeps")</li>
                        </ul>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>Word Sense Disambiguation</h4>
                            <p>A central challenge in lexical semantics is <strong>polysemy</strong>—words having multiple related meanings. The word "bank" can mean a financial institution, a river edge, or to tilt an aircraft. NLP systems must disambiguate the intended sense based on context.</p>
                        </div>

<pre><code class="language-python"># Exploring lexical semantics with WordNet
import nltk
from nltk.corpus import wordnet as wn

# Download WordNet if needed
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

# Explore word senses (polysemy)
word = "bank"
synsets = wn.synsets(word)

print(f"=== Word Senses for '{word}' ===")
print(f"Found {len(synsets)} different senses:\n")
for i, syn in enumerate(synsets[:5], 1):  # Show first 5
    print(f"{i}. {syn.name()}")
    print(f"   Definition: {syn.definition()}")
    print(f"   Examples: {syn.examples()[:2]}")
    print()

# Semantic relationships
print("=== Semantic Relations for 'dog' ===")
dog = wn.synset('dog.n.01')

# Hypernyms (is-a, more general)
print(f"\nHypernyms (dog IS A...):")
for hyp in dog.hypernyms():
    print(f"  - {hyp.name()}: {hyp.definition()}")

# Hyponyms (is-a, more specific)
print(f"\nHyponyms (... IS A dog):")
for hypo in dog.hyponyms()[:5]:
    print(f"  - {hypo.name()}")

# Synonyms (words in the same synset)
print(f"\nSynonyms/Lemmas:")
print(f"  {[lemma.name() for lemma in dog.lemmas()]}")
</code></pre>

                        <p>Key semantic relationships include:</p>
                        <ul>
                            <li><strong>Synonymy:</strong> Words with similar meanings (big/large, happy/joyful)</li>
                            <li><strong>Antonymy:</strong> Words with opposite meanings (hot/cold, buy/sell)</li>
                            <li><strong>Hyponymy/Hypernymy:</strong> Is-a relationships (dog is a mammal, mammal is an animal)</li>
                            <li><strong>Meronymy/Holonymy:</strong> Part-of relationships (wheel is part of car)</li>
                            <li><strong>Entailment:</strong> If X then Y ("snoring" entails "sleeping")</li>
                        </ul>

                        <h3 id="pragmatics">Pragmatics & Discourse</h3>
                        
                        <p><strong>Pragmatics</strong> studies how context contributes to meaning beyond the literal semantic content. It addresses how speakers use language to accomplish goals, how listeners infer intended meaning, and how utterances function in real communicative situations.</p>

                        <p>Key pragmatic phenomena include:</p>
                        <ul>
                            <li><strong>Speech Acts:</strong> The actions performed through language (requesting, promising, warning, apologizing)</li>
                            <li><strong>Implicature:</strong> Meaning implied but not explicitly stated ("It's cold in here" may imply "Please close the window")</li>
                            <li><strong>Presupposition:</strong> Background assumptions taken for granted ("Have you stopped smoking?" presupposes you once smoked)</li>
                            <li><strong>Deixis:</strong> Context-dependent references ("I", "here", "now", "this")</li>
                            <li><strong>Reference Resolution:</strong> Determining what pronouns and other referring expressions refer to</li>
                        </ul>

                        <div class="experiment-card">
                            <h4><i class="fas fa-comments me-2"></i>Grice's Maxims of Conversation</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Pragmatics</span>
                                <span class="badge bg-crimson">Foundational Theory</span>
                            </div>
                            <div class="content">
                                <p>Philosopher Paul Grice proposed that conversations follow implicit cooperative principles:</p>
                                <ul>
                                    <li><strong>Quantity:</strong> Be as informative as required, but not more</li>
                                    <li><strong>Quality:</strong> Only say what you believe to be true</li>
                                    <li><strong>Relation:</strong> Be relevant to the conversation</li>
                                    <li><strong>Manner:</strong> Be clear, brief, and orderly</li>
                                </ul>
                                <p>Violations of these maxims often signal implicature—the speaker intends the listener to infer additional meaning.</p>
                            </div>
                        </div>

                        <p><strong>Discourse Analysis</strong> studies language beyond the sentence level—how sentences connect to form coherent texts and conversations. It examines:</p>
                        <ul>
                            <li><strong>Coherence:</strong> How texts hang together logically and thematically</li>
                            <li><strong>Cohesion:</strong> Linguistic devices that connect sentences (pronouns, conjunctions, ellipsis)</li>
                            <li><strong>Discourse Relations:</strong> How clauses and sentences relate (cause-effect, contrast, elaboration)</li>
                            <li><strong>Topic Structure:</strong> How topics are introduced, maintained, and shifted</li>
                        </ul>

<pre><code class="language-python"># Coreference resolution - tracking entity mentions across text
import spacy

# Load spaCy with coreference support
# Note: Requires 'pip install spacy-experimental' and the coref model
nlp = spacy.load("en_core_web_sm")

# Example text with pronouns and references
text = """John went to the store. He bought some apples.
The store was closing soon, so he hurried.
John gave the apples to Mary. She was grateful."""

doc = nlp(text)

# Basic entity tracking (simplified approach)
print("=== Entity Mentions Across Text ===")
print(f"\nText: {text}\n")

# Extract named entities
print("Named Entities:")
for ent in doc.ents:
    print(f"  - {ent.text} ({ent.label_})")

# Find pronouns and their potential antecedents
print("\nPronouns found:")
for token in doc:
    if token.pos_ == "PRON" and token.text.lower() in ["he", "she", "it", "they"]:
        print(f"  - '{token.text}' at position {token.i}")

print("\n(Full coreference resolution requires specialized models")
print("like neuralcoref or spacy-experimental coreference)")
</code></pre>

                        <h2 id="nlp-approaches"><i class="fas fa-code-branch me-2"></i>NLP Approaches</h2>

                        <p>The history of NLP can be characterized by three major paradigms, each representing different philosophies about how to process language computationally. Understanding these approaches helps contextualize modern techniques and recognize when different methods are appropriate.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-history me-2"></i>Evolution of NLP</h4>
                            <p>NLP has evolved from <strong>hand-crafted rules</strong> (1950s-1980s) to <strong>statistical learning</strong> (1990s-2010s) to <strong>deep learning</strong> (2010s-present). Each paradigm built on insights from previous approaches, and elements of all three remain relevant today.</p>
                        </div>

                        <h3 id="rule-based">Rule-Based NLP</h3>
                        
                        <p>The earliest NLP systems relied on <strong>hand-crafted rules</strong> written by linguists and knowledge engineers. These systems encode explicit linguistic knowledge in the form of grammars, patterns, and if-then rules. While largely superseded by statistical methods for many tasks, rule-based approaches remain valuable for specific applications.</p>

                        <p>Characteristics of rule-based NLP:</p>
                        <ul>
                            <li><strong>Explicit knowledge:</strong> Rules are written by humans based on linguistic expertise</li>
                            <li><strong>Interpretable:</strong> Easy to understand why the system made a particular decision</li>
                            <li><strong>Precise:</strong> Can achieve very high precision for well-defined patterns</li>
                            <li><strong>Labor-intensive:</strong> Requires significant effort to create and maintain rules</li>
                            <li><strong>Brittle:</strong> Struggles with language variation, exceptions, and novel inputs</li>
                        </ul>

<pre><code class="language-python"># Rule-based NLP example: Simple pattern matching
import re
from typing import List, Tuple

def extract_dates_rule_based(text: str) -> List[str]:
    """Extract dates using regular expression rules."""
    patterns = [
        # MM/DD/YYYY or MM-DD-YYYY
        r'\b(\d{1,2}[/-]\d{1,2}[/-]\d{2,4})\b',
        # Month DD, YYYY
        r'\b((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\s+\d{1,2},?\s+\d{4})\b',
        # DD Month YYYY
        r'\b(\d{1,2}\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{4})\b',
    ]
    
    dates = []
    for pattern in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        dates.extend(matches)
    
    return dates

def extract_emails_rule_based(text: str) -> List[str]:
    """Extract email addresses using regular expression rules."""
    pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    return re.findall(pattern, text)

# Test the rule-based extractors
test_text = """
Meeting scheduled for January 15, 2026 at 3pm.
Contact us at support@example.com or sales@company.org.
The deadline is 01/20/2026. Sincerely, John (john.doe@email.com)
"""

print("=== Rule-Based Information Extraction ===")
print(f"\nInput text:\n{test_text}")

print("\nExtracted dates:")
for date in extract_dates_rule_based(test_text):
    print(f"  - {date}")

print("\nExtracted emails:")
for email in extract_emails_rule_based(test_text):
    print(f"  - {email}")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-balance-scale me-2"></i>When to Use Rule-Based Approaches</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Best Practices</span>
                                <span class="badge bg-crimson">Still Relevant</span>
                            </div>
                            <div class="content">
                                <p>Rule-based methods excel in specific scenarios:</p>
                                <ul>
                                    <li><strong>High-precision requirements:</strong> When false positives are costly (medical, legal)</li>
                                    <li><strong>Structured patterns:</strong> Extracting phone numbers, IDs, formatted codes</li>
                                    <li><strong>Limited training data:</strong> When insufficient examples for statistical learning</li>
                                    <li><strong>Interpretability needs:</strong> When you must explain every decision</li>
                                    <li><strong>Hybrid systems:</strong> Combining rules with ML for preprocessing or post-processing</li>
                                </ul>
                            </div>
                        </div>

                        <h3 id="statistical">Statistical NLP</h3>
                        
                        <p>Starting in the 1990s, <strong>statistical approaches</strong> revolutionized NLP by learning patterns from data rather than relying on hand-crafted rules. These methods use probability theory and machine learning to build models that capture linguistic regularities automatically.</p>

                        <p>Key statistical NLP techniques include:</p>
                        <ul>
                            <li><strong>N-gram Language Models:</strong> Predicting words based on preceding context</li>
                            <li><strong>Hidden Markov Models (HMMs):</strong> Sequence labeling for POS tagging, named entity recognition</li>
                            <li><strong>Naive Bayes:</strong> Text classification based on word frequencies</li>
                            <li><strong>Maximum Entropy/Logistic Regression:</strong> Discriminative classification with features</li>
                            <li><strong>Conditional Random Fields (CRFs):</strong> Structured prediction for sequences</li>
                            <li><strong>Topic Models (LDA):</strong> Discovering latent topics in document collections</li>
                        </ul>

<pre><code class="language-python"># Statistical NLP: Text classification with scikit-learn
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import numpy as np

# Sample dataset: sentiment classification
texts = [
    "I love this product, it's amazing!",
    "Great quality and fast shipping",
    "Best purchase I've ever made",
    "Absolutely wonderful experience",
    "This is terrible, waste of money",
    "Broke after one day, very disappointed",
    "Poor quality, would not recommend",
    "Worst product ever, avoid at all costs",
]
labels = [1, 1, 1, 1, 0, 0, 0, 0]  # 1=positive, 0=negative

# Feature extraction: Bag of Words
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

# Train classifier
clf = MultinomialNB()
clf.fit(X, labels)

# Test on new examples
test_texts = [
    "This product exceeded my expectations!",
    "I regret buying this, it's awful",
]
X_test = vectorizer.transform(test_texts)
predictions = clf.predict(X_test)
probabilities = clf.predict_proba(X_test)

print("=== Statistical Text Classification ===")
print(f"\nVocabulary size: {len(vectorizer.vocabulary_)}")
print(f"Features (sample): {list(vectorizer.vocabulary_.keys())[:10]}")

print("\nPredictions:")
for text, pred, prob in zip(test_texts, predictions, probabilities):
    sentiment = "Positive" if pred == 1 else "Negative"
    confidence = max(prob) * 100
    print(f"  '{text[:40]}...'")
    print(f"    → {sentiment} (confidence: {confidence:.1f}%)")
</code></pre>

                        <h3 id="neural">Neural NLP</h3>
                        
                        <p>The <strong>deep learning revolution</strong> transformed NLP starting around 2013 with word embeddings and accelerating with attention mechanisms and transformers. Neural approaches learn hierarchical representations automatically from data, achieving state-of-the-art results on virtually every NLP benchmark.</p>

                        <p>Key neural NLP innovations:</p>
                        <ul>
                            <li><strong>Word Embeddings (2013):</strong> Word2Vec, GloVe—dense vector representations capturing semantic relationships</li>
                            <li><strong>Recurrent Neural Networks:</strong> LSTMs and GRUs for sequence modeling</li>
                            <li><strong>Attention Mechanism (2014):</strong> Allowing models to focus on relevant parts of input</li>
                            <li><strong>Transformer Architecture (2017):</strong> Self-attention enabling parallel processing and capturing long-range dependencies</li>
                            <li><strong>Pretrained Language Models (2018+):</strong> BERT, GPT, and their successors—transfer learning for NLP</li>
                        </ul>

                        <div class="highlight-box">
                            <h4><i class="fas fa-rocket me-2"></i>The Transformer Revolution</h4>
                            <p>The 2017 paper "Attention Is All You Need" introduced the <strong>Transformer architecture</strong>, which has become the foundation for virtually all modern NLP. Transformers process sequences in parallel, capture long-range dependencies, and scale efficiently—enabling models with billions of parameters that achieve remarkable language understanding.</p>
                        </div>

<pre><code class="language-python"># Neural NLP with Hugging Face Transformers
from transformers import pipeline

# Sentiment analysis with pretrained model
print("=== Neural NLP with Transformers ===")

# Initialize sentiment analysis pipeline
sentiment_analyzer = pipeline("sentiment-analysis")

# Analyze sentiments
texts = [
    "I absolutely love this new feature!",
    "This is the worst experience I've ever had.",
    "The product is okay, nothing special.",
    "Outstanding quality and excellent service!",
]

print("\nSentiment Analysis:")
for text in texts:
    result = sentiment_analyzer(text)[0]
    print(f"  '{text[:45]}...'")
    print(f"    → {result['label']} ({result['score']:.3f})")

# Named Entity Recognition
print("\n" + "="*50)
ner_pipeline = pipeline("ner", aggregation_strategy="simple")

text = "Apple Inc. CEO Tim Cook announced new products in Cupertino, California."
entities = ner_pipeline(text)

print("\nNamed Entity Recognition:")
print(f"Text: {text}\n")
for ent in entities:
    print(f"  {ent['word']}: {ent['entity_group']} (score: {ent['score']:.3f})")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-chart-line me-2"></i>Comparing NLP Approaches</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Comparison</span>
                                <span class="badge bg-crimson">Trade-offs</span>
                            </div>
                            <div class="content">
                                <table class="table table-bordered">
                                    <thead>
                                        <tr>
                                            <th>Aspect</th>
                                            <th>Rule-Based</th>
                                            <th>Statistical</th>
                                            <th>Neural</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td><strong>Data needs</strong></td>
                                            <td>None (expert knowledge)</td>
                                            <td>Moderate (labeled data)</td>
                                            <td>Large (pretrained + fine-tune)</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Interpretability</strong></td>
                                            <td>High</td>
                                            <td>Medium</td>
                                            <td>Low (black box)</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Generalization</strong></td>
                                            <td>Poor</td>
                                            <td>Good</td>
                                            <td>Excellent</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Development time</strong></td>
                                            <td>High (manual)</td>
                                            <td>Medium</td>
                                            <td>Low (pretrained models)</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Compute needs</strong></td>
                                            <td>Low</td>
                                            <td>Low-Medium</td>
                                            <td>High</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>

                        <h2 id="text-vs-speech"><i class="fas fa-microphone me-2"></i>Text vs Speech NLP</h2>
                        
                        <p>While "NLP" often focuses on text, natural language exists in both written and spoken forms, each with unique characteristics and processing challenges. Understanding the differences between text and speech processing is crucial for building comprehensive language systems.</p>

                        <h3>Text Processing</h3>
                        
                        <p>Text-based NLP works with written language, which has several advantages:</p>
                        <ul>
                            <li><strong>Discrete tokens:</strong> Words are clearly separated by spaces and punctuation</li>
                            <li><strong>Persistent:</strong> Text can be read and re-read, processed offline</li>
                            <li><strong>Structured:</strong> Sentences, paragraphs, and documents provide organization</li>
                            <li><strong>Edited:</strong> Written text is often revised and polished</li>
                            <li><strong>Abundant data:</strong> Massive amounts of text available online</li>
                        </ul>

                        <h3>Speech Processing</h3>
                        
                        <p>Speech-based NLP (also called Spoken Language Processing or SLP) works with audio signals and faces additional challenges:</p>
                        <ul>
                            <li><strong>Continuous signal:</strong> No clear word boundaries in the audio waveform</li>
                            <li><strong>Speaker variation:</strong> Accents, speaking rates, voice characteristics differ</li>
                            <li><strong>Acoustic noise:</strong> Background sounds, microphone quality, room acoustics</li>
                            <li><strong>Disfluencies:</strong> "Um", "uh", false starts, self-corrections, filled pauses</li>
                            <li><strong>Prosody:</strong> Intonation, stress, and rhythm carry meaning not present in text</li>
                            <li><strong>Real-time:</strong> Often requires immediate processing and response</li>
                        </ul>

                        <div class="highlight-box">
                            <h4><i class="fas fa-volume-up me-2"></i>Speech Processing Pipeline</h4>
                            <p>Speech systems typically involve multiple stages: <strong>Automatic Speech Recognition (ASR)</strong> converts audio to text, <strong>NLP/NLU</strong> processes the transcribed text, and <strong>Text-to-Speech (TTS)</strong> converts text responses back to audio. Modern end-to-end systems increasingly blur these boundaries.</p>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-balance-scale me-2"></i>Text vs Speech: Key Differences</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Comparison</span>
                                <span class="badge bg-crimson">Important</span>
                            </div>
                            <div class="content">
                                <table class="table table-bordered">
                                    <thead>
                                        <tr>
                                            <th>Aspect</th>
                                            <th>Text NLP</th>
                                            <th>Speech NLP</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td><strong>Input</strong></td>
                                            <td>Character sequences</td>
                                            <td>Audio waveforms</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Segmentation</strong></td>
                                            <td>Explicit (spaces, punctuation)</td>
                                            <td>Implicit (must be detected)</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Errors</strong></td>
                                            <td>Typos, OCR errors</td>
                                            <td>Recognition errors, homophones</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Additional info</strong></td>
                                            <td>Formatting, punctuation</td>
                                            <td>Prosody, speaker emotion</td>
                                        </tr>
                                        <tr>
                                            <td><strong>Style</strong></td>
                                            <td>Often formal, edited</td>
                                            <td>Often informal, spontaneous</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>

<pre><code class="language-python"># Speech-to-Text with OpenAI Whisper
# Demonstrating ASR (Automatic Speech Recognition)

import whisper
import numpy as np

# Load Whisper model (base model for demo)
# Options: tiny, base, small, medium, large
model = whisper.load_model("base")

# Transcribe audio file
# result = model.transcribe("audio_file.mp3")

# For demonstration, let's show the API usage
print("=== Speech Recognition with Whisper ===")
print("""
# To transcribe an audio file:
result = model.transcribe("speech.mp3")

# Result contains:
print(result["text"])       # Full transcription
print(result["language"])   # Detected language

# With timestamps:
for segment in result["segments"]:
    print(f"[{segment['start']:.2f}s - {segment['end']:.2f}s]")
    print(f"  {segment['text']}")

# Whisper features:
# - Multilingual (99 languages)
# - Translation to English
# - Timestamp alignment
# - Robust to accents and noise
""")

print("\nWhisper model sizes and capabilities:")
models_info = [
    ("tiny", "39M params", "~1GB VRAM", "Fastest, lowest accuracy"),
    ("base", "74M params", "~1GB VRAM", "Good balance for demos"),
    ("small", "244M params", "~2GB VRAM", "Improved accuracy"),
    ("medium", "769M params", "~5GB VRAM", "High accuracy"),
    ("large", "1550M params", "~10GB VRAM", "Best accuracy"),
]
for name, params, vram, note in models_info:
    print(f"  {name:8} | {params:12} | {vram:12} | {note}")
</code></pre>

                        <h2 id="pipelines"><i class="fas fa-stream me-2"></i>NLP Pipelines & Workflows</h2>
                        
                        <p>NLP systems rarely perform a single task in isolation. Instead, they combine multiple processing steps into <strong>pipelines</strong>—sequences of components that progressively transform raw text into structured information or desired outputs. Understanding pipeline architecture is essential for building effective NLP applications.</p>

                        <h3>Standard NLP Pipeline Components</h3>
                        
                        <p>A typical NLP pipeline includes these stages:</p>
                        <ol>
                            <li><strong>Data Acquisition:</strong> Collecting text from files, APIs, databases, web scraping</li>
                            <li><strong>Text Preprocessing:</strong> Cleaning, normalizing, handling encoding issues</li>
                            <li><strong>Tokenization:</strong> Splitting text into words, subwords, or characters</li>
                            <li><strong>Linguistic Analysis:</strong> POS tagging, parsing, named entity recognition</li>
                            <li><strong>Feature Extraction:</strong> Converting text to numerical representations</li>
                            <li><strong>Task-Specific Processing:</strong> Classification, generation, extraction, etc.</li>
                            <li><strong>Post-processing:</strong> Formatting output, filtering, aggregation</li>
                        </ol>

                        <div class="highlight-box">
                            <h4><i class="fas fa-project-diagram me-2"></i>Pipeline Design Principles</h4>
                            <ul>
                                <li><strong>Modularity:</strong> Each component should have a single responsibility</li>
                                <li><strong>Error propagation:</strong> Errors in early stages affect downstream components</li>
                                <li><strong>Configurability:</strong> Allow swapping components for different use cases</li>
                                <li><strong>Efficiency:</strong> Consider batching, caching, and parallel processing</li>
                                <li><strong>Monitoring:</strong> Track performance at each stage</li>
                            </ul>
                        </div>

<pre><code class="language-python"># Building a complete NLP pipeline with spaCy
import spacy
from collections import Counter

# Load the English language model
nlp = spacy.load("en_core_web_sm")

# Sample document
text = """
Apple Inc. announced its quarterly earnings yesterday in Cupertino.
CEO Tim Cook reported revenue of $90 billion, exceeding analyst expectations.
The company's new M3 chips have driven strong MacBook sales worldwide.
"""

print("=== Complete NLP Pipeline with spaCy ===")
print(f"\nInput text:\n{text}")

# Process the text through the pipeline
doc = nlp(text)

# Stage 1: Tokenization
print("\n--- Stage 1: Tokenization ---")
tokens = [token.text for token in doc if not token.is_space]
print(f"Tokens ({len(tokens)}): {tokens[:15]}...")

# Stage 2: Sentence segmentation
print("\n--- Stage 2: Sentence Segmentation ---")
for i, sent in enumerate(doc.sents, 1):
    print(f"Sentence {i}: {sent.text.strip()[:60]}...")

# Stage 3: Part-of-Speech tagging
print("\n--- Stage 3: POS Tagging (sample) ---")
pos_dist = Counter(token.pos_ for token in doc)
print(f"POS distribution: {dict(pos_dist)}")

# Stage 4: Named Entity Recognition
print("\n--- Stage 4: Named Entity Recognition ---")
for ent in doc.ents:
    print(f"  {ent.text}: {ent.label_} ({spacy.explain(ent.label_)})")

# Stage 5: Dependency parsing (show root verbs)
print("\n--- Stage 5: Dependency Parsing ---")
for sent in doc.sents:
    root = [token for token in sent if token.dep_ == "ROOT"][0]
    subj = [child for child in root.children if "subj" in child.dep_]
    print(f"Root verb: '{root.text}', Subject: {[s.text for s in subj]}")
</code></pre>

                        <h3>Pipeline Architecture Patterns</h3>
                        
                        <div class="experiment-card">
                            <h4><i class="fas fa-stream me-2"></i>Sequential Pipeline</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Pattern</span>
                                <span class="badge bg-crimson">Most Common</span>
                            </div>
                            <div class="content">
                                <p>Components execute in fixed order, each passing output to the next. Simple to understand and implement, but no parallelism and early errors propagate.</p>
                                <p><code>Input → Tokenize → POS Tag → Parse → NER → Output</code></p>
                            </div>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-code-branch me-2"></i>Parallel/Fan-out Pipeline</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Pattern</span>
                                <span class="badge bg-crimson">Scalable</span>
                            </div>
                            <div class="content">
                                <p>After preprocessing, multiple independent analyses run in parallel, with results aggregated at the end. Better throughput but more complex orchestration.</p>
                                <pre style="background: #f5f5f5; padding: 10px; font-size: 0.85rem;">
Input → Tokenize → ┬→ Sentiment Analysis ─┐
                   ├→ Entity Extraction  ─┤→ Aggregate
                   └→ Topic Classification┘
                                </pre>
                            </div>
                        </div>

<pre><code class="language-python"># Custom NLP Pipeline with scikit-learn
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import cross_val_score
import numpy as np

# Sample data
texts = [
    "The stock market rallied on positive earnings",
    "Tech companies reported strong quarterly growth",
    "Federal Reserve announces interest rate decision",
    "Unemployment rates drop to historic lows",
    "New smartphone features impressive camera technology",
    "Electric vehicle sales continue upward trend",
    "AI breakthrough achieves human-level performance",
    "Climate summit reaches landmark agreement",
]
categories = ["finance", "finance", "finance", "finance", 
              "tech", "tech", "tech", "environment"]

# Build a classification pipeline
pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer(
        lowercase=True,
        stop_words='english',
        ngram_range=(1, 2),
        max_features=1000
    )),
    ('classifier', MultinomialNB())
])

print("=== Sklearn NLP Pipeline ===")
print(f"\nPipeline steps: {[name for name, _ in pipeline.steps]}")

# Fit the pipeline
pipeline.fit(texts, categories)

# Make predictions
test_texts = [
    "Stock prices surge after merger announcement",
    "Revolutionary AI model transforms healthcare",
]

print("\nPredictions:")
predictions = pipeline.predict(test_texts)
for text, pred in zip(test_texts, predictions):
    print(f"  '{text[:40]}...' → {pred}")

# Inspect pipeline internals
vectorizer = pipeline.named_steps['vectorizer']
print(f"\nVocabulary size: {len(vectorizer.vocabulary_)}")
print(f"Sample features: {list(vectorizer.vocabulary_.keys())[:10]}")
</code></pre>

                        <h3>Modern Pipeline Considerations</h3>
                        
                        <p>Contemporary NLP pipelines often incorporate:</p>
                        <ul>
                            <li><strong>Pretrained models:</strong> Starting with BERT, GPT, or similar as foundation</li>
                            <li><strong>GPU acceleration:</strong> Batching for efficient neural model inference</li>
                            <li><strong>Streaming processing:</strong> Handling continuous data flows</li>
                            <li><strong>Caching:</strong> Storing intermediate results for repeated documents</li>
                            <li><strong>Error handling:</strong> Graceful degradation when components fail</li>
                            <li><strong>Observability:</strong> Logging, metrics, and tracing throughout the pipeline</li>
                        </ul>

                        <h2 id="datasets"><i class="fas fa-database me-2"></i>Datasets, Corpora & Annotation</h2>
                        
                        <p>Data is the foundation of modern NLP. Whether training machine learning models or evaluating system performance, high-quality datasets are essential. Understanding corpus linguistics, annotation methodologies, and available resources is crucial for NLP practitioners.</p>

                        <h3>Corpora and Datasets</h3>
                        
                        <p>A <strong>corpus</strong> (plural: corpora) is a large, structured collection of texts used for linguistic research and NLP development. Corpora vary widely in size, domain, language, and annotation.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-database me-2"></i>Major NLP Corpora</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Resources</span>
                                <span class="badge bg-crimson">Essential</span>
                            </div>
                            <div class="content">
                                <ul>
                                    <li><strong>Penn Treebank:</strong> Gold-standard syntactic annotations for English (POS tags, parse trees)</li>
                                    <li><strong>CoNLL Datasets:</strong> Shared task data for NER, coreference, dependency parsing</li>
                                    <li><strong>SQuAD:</strong> Stanford Question Answering Dataset—reading comprehension</li>
                                    <li><strong>GLUE/SuperGLUE:</strong> Multi-task benchmarks for language understanding</li>
                                    <li><strong>Common Crawl:</strong> Petabytes of web text for pretraining</li>
                                    <li><strong>Wikipedia:</strong> Encyclopedic text in 300+ languages</li>
                                    <li><strong>IMDb Reviews:</strong> Movie reviews for sentiment analysis</li>
                                    <li><strong>WMT:</strong> Parallel corpora for machine translation</li>
                                </ul>
                            </div>
                        </div>

<pre><code class="language-python"># Accessing NLP datasets with NLTK and Hugging Face
import nltk

# Download NLTK data
nltk.download('brown', quiet=True)
nltk.download('gutenberg', quiet=True)
nltk.download('reuters', quiet=True)

from nltk.corpus import brown, gutenberg, reuters

print("=== Exploring NLTK Corpora ===")

# Brown Corpus - tagged American English
print("\n--- Brown Corpus ---")
print(f"Categories: {brown.categories()[:8]}...")
print(f"Total words: {len(brown.words()):,}")
print(f"Sample (news): {' '.join(brown.words(categories='news')[:15])}...")

# Tagged sentences
tagged = brown.tagged_sents(categories='news')[0][:8]
print(f"Tagged sample: {tagged}")

# Gutenberg Corpus - literary texts
print("\n--- Gutenberg Corpus ---")
print(f"Available texts: {gutenberg.fileids()[:5]}...")
austen = gutenberg.words('austen-emma.txt')
print(f"Emma word count: {len(austen):,}")
print(f"Sample: {' '.join(austen[:20])}")

# Reuters Corpus - news articles
print("\n--- Reuters Corpus ---")
print(f"Categories: {reuters.categories()[:10]}...")
print(f"Total documents: {len(reuters.fileids()):,}")
</code></pre>

                        <h3>Data Annotation</h3>
                        
                        <p><strong>Annotation</strong> is the process of adding labels or markup to text data. High-quality annotations are expensive to produce but essential for supervised learning. Key considerations include:</p>

                        <ul>
                            <li><strong>Annotation scheme:</strong> Defining what labels to use and what they mean</li>
                            <li><strong>Annotation guidelines:</strong> Detailed instructions ensuring consistency</li>
                            <li><strong>Inter-annotator agreement:</strong> Measuring how consistently annotators apply labels</li>
                            <li><strong>Quality control:</strong> Checking for errors, biases, and inconsistencies</li>
                            <li><strong>Annotation tools:</strong> Software for efficient labeling (Prodigy, Label Studio, doccano)</li>
                        </ul>

                        <div class="highlight-box">
                            <h4><i class="fas fa-users me-2"></i>Crowdsourcing vs Expert Annotation</h4>
                            <p><strong>Crowdsourcing</strong> (Amazon Mechanical Turk, Appen) is cost-effective for simple tasks but may have quality issues. <strong>Expert annotation</strong> is more reliable for complex linguistic judgments but significantly more expensive. Many projects use a hybrid approach—crowdsourcing initial labels with expert review.</p>
                        </div>

<pre><code class="language-python"># Working with annotated datasets from Hugging Face
from datasets import load_dataset

print("=== Hugging Face Datasets ===")

# Load a NER dataset
print("\n--- CoNLL-2003 Named Entity Recognition ---")
conll = load_dataset("conll2003", split="train[:5]")

print(f"Dataset features: {conll.features}")
print(f"\nSample sentence:")
example = conll[0]
print(f"Tokens: {example['tokens']}")
print(f"NER tags: {example['ner_tags']}")

# Label mapping
ner_labels = conll.features['ner_tags'].feature.names
print(f"\nNER label scheme: {ner_labels}")

# Load a sentiment dataset
print("\n--- IMDb Sentiment Analysis ---")
imdb = load_dataset("imdb", split="train[:3]")

for i, example in enumerate(imdb):
    sentiment = "Positive" if example['label'] == 1 else "Negative"
    text_preview = example['text'][:80].replace('\n', ' ')
    print(f"\nExample {i+1} ({sentiment}):")
    print(f"  '{text_preview}...'")

# Load a question answering dataset
print("\n--- SQuAD Question Answering ---")
squad = load_dataset("squad", split="train[:2]")

for example in squad:
    print(f"\nContext: {example['context'][:100]}...")
    print(f"Question: {example['question']}")
    print(f"Answer: {example['answers']['text'][0]}")
</code></pre>

                        <h3>Creating Your Own Dataset</h3>
                        
                        <p>When existing datasets don't meet your needs, you may need to create your own. Key steps include:</p>
                        <ol>
                            <li><strong>Define the task:</strong> What exactly should annotators label?</li>
                            <li><strong>Design the schema:</strong> What categories, entities, or structures?</li>
                            <li><strong>Write guidelines:</strong> Clear instructions with examples and edge cases</li>
                            <li><strong>Pilot annotation:</strong> Small-scale test to refine guidelines</li>
                            <li><strong>Calculate agreement:</strong> Measure inter-annotator reliability (Cohen's κ, Fleiss' κ)</li>
                            <li><strong>Scale up:</strong> Annotate full dataset with quality checks</li>
                            <li><strong>Adjudicate disagreements:</strong> Resolve conflicts through discussion or majority vote</li>
                        </ol>

                        <div class="experiment-card">
                            <h4><i class="fas fa-chart-bar me-2"></i>Inter-Annotator Agreement Metrics</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Quality</span>
                                <span class="badge bg-crimson">Essential</span>
                            </div>
                            <div class="content">
                                <ul>
                                    <li><strong>Cohen's Kappa (κ):</strong> Agreement between 2 annotators, correcting for chance</li>
                                    <li><strong>Fleiss' Kappa:</strong> Agreement among multiple annotators</li>
                                    <li><strong>Krippendorff's Alpha:</strong> Handles missing data and various data types</li>
                                </ul>
                                <p>Interpretation: κ > 0.8 = excellent, 0.6-0.8 = substantial, 0.4-0.6 = moderate, < 0.4 = poor</p>
                            </div>
                        </div>

                        <h2 id="conclusion"><i class="fas fa-flag-checkered me-2"></i>Conclusion & Next Steps</h2>
                        
                        <p>In this foundational guide, we've explored the essential concepts that underpin all Natural Language Processing systems. From understanding what NLP is and its real-world applications to diving deep into the linguistic levels of language analysis, we've built a comprehensive foundation for the journey ahead.</p>

                        <h3>Key Takeaways</h3>
                        
                        <div class="highlight-box">
                            <h4><i class="fas fa-check-circle me-2"></i>What You've Learned</h4>
                            <ul>
                                <li><strong>NLP fundamentals:</strong> The field bridges human communication and machine understanding through NLU and NLG</li>
                                <li><strong>Linguistic levels:</strong> Language operates at phonetic, morphological, syntactic, semantic, and pragmatic levels—each presenting unique challenges</li>
                                <li><strong>Three paradigms:</strong> Rule-based, statistical, and neural approaches each have their place in modern NLP</li>
                                <li><strong>Text vs speech:</strong> Written and spoken language require different processing strategies</li>
                                <li><strong>Pipelines:</strong> NLP systems combine multiple processing stages for real-world applications</li>
                                <li><strong>Data matters:</strong> High-quality annotated corpora are the foundation of supervised NLP</li>
                            </ul>
                        </div>

                        <h3>What's Next</h3>
                        
                        <p>With this foundation in place, you're ready to dive into the practical aspects of NLP. In <strong>Part 2: Tokenization & Text Cleaning</strong>, we'll explore how to convert raw text into the structured input that NLP models require—the critical first step in any NLP pipeline.</p>

                        <p>Upcoming topics in the series include:</p>
                        <ul>
                            <li><strong>Part 2:</strong> Tokenization strategies (word, subword, character) and text preprocessing techniques</li>
                            <li><strong>Part 3:</strong> Feature engineering with Bag of Words, TF-IDF, and N-grams</li>
                            <li><strong>Part 4:</strong> Word embeddings that capture semantic relationships (Word2Vec, GloVe, FastText)</li>
                            <li><strong>Parts 5-8:</strong> From statistical language models to the Transformer architecture</li>
                            <li><strong>Parts 9-10:</strong> Pretrained models (BERT, GPT) and transfer learning</li>
                            <li><strong>Parts 11-16:</strong> Core tasks, advanced applications, multilingual NLP, ethics, and production systems</li>
                        </ul>

                        <div class="experiment-card">
                            <h4><i class="fas fa-code me-2"></i>Practice Exercises</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Hands-On</span>
                                <span class="badge bg-crimson">Recommended</span>
                            </div>
                            <div class="content">
                                <p>To reinforce your learning, try these exercises:</p>
                                <ol>
                                    <li><strong>Explore NLTK corpora:</strong> Load different corpora (brown, gutenberg, reuters) and compute basic statistics</li>
                                    <li><strong>Build a rule-based extractor:</strong> Write regex patterns to extract phone numbers, URLs, and hashtags</li>
                                    <li><strong>Analyze dependencies:</strong> Use spaCy to visualize dependency trees for complex sentences</li>
                                    <li><strong>Compare stemmers:</strong> Apply Porter, Lancaster, and Snowball stemmers to the same text and compare results</li>
                                    <li><strong>Explore WordNet:</strong> Find synonyms, antonyms, and hypernyms for 10 common words</li>
                                </ol>
                            </div>
                        </div>

                        <h3>Additional Resources</h3>
                        
                        <p>To deepen your understanding, explore these resources:</p>
                        <ul>
                            <li><strong>Books:</strong> "Speech and Language Processing" by Jurafsky & Martin (free online), "Natural Language Processing with Python" by Bird, Klein & Loper</li>
                            <li><strong>Courses:</strong> Stanford CS224N (NLP with Deep Learning), Coursera NLP Specialization</li>
                            <li><strong>Libraries:</strong> spaCy documentation, Hugging Face tutorials, NLTK book</li>
                            <li><strong>Papers:</strong> "Attention Is All You Need", "BERT: Pre-training of Deep Bidirectional Transformers"</li>
                        </ul>

                        <div class="highlight-box">
                            <h4><i class="fas fa-forward me-2"></i>Ready for Part 2?</h4>
                            <p>Continue your NLP journey with <a href="nlp-tokenization-text-cleaning.html">Part 2: Tokenization & Text Cleaning</a>, where we'll transform raw text into structured input for NLP models.</p>
                        </div>

                        <!-- Related Posts -->
                        <div class="related-posts">
                            <h3><i class="fas fa-book-reader me-2"></i>Continue the NLP Series</h3>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 2: Tokenization & Text Cleaning</h5>
                                <p class="text-muted small mb-2">Learn to convert raw text into usable input—word, subword, and character tokenization.</p>
                                <a href="nlp-tokenization-text-cleaning.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 3: Text Representation & Feature Engineering</h5>
                                <p class="text-muted small mb-2">Turn text into numbers with Bag of Words, TF-IDF, and N-grams.</p>
                                <a href="nlp-text-representation-features.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 4: Word Embeddings</h5>
                                <p class="text-muted small mb-2">Capture meaning and similarity with Word2Vec, GloVe, and FastText.</p>
                                <a href="nlp-word-embeddings.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.</p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook"><i class="fab fa-facebook-f"></i></a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter"><i class="fab fa-twitter"></i></a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn"><i class="fab fa-linkedin-in"></i></a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube"><i class="fab fa-youtube"></i></a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram"><i class="fab fa-instagram"></i></a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest"><i class="fab fa-pinterest-p"></i></a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email"><i class="fas fa-envelope"></i></a>
                    </div>
                </div>
            </div>
            <hr class="bg-secondary">
            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small"><i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a></p>
                    <p class="small mt-3"><a href="/" class="text-light text-decoration-none">Home</a> | <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a></p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">Enjoying this content? ☕ <a href="https://buymeacoffee.com/itswzee" target="_blank" class="text-light" style="text-decoration: underline;">Keep me caffeinated</a> to keep the pixels flowing!</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    
    <!-- Scroll-to-Top Button -->
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top">
        <i class="fas fa-arrow-up"></i>
    </button>
    <!-- Category Indicator -->
    <div id="categoryIndicator" class="category-indicator" title="Current Section">
        <i class="fas fa-tag"></i><span id="categoryText">Technology</span>
    </div>
    
    <!-- Cookie Consent JS -->
    <script src="../../../js/cookie-consent.js"></script>
    
    <!-- Main JS -->
    <script src="../../../js/main.js"></script>

    <!-- Prism.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

            </body>
</html>
