<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Part 6 of the Complete NLP Series: Apply deep learning to NLP—feedforward networks, CNNs for text, activation functions, loss functions, and training neural text classifiers." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="NLP, Neural Networks, Deep Learning, CNN, Text Classification, Feedforward Networks, Activation Functions, Backpropagation, PyTorch, TensorFlow" />
    <meta property="og:title" content="Neural Networks for NLP - Complete NLP Series Part 6" />
    <meta property="og:description" content="Apply deep learning fundamentals to natural language processing tasks." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2026-01-27" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>Neural Networks for NLP - Complete NLP Series Part 6 - Wasil Zafar</title>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('consent', 'default', { 'ad_storage': 'denied', 'ad_user_data': 'denied', 'ad_personalization': 'denied', 'analytics_storage': 'denied', 'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE'] });
        gtag('consent', 'default', { 'ad_storage': 'granted', 'ad_user_data': 'granted', 'ad_personalization': 'granted', 'analytics_storage': 'granted' });
        gtag('set', 'url_passthrough', true);
    </script>
    <script>
        (function(w, d, s, l, i) { w[l] = w[l] || []; w[l].push({ 'gtm.start': new Date().getTime(), event: 'gtm.js' }); var f = d.getElementsByTagName(s)[0], j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f); })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    <style>
        .blog-hero { background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%); color: white; padding: 80px 0; }
        .blog-meta { font-size: 0.95rem; color: var(--color-teal); margin-bottom: 1rem; display: flex; align-items: center; flex-wrap: wrap; gap: 1rem; }
        .print-btn { background: var(--color-teal); color: white; border: none; padding: 0.4rem 1rem; border-radius: 4px; font-size: 0.9rem; cursor: pointer; transition: all 0.3s ease; display: inline-flex; align-items: center; gap: 0.5rem; }
        .print-btn:hover { background: var(--color-crimson); transform: translateY(-1px); }
        @media print { .print-btn, nav, .navbar, footer, .back-link, .related-posts, .scroll-to-top, .toc-toggle-btn, .sidenav-toc, .sidenav-overlay { display: none !important; } * { -webkit-print-color-adjust: exact !important; print-color-adjust: exact !important; } }
        .blog-content { max-width: 900px; margin: 0 auto; font-size: 1.05rem; line-height: 1.8; color: #333; }
        .blog-content h2 { font-size: 1.8rem; font-weight: 700; margin-top: 2.5rem; margin-bottom: 1.5rem; color: var(--color-navy); border-bottom: 3px solid var(--color-teal); padding-bottom: 0.5rem; }
        .blog-content h3 { font-size: 1.3rem; font-weight: 600; margin-top: 2rem; margin-bottom: 1rem; color: var(--color-blue); }
        .blog-content h4 { font-size: 1.1rem; font-weight: 600; margin-top: 1.5rem; margin-bottom: 1rem; color: var(--color-teal); }
        .blog-content p { margin-bottom: 1.2rem; text-align: justify; }
        .blog-content strong { color: var(--color-crimson); }
        .blog-content pre[class*="language-"] { border-radius: 6px; margin: 1.5rem 0; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1); }
        .blog-content p code, .blog-content li code { background: rgba(59, 151, 151, 0.1); color: var(--color-crimson); padding: 0.2rem 0.4rem; border-radius: 3px; font-family: 'Consolas', monospace; font-size: 0.9em; }
        .highlight-box { background: rgba(59, 151, 151, 0.1); border-left: 4px solid var(--color-teal); padding: 1.5rem; margin: 2rem 0; border-radius: 4px; }
        .experiment-card { background: #f8f9fa; border: 1px solid #ddd; border-radius: 8px; padding: 1.5rem; margin-bottom: 1.5rem; transition: all 0.3s ease; }
        .experiment-card:hover { box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1); transform: translateY(-2px); }
        .experiment-card h4 { color: var(--color-crimson); font-weight: 700; margin-bottom: 0.5rem; }
        .bg-teal { background-color: var(--color-teal) !important; }
        .bg-crimson { background-color: var(--color-crimson) !important; }
        .toc-toggle-btn { position: fixed; bottom: 2rem; left: 2rem; width: 50px; height: 50px; background: var(--color-teal); color: white; border: none; border-radius: 50%; font-size: 1.2rem; cursor: pointer; box-shadow: 0 4px 12px rgba(59, 151, 151, 0.4); transition: all 0.3s ease; z-index: 1049; display: flex; align-items: center; justify-content: center; }
        .toc-toggle-btn:hover { background: var(--color-crimson); transform: scale(1.1); }
        .sidenav-toc { height: calc(100% - 64px); width: 0; position: fixed; z-index: 1050; top: 64px; left: 0; background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%); overflow-x: hidden; overflow-y: auto; transition: width 0.4s ease; padding-top: 30px; box-shadow: 4px 0 15px rgba(0, 0, 0, 0.3); }
        .sidenav-toc.open { width: 350px; }
        .sidenav-toc .toc-header { display: flex; align-items: center; justify-content: space-between; padding: 20px 30px; margin-bottom: 20px; border-bottom: 2px solid var(--color-teal); opacity: 0; visibility: hidden; transition: all 0.3s ease; }
        .sidenav-toc.open .toc-header { opacity: 1; visibility: visible; }
        .sidenav-toc .closebtn { font-size: 32px; color: white; background: transparent; border: none; cursor: pointer; transition: all 0.3s ease; }
        .sidenav-toc .closebtn:hover { color: var(--color-crimson); transform: rotate(90deg); }
        .sidenav-toc h3 { color: white; margin: 0; font-weight: 700; font-size: 1.3rem; }
        .sidenav-toc ol { list-style: decimal; padding-left: 30px; margin: 0; color: rgba(255, 255, 255, 0.9); }
        .sidenav-toc ol li { margin-bottom: 8px; }
        .sidenav-toc ul { list-style-type: lower-alpha; padding-left: 30px; margin-top: 8px; }
        .sidenav-toc a { padding: 12px 30px; text-decoration: none; font-size: 0.95rem; color: rgba(255, 255, 255, 0.85); display: block; transition: all 0.3s ease; border-left: 4px solid transparent; }
        .sidenav-toc a:hover { color: white; background: rgba(59, 151, 151, 0.2); border-left-color: var(--color-teal); }
        .sidenav-toc a.active { color: white; background: rgba(191, 9, 47, 0.3); border-left-color: var(--color-crimson); font-weight: 600; }
        .sidenav-overlay { display: none; position: fixed; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0, 0, 0, 0.5); z-index: 1049; }
        .sidenav-overlay.show { display: block; }
        .reading-time { display: inline-block; background: var(--color-crimson); color: white; padding: 0.3rem 0.8rem; border-radius: 4px; font-size: 0.9rem; }
        .back-link { display: inline-block; color: white; text-decoration: none; transition: all 0.3s ease; margin-bottom: 1rem; opacity: 0.9; }
        .back-link:hover { color: var(--color-teal); transform: translateX(-5px); }
        .related-posts { background: #f8f9fa; border-radius: 8px; padding: 2rem; margin-top: 3rem; }
        .related-posts h3 { color: var(--color-navy); margin-bottom: 1.5rem; }
        .related-post-item { padding: 1rem; border-left: 3px solid var(--color-teal); margin-bottom: 1rem; transition: all 0.3s ease; }
        .related-post-item:hover { background: white; border-left-color: var(--color-crimson); }
        .related-post-item a { color: var(--color-blue); text-decoration: none; font-weight: 600; }
        .related-post-item a:hover { color: var(--color-crimson); }
        div.code-toolbar > .toolbar { opacity: 1; display: flex; gap: 0.5rem; }
        div.code-toolbar > .toolbar > .toolbar-item > button { background: var(--color-teal); color: white; border: none; padding: 0.4rem 0.8rem; border-radius: 4px; font-size: 0.85rem; cursor: pointer; }
        div.code-toolbar > .toolbar > .toolbar-item > select { background: var(--color-navy); color: white; border: 1px solid var(--color-teal); padding: 0.4rem 0.8rem; border-radius: 4px; font-size: 0.85rem; }
        .scroll-to-top { position: fixed; bottom: 2rem; right: 2rem; width: 50px; height: 50px; background: var(--color-teal); color: white; border: none; border-radius: 50%; font-size: 1.2rem; cursor: pointer; display: flex; align-items: center; justify-content: center; opacity: 0; visibility: hidden; transition: all 0.3s ease; box-shadow: 0 4px 12px rgba(59, 151, 151, 0.3); z-index: 999; }
        .scroll-to-top.show { opacity: 1; visibility: visible; }
        .scroll-to-top:hover { background: var(--color-crimson); transform: translateY(-3px); }
        @media (max-width: 768px) { .sidenav-toc.open { width: 280px; } .toc-toggle-btn { left: 15px; } }
        html { scroll-behavior: smooth; }
        /* Category Indicator */
        .category-indicator {
            position: fixed;
            bottom: 2rem;
            right: 6.5rem;
            background: var(--color-navy);
            color: white;
            padding: 0.75rem 1.25rem;
            border-radius: 25px;
            font-size: 0.9rem;
            font-weight: 600;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            box-shadow: 0 4px 12px rgba(19, 36, 64, 0.3);
            z-index: 998;
            max-width: 150px;
            text-align: center;
            white-space: nowrap;
            overflow: hidden;
            text-overflow: ellipsis;
        }

        .category-indicator.show {
            opacity: 1;
            visibility: visible;
        }

        .category-indicator i {
            margin-right: 0.5rem;
            color: var(--color-teal);
        }

        @media (max-width: 768px) {
            .category-indicator {
                display: none;
            }
        }
    </style>
</head>
<body>
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/"><span class="gradient-text">Wasil Zafar</span></a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item"><a class="nav-link" href="/">Home</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#about">About</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#skills">Skills</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#certifications">Certifications</a></li>
                    <li class="nav-item"><a class="nav-link" href="/#interests">Interests</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link"><i class="fas fa-arrow-left me-2"></i>Back to Technology</a>
                <h1 class="display-4 fw-bold mb-3">Neural Networks for NLP</h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>January 27, 2026</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-1"></i>35 min read</span>
                    <button onclick="window.print()" class="print-btn" title="Print this article"><i class="fas fa-print"></i> Print</button>
                </div>
                <p class="lead">Part 6 of 16: Apply deep learning fundamentals to natural language processing—feedforward networks, CNNs for text, and training neural classifiers.</p>
            </div>
        </div>
    </section>

    <button class="toc-toggle-btn" onclick="openNav()" title="Table of Contents" aria-label="Open Table of Contents"><i class="fas fa-list"></i></button>

    <div id="tocSidenav" class="sidenav-toc">
        <div class="toc-header">
            <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
            <button class="closebtn" onclick="closeNav()" aria-label="Close">&times;</button>
        </div>
        <ol>
            <li><a href="#introduction" onclick="closeNav()">Introduction to Neural NLP</a></li>
            <li><a href="#feedforward" onclick="closeNav()">Feedforward Networks for Text</a>
                <ul>
                    <li><a href="#architecture" onclick="closeNav()">Architecture Overview</a></li>
                    <li><a href="#activations" onclick="closeNav()">Activation Functions</a></li>
                </ul>
            </li>
            <li><a href="#cnn-text" onclick="closeNav()">CNNs for Text Classification</a>
                <ul>
                    <li><a href="#convolutions" onclick="closeNav()">1D Convolutions</a></li>
                    <li><a href="#pooling" onclick="closeNav()">Pooling Strategies</a></li>
                </ul>
            </li>
            <li><a href="#training" onclick="closeNav()">Training Neural Text Models</a>
                <ul>
                    <li><a href="#loss" onclick="closeNav()">Loss Functions</a></li>
                    <li><a href="#optimization" onclick="closeNav()">Optimization & Regularization</a></li>
                </ul>
            </li>
            <li><a href="#embeddings-layer" onclick="closeNav()">Embedding Layers</a></li>
            <li><a href="#practical" onclick="closeNav()">Practical Implementation</a></li>
            <li><a href="#conclusion" onclick="closeNav()">Conclusion & Next Steps</a></li>
        </ol>
    </div>

    <div id="tocOverlay" class="sidenav-overlay" onclick="closeNav()"></div>

    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="blog-content">
                        
                        <h2 id="introduction"><i class="fas fa-network-wired me-2"></i>Introduction to Neural NLP</h2>
                        
                        <p>Deep learning transformed NLP by learning features automatically from data instead of hand-crafting them. This part covers the fundamentals of applying neural networks to text.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-star me-2"></i>Key Insight</h4>
                            <p><strong>Neural networks learn hierarchical representations of text, capturing increasingly abstract features from characters to words to phrases to semantics.</strong></p>
                        </div>

                        <div class="experiment-card">
                            <h4><i class="fas fa-map-signs me-2"></i>Complete NLP Series Navigation</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">16-Part Series</span>
                                <span class="badge bg-crimson">NLP Mastery</span>
                            </div>
                            <div class="content">
                                <ol>
                                    <li><a href="nlp-fundamentals-linguistic-basics.html">NLP Fundamentals & Linguistic Basics</a></li>
                                    <li><a href="nlp-tokenization-text-cleaning.html">Tokenization & Text Cleaning</a></li>
                                    <li><a href="nlp-text-representation-features.html">Text Representation & Feature Engineering</a></li>
                                    <li><a href="nlp-word-embeddings.html">Word Embeddings</a></li>
                                    <li><a href="nlp-statistical-language-models.html">Statistical Language Models & N-grams</a></li>
                                    <li><strong>Neural Networks for NLP (This Guide)</strong></li>
                                    <li><a href="nlp-rnn-lstm-gru.html">RNNs, LSTMs & GRUs</a></li>
                                    <li><a href="nlp-transformers-attention.html">Transformers & Attention Mechanism</a></li>
                                    <li><a href="nlp-pretrained-models-transfer-learning.html">Pretrained Language Models & Transfer Learning</a></li>
                                    <li><a href="nlp-gpt-text-generation.html">GPT Models & Text Generation</a></li>
                                    <li><a href="nlp-core-tasks.html">Core NLP Tasks</a></li>
                                    <li><a href="nlp-advanced-tasks.html">Advanced NLP Tasks</a></li>
                                    <li><a href="nlp-multilingual-crosslingual.html">Multilingual & Cross-lingual NLP</a></li>
                                    <li><a href="nlp-evaluation-ethics.html">Evaluation, Ethics & Responsible NLP</a></li>
                                    <li><a href="nlp-systems-production.html">NLP Systems, Optimization & Production</a></li>
                                    <li><a href="nlp-cutting-edge-research.html">Cutting-Edge & Research Topics</a></li>
                                </ol>
                            </div>
                        </div>

                        <h2 id="feedforward"><i class="fas fa-project-diagram me-2"></i>Feedforward Networks for Text</h2>

                        <p>Feedforward neural networks (FFNs), also called Multi-Layer Perceptrons (MLPs), are the simplest form of neural networks where information flows in one direction—from input through hidden layers to output. For NLP tasks, these networks take fixed-size text representations (like bag-of-words vectors or averaged word embeddings) and transform them through successive layers to make predictions such as sentiment classification or topic categorization.</p>

                        <p>The key insight behind using feedforward networks for text is that each layer learns increasingly abstract representations. The first layer might learn simple word co-occurrence patterns, while deeper layers can capture semantic relationships and complex linguistic features. Unlike traditional machine learning models that require manual feature engineering, neural networks automatically discover useful features from raw text representations.</p>

                        <h3 id="architecture">Architecture Overview</h3>
                        
                        <p>A typical feedforward network for text classification consists of an input layer that accepts vectorized text, one or more hidden layers with nonlinear activation functions, and an output layer that produces class probabilities. The hidden layers are called "dense" or "fully connected" because every neuron connects to every neuron in the adjacent layers. The width (number of neurons) and depth (number of layers) are hyperparameters that control the network's capacity to learn complex patterns.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-diagram-project me-2"></i>Feedforward Network Architecture for Text</h4>
<pre style="font-family: monospace; background: white; padding: 1rem; border-radius: 4px; overflow-x: auto;">
┌─────────────────────────────────────────────────────────────────┐
│                    TEXT CLASSIFICATION MLP                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   INPUT LAYER          HIDDEN LAYERS           OUTPUT LAYER     │
│   (Text Vector)        (Feature Learning)      (Predictions)    │
│                                                                  │
│   ┌───────┐            ┌───────┐               ┌───────┐        │
│   │  x₁   │───────────►│  h₁   │──────────────►│  y₁   │        │
│   │ (the) │            │       │               │ (pos) │        │
│   ├───────┤     W₁     ├───────┤      W₂       ├───────┤        │
│   │  x₂   │───────────►│  h₂   │──────────────►│  y₂   │        │
│   │ (cat) │            │       │               │ (neg) │        │
│   ├───────┤            ├───────┤               ├───────┤        │
│   │  x₃   │───────────►│  h₃   │──────────────►│  y₃   │        │
│   │ (sat) │            │       │               │(neut) │        │
│   ├───────┤            ├───────┤               └───────┘        │
│   │  ...  │            │  ...  │                                 │
│   ├───────┤            ├───────┤                                 │
│   │  xₙ   │            │  hₘ   │                                 │
│   │(vocab)│            │       │                                 │
│   └───────┘            └───────┘                                 │
│                                                                  │
│   Size: V              Size: 256-512           Size: num_classes │
│   (vocabulary)         (learned features)      (+ softmax)       │
│                                                                  │
│   Each arrow represents learnable weights + bias:                │
│   h = activation(W·x + b)                                        │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
</pre>
                        </div>

                        <p>Let's implement a basic feedforward network for text classification in PyTorch:</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# Define a simple Feedforward Network for text classification
class TextMLP(nn.Module):
    def __init__(self, vocab_size, hidden_dim=256, num_classes=3, dropout=0.5):
        super(TextMLP, self).__init__()
        
        # Input layer to first hidden layer
        self.fc1 = nn.Linear(vocab_size, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        
        # Second hidden layer
        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)
        
        # Output layer
        self.fc3 = nn.Linear(hidden_dim // 2, num_classes)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # First hidden layer with ReLU activation
        x = self.fc1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Second hidden layer
        x = self.fc2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # Output layer (no activation - raw logits for CrossEntropyLoss)
        x = self.fc3(x)
        return x

# Create sample data: bag-of-words vectors
vocab_size = 5000
num_samples = 100
num_classes = 3

# Simulated sparse BoW vectors
X = torch.randn(num_samples, vocab_size).abs()  # Simulating word counts
y = torch.randint(0, num_classes, (num_samples,))

# Initialize model
model = TextMLP(vocab_size=vocab_size, hidden_dim=256, num_classes=num_classes)

# Forward pass
logits = model(X)
print(f"Input shape: {X.shape}")
print(f"Output logits shape: {logits.shape}")
print(f"Sample predictions (softmax): {F.softmax(logits[0], dim=0)}")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-layer-group me-2"></i>Understanding Layer Dimensions</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Architecture</span>
                                <span class="badge bg-crimson">PyTorch</span>
                            </div>
                            <div class="content">
                                <p>For a vocabulary of 10,000 words and 3-class classification with a 2-layer MLP:</p>
                                <ul>
                                    <li><strong>Input → Hidden1:</strong> 10,000 × 512 = 5.12M parameters</li>
                                    <li><strong>Hidden1 → Hidden2:</strong> 512 × 256 = 131K parameters</li>
                                    <li><strong>Hidden2 → Output:</strong> 256 × 3 = 768 parameters</li>
                                </ul>
                                <p>The first layer dominates parameter count. Using embeddings instead of raw BoW dramatically reduces this.</p>
                            </div>
                        </div>

                        <h3 id="activations">Activation Functions</h3>
                        
                        <p>Activation functions introduce nonlinearity into neural networks, allowing them to learn complex patterns beyond what linear transformations can capture. Without activation functions, stacking multiple linear layers would collapse into a single linear transformation. For NLP tasks, the choice of activation function affects both model performance and training dynamics.</p>

                        <p>The three most common activation functions in modern neural networks are <strong>ReLU</strong> (Rectified Linear Unit), <strong>Tanh</strong> (hyperbolic tangent), and <strong>Sigmoid</strong>. ReLU has become the default choice for hidden layers due to its computational efficiency and mitigation of the vanishing gradient problem. However, variants like Leaky ReLU and GELU (Gaussian Error Linear Unit) are increasingly popular, especially in transformer architectures.</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np

# Create input range for visualization
x = torch.linspace(-5, 5, 200)

# Define activation functions
activations = {
    'ReLU': F.relu(x),
    'Leaky ReLU': F.leaky_relu(x, negative_slope=0.1),
    'Tanh': torch.tanh(x),
    'Sigmoid': torch.sigmoid(x),
    'GELU': F.gelu(x),
    'ELU': F.elu(x, alpha=1.0)
}

# Visualize all activations
fig, axes = plt.subplots(2, 3, figsize=(14, 8))
axes = axes.flatten()

for idx, (name, activation) in enumerate(activations.items()):
    axes[idx].plot(x.numpy(), activation.numpy(), linewidth=2, color='teal')
    axes[idx].axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    axes[idx].axvline(x=0, color='gray', linestyle='--', alpha=0.5)
    axes[idx].set_title(name, fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Input')
    axes[idx].set_ylabel('Output')
    axes[idx].grid(True, alpha=0.3)
    axes[idx].set_xlim(-5, 5)

plt.tight_layout()
plt.savefig('activation_functions.png', dpi=150)
plt.show()
print("Activation functions visualized successfully!")
</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-lightbulb me-2"></i>Activation Function Selection Guide</h4>
                            <ul>
                                <li><strong>ReLU:</strong> Default for hidden layers. Fast, but can cause "dying neurons" (output always 0).</li>
                                <li><strong>Leaky ReLU:</strong> Prevents dying neurons by allowing small negative gradients.</li>
                                <li><strong>GELU:</strong> Smooth approximation of ReLU, used in BERT/GPT models.</li>
                                <li><strong>Tanh:</strong> Output range [-1, 1], useful when centered outputs are needed.</li>
                                <li><strong>Sigmoid:</strong> Output range [0, 1], used for binary classification output layers.</li>
                                <li><strong>Softmax:</strong> Converts logits to probability distribution (multi-class output).</li>
                            </ul>
                        </div>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

# Compare activation functions with gradients
class ActivationComparison(nn.Module):
    def __init__(self, activation_name='relu'):
        super().__init__()
        self.activation_name = activation_name
        
    def forward(self, x):
        if self.activation_name == 'relu':
            return F.relu(x)
        elif self.activation_name == 'leaky_relu':
            return F.leaky_relu(x, negative_slope=0.01)
        elif self.activation_name == 'gelu':
            return F.gelu(x)
        elif self.activation_name == 'tanh':
            return torch.tanh(x)
        elif self.activation_name == 'sigmoid':
            return torch.sigmoid(x)
        else:
            return x

# Test gradient flow for different activations
print("Gradient Analysis for Different Activations:")
print("=" * 50)

for act_name in ['relu', 'leaky_relu', 'gelu', 'tanh', 'sigmoid']:
    # Create test input with gradient tracking
    x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0], requires_grad=True)
    
    # Apply activation
    model = ActivationComparison(act_name)
    y = model(x)
    
    # Compute gradients (dy/dx for each input)
    y.sum().backward()
    
    print(f"\n{act_name.upper()}:")
    print(f"  Input:    {x.data.numpy()}")
    print(f"  Output:   {y.data.numpy().round(3)}")
    print(f"  Gradient: {x.grad.numpy().round(3)}")
</code></pre>

                        <h2 id="cnn-text"><i class="fas fa-filter me-2"></i>CNNs for Text Classification</h2>

                        <p>Convolutional Neural Networks (CNNs), originally designed for image processing, have proven remarkably effective for text classification. The key insight is that local patterns—n-gram features like "not good" or "very helpful"—are crucial for understanding text meaning. CNNs excel at detecting these patterns regardless of their position in the text, making them ideal for sentiment analysis and document classification.</p>

                        <p>Unlike feedforward networks that treat text as a bag of words, CNNs preserve word order and learn to recognize sequential patterns through convolution filters. The 2014 paper "Convolutional Neural Networks for Sentence Classification" by Yoon Kim demonstrated that a simple CNN architecture with multiple filter sizes could achieve state-of-the-art results on various text classification benchmarks.</p>

                        <h3 id="convolutions">1D Convolutions</h3>

                        <p>In text processing, we use 1D convolutions where filters slide across the sequence dimension (words) while spanning the entire embedding dimension. A filter with kernel size 3 looks at trigrams—three consecutive words at a time. Multiple filters with different kernel sizes (e.g., 2, 3, 4, 5) capture n-grams of varying lengths, allowing the network to detect both short phrases ("not bad") and longer expressions ("would definitely recommend").</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-diagram-project me-2"></i>CNN Architecture for Text Classification</h4>
<pre style="font-family: monospace; background: white; padding: 1rem; border-radius: 4px; overflow-x: auto;">
┌──────────────────────────────────────────────────────────────────────────┐
│                     CNN FOR SENTENCE CLASSIFICATION                       │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                           │
│  INPUT: "This movie was absolutely fantastic"                             │
│                                                                           │
│  ┌─────┬─────┬─────┬─────┬─────┐                                         │
│  │This │movie│ was │absol│fanta│  Word Tokens (seq_len=5)                │
│  └──┬──┴──┬──┴──┬──┴──┬──┴──┬──┘                                         │
│     ▼     ▼     ▼     ▼     ▼                                            │
│  ┌─────────────────────────────┐                                         │
│  │   EMBEDDING LAYER           │  (5 × 100) - each word → 100-dim vector │
│  │   [0.2, 0.5, ...] × 5       │                                         │
│  └─────────────┬───────────────┘                                         │
│                ▼                                                          │
│  ┌─────────────────────────────────────────────────────────────┐         │
│  │            PARALLEL CONVOLUTIONAL FILTERS                    │         │
│  │                                                              │         │
│  │  ┌────────────┐  ┌────────────┐  ┌────────────┐             │         │
│  │  │ Filter k=2 │  │ Filter k=3 │  │ Filter k=4 │             │         │
│  │  │ (bigrams)  │  │ (trigrams) │  │ (4-grams)  │             │         │
│  │  │ 100 filters│  │ 100 filters│  │ 100 filters│             │         │
│  │  └─────┬──────┘  └─────┬──────┘  └─────┬──────┘             │         │
│  │        ▼               ▼               ▼                     │         │
│  │   [f₁...f₁₀₀]    [f₁...f₁₀₀]    [f₁...f₁₀₀]                │         │
│  │   seq_len=4      seq_len=3      seq_len=2                   │         │
│  └─────────────────────────────────────────────────────────────┘         │
│                                                                           │
│  ┌─────────────────────────────────────────────────────────────┐         │
│  │                  MAX POOLING (over sequence)                 │         │
│  │                                                              │         │
│  │   [max₁...max₁₀₀] [max₁...max₁₀₀] [max₁...max₁₀₀]          │         │
│  │        100 values      100 values      100 values           │         │
│  └──────────────────────────┬──────────────────────────────────┘         │
│                             ▼                                             │
│  ┌─────────────────────────────────────────────────────────────┐         │
│  │                    CONCATENATE                               │         │
│  │           300-dimensional feature vector                     │         │
│  └──────────────────────────┬──────────────────────────────────┘         │
│                             ▼                                             │
│  ┌─────────────────────────────────────────────────────────────┐         │
│  │              DROPOUT (0.5) + FULLY CONNECTED                 │         │
│  │                    300 → num_classes                         │         │
│  └──────────────────────────┬──────────────────────────────────┘         │
│                             ▼                                             │
│                     ┌───────────────┐                                    │
│                     │ Softmax Output│  [0.02, 0.08, 0.90]               │
│                     │ pos/neg/neut  │                                    │
│                     └───────────────┘                                    │
│                                                                           │
└──────────────────────────────────────────────────────────────────────────┘
</pre>
                        </div>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

# Demonstrate 1D Convolution mechanics for text
print("Understanding 1D Convolution for Text")
print("=" * 50)

# Sample embedded sentence: batch=1, seq_len=7, embed_dim=5
# Simulating: "The movie was really very quite good"
torch.manual_seed(42)
embedded_sentence = torch.randn(1, 7, 5)  # (batch, seq_len, embed_dim)
print(f"Input shape: {embedded_sentence.shape}")
print(f"Interpretation: batch_size=1, sequence_length=7, embedding_dim=5")

# PyTorch Conv1d expects: (batch, channels, seq_len)
# For text: channels = embedding_dim
text_input = embedded_sentence.transpose(1, 2)  # (1, 5, 7)
print(f"\nAfter transpose for Conv1d: {text_input.shape}")
print(f"Interpretation: batch=1, embed_dim=5, seq_len=7")

# Create convolution filters
conv_bigram = nn.Conv1d(in_channels=5, out_channels=10, kernel_size=2)
conv_trigram = nn.Conv1d(in_channels=5, out_channels=10, kernel_size=3)
conv_fourgram = nn.Conv1d(in_channels=5, out_channels=10, kernel_size=4)

# Apply convolutions
bigram_features = conv_bigram(text_input)
trigram_features = conv_trigram(text_input)
fourgram_features = conv_fourgram(text_input)

print(f"\nBigram convolution output: {bigram_features.shape}")
print(f"  (seq_len=7, kernel=2 → output_len=7-2+1=6)")
print(f"Trigram convolution output: {trigram_features.shape}")
print(f"  (seq_len=7, kernel=3 → output_len=7-3+1=5)")
print(f"4-gram convolution output: {fourgram_features.shape}")
print(f"  (seq_len=7, kernel=4 → output_len=7-4+1=4)")
</code></pre>

                        <h3 id="pooling">Pooling Strategies</h3>

                        <p>After convolution, pooling reduces the variable-length feature maps to fixed-size vectors. <strong>Max pooling</strong> over the sequence selects the strongest activation for each filter, capturing the most salient n-gram pattern regardless of its position. This is crucial for text where a sentiment-bearing phrase could appear anywhere in the document. Alternative strategies include <strong>average pooling</strong> (captures overall feature presence) and <strong>k-max pooling</strong> (retains top-k activations for more nuanced representation).</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

# Demonstrate different pooling strategies
torch.manual_seed(42)

# Simulated convolution output: (batch=2, num_filters=4, seq_len=10)
conv_output = torch.randn(2, 4, 10)
print("Convolution output shape:", conv_output.shape)
print("Sample filter activations (batch 0, filter 0):")
print(conv_output[0, 0].numpy().round(2))

# 1. Global Max Pooling - most common for text
max_pooled = F.max_pool1d(conv_output, kernel_size=conv_output.size(2))
max_pooled = max_pooled.squeeze(-1)
print(f"\nMax Pooling result: {max_pooled.shape}")
print(f"Values (batch 0): {max_pooled[0].numpy().round(3)}")
print("(Captures strongest activation per filter)")

# 2. Global Average Pooling
avg_pooled = F.avg_pool1d(conv_output, kernel_size=conv_output.size(2))
avg_pooled = avg_pooled.squeeze(-1)
print(f"\nAverage Pooling result: {avg_pooled.shape}")
print(f"Values (batch 0): {avg_pooled[0].numpy().round(3)}")
print("(Captures overall feature presence)")

# 3. K-Max Pooling (keep top-k values per filter)
k = 3
k_max_pooled, indices = torch.topk(conv_output, k=k, dim=2)
print(f"\nK-Max Pooling (k=3) result: {k_max_pooled.shape}")
print(f"Top-3 values (batch 0, filter 0): {k_max_pooled[0, 0].numpy().round(3)}")
print(f"Their positions: {indices[0, 0].numpy()}")
print("(Retains multiple strong activations per filter)")

# 4. Attention-weighted pooling (learned importance)
attention_weights = F.softmax(conv_output, dim=2)
attention_pooled = (conv_output * attention_weights).sum(dim=2)
print(f"\nAttention Pooling result: {attention_pooled.shape}")
print(f"Values (batch 0): {attention_pooled[0].numpy().round(3)}")
print("(Weighted sum based on learned importance)")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-flask me-2"></i>Complete CNN Text Classifier (Kim 2014 Style)</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Implementation</span>
                                <span class="badge bg-crimson">Production-Ready</span>
                            </div>
                            <div class="content">
                                <p>This implementation follows the influential "Convolutional Neural Networks for Sentence Classification" paper with multiple parallel filter sizes.</p>
                            </div>
                        </div>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class TextCNN(nn.Module):
    """
    CNN for text classification following Kim (2014) architecture.
    Multiple filter sizes capture n-grams of different lengths.
    """
    def __init__(self, vocab_size, embed_dim=300, num_classes=2, 
                 num_filters=100, filter_sizes=[2, 3, 4, 5], 
                 dropout=0.5, pretrained_embeddings=None):
        super(TextCNN, self).__init__()
        
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # Load pretrained embeddings if provided
        if pretrained_embeddings is not None:
            self.embedding.weight.data.copy_(pretrained_embeddings)
            self.embedding.weight.requires_grad = True  # Fine-tune embeddings
        
        # Convolutional layers - one for each filter size
        self.convs = nn.ModuleList([
            nn.Conv1d(in_channels=embed_dim, 
                      out_channels=num_filters, 
                      kernel_size=fs)
            for fs in filter_sizes
        ])
        
        # Fully connected layer
        # Input: num_filters * len(filter_sizes) features after concatenation
        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # Store config
        self.filter_sizes = filter_sizes
        self.num_filters = num_filters
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len) - token indices
        Returns:
            logits: (batch_size, num_classes)
        """
        # Embed tokens: (batch, seq_len) -> (batch, seq_len, embed_dim)
        embedded = self.embedding(x)
        
        # Transpose for Conv1d: (batch, embed_dim, seq_len)
        embedded = embedded.transpose(1, 2)
        
        # Apply convolutions + ReLU + max-pool
        conv_outputs = []
        for conv in self.convs:
            # Conv: (batch, num_filters, seq_len - filter_size + 1)
            conv_out = F.relu(conv(embedded))
            # Max pool over sequence: (batch, num_filters, 1)
            pooled = F.max_pool1d(conv_out, conv_out.size(2))
            # Squeeze: (batch, num_filters)
            pooled = pooled.squeeze(2)
            conv_outputs.append(pooled)
        
        # Concatenate all filter outputs: (batch, num_filters * len(filter_sizes))
        cat = torch.cat(conv_outputs, dim=1)
        
        # Dropout + FC
        cat = self.dropout(cat)
        logits = self.fc(cat)
        
        return logits

# Test the model
vocab_size = 10000
batch_size = 32
seq_len = 50

# Create model
model = TextCNN(
    vocab_size=vocab_size,
    embed_dim=300,
    num_classes=3,
    num_filters=100,
    filter_sizes=[2, 3, 4, 5],
    dropout=0.5
)

# Sample input: batch of token indices
sample_input = torch.randint(1, vocab_size, (batch_size, seq_len))

# Forward pass
output = model(sample_input)
print(f"Input shape: {sample_input.shape}")
print(f"Output shape: {output.shape}")
print(f"\nModel architecture:")
print(model)
print(f"\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}")
</code></pre>

                        <h2 id="training"><i class="fas fa-graduation-cap me-2"></i>Training Neural Text Models</h2>

                        <p>Training neural networks for NLP involves forward propagation (computing predictions), loss calculation (measuring error), backpropagation (computing gradients), and parameter updates (optimization). The training loop iterates through the dataset multiple times (epochs), gradually adjusting weights to minimize the loss function. Understanding the components of this process is essential for building effective text classifiers.</p>

                        <h3 id="loss">Loss Functions</h3>

                        <p>Loss functions quantify how far model predictions are from the true labels. For classification tasks, <strong>Cross-Entropy Loss</strong> is the standard choice, combining softmax activation with negative log-likelihood. It penalizes confident wrong predictions more than uncertain ones, encouraging the model to assign high probability to correct classes. For binary classification, <strong>Binary Cross-Entropy (BCE)</strong> is used with sigmoid activation.</p>

                        <div class="highlight-box">
                            <h4><i class="fas fa-calculator me-2"></i>Cross-Entropy Loss Explained</h4>
                            <p><strong>Cross-Entropy Formula:</strong></p>
                            <p style="text-align: center; font-family: monospace;">CE(y, ŷ) = -Σᵢ yᵢ · log(ŷᵢ)</p>
                            <p>Where <code>y</code> is the true distribution (one-hot) and <code>ŷ</code> is the predicted probability distribution.</p>
                            <p>For a single sample with true class <code>c</code>: <strong>CE = -log(ŷ_c)</strong></p>
                            <ul>
                                <li>If model predicts 0.9 for correct class: loss = -log(0.9) = 0.105</li>
                                <li>If model predicts 0.5 for correct class: loss = -log(0.5) = 0.693</li>
                                <li>If model predicts 0.1 for correct class: loss = -log(0.1) = 2.302</li>
                            </ul>
                        </div>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# Understanding Cross-Entropy Loss
print("Cross-Entropy Loss Deep Dive")
print("=" * 50)

# Scenario: 3-class classification (pos, neg, neutral)
num_classes = 3
batch_size = 4

# Model outputs raw logits (before softmax)
logits = torch.tensor([
    [2.0, 0.5, 0.1],   # Confident positive
    [0.1, 2.5, 0.2],   # Confident negative
    [0.8, 0.7, 0.9],   # Uncertain
    [0.2, 3.0, 0.1],   # Very confident negative
])

# True labels
labels = torch.tensor([0, 1, 2, 0])  # [pos, neg, neutral, pos]
label_names = ['positive', 'negative', 'neutral']

# Method 1: PyTorch CrossEntropyLoss (combines LogSoftmax + NLLLoss)
criterion = nn.CrossEntropyLoss(reduction='none')  # Per-sample loss
losses = criterion(logits, labels)

print("Logits and corresponding losses:")
for i in range(batch_size):
    probs = F.softmax(logits[i], dim=0)
    true_class = labels[i].item()
    print(f"\nSample {i+1}:")
    print(f"  Logits: {logits[i].numpy().round(2)}")
    print(f"  Softmax probabilities: {probs.numpy().round(3)}")
    print(f"  True class: {label_names[true_class]} (index {true_class})")
    print(f"  P(true class): {probs[true_class]:.3f}")
    print(f"  Loss: -log({probs[true_class]:.3f}) = {losses[i]:.3f}")

print(f"\nMean loss: {losses.mean():.3f}")

# Method 2: Manual calculation to understand
print("\n" + "=" * 50)
print("Manual Cross-Entropy Calculation:")
log_probs = F.log_softmax(logits, dim=1)
manual_loss = -log_probs[range(batch_size), labels]
print(f"Manual losses: {manual_loss.detach().numpy().round(3)}")
print(f"Matches PyTorch: {torch.allclose(losses, manual_loss)}")
</code></pre>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

# Different loss functions for different scenarios
print("Loss Functions for NLP Tasks")
print("=" * 50)

# 1. Multi-class classification (mutually exclusive)
print("\n1. CrossEntropyLoss (Multi-class, single label)")
ce_loss = nn.CrossEntropyLoss()
logits_multi = torch.randn(4, 5)  # 4 samples, 5 classes
labels_multi = torch.tensor([0, 2, 1, 4])
loss1 = ce_loss(logits_multi, labels_multi)
print(f"   Loss: {loss1.item():.4f}")

# 2. Binary classification
print("\n2. BCEWithLogitsLoss (Binary classification)")
bce_loss = nn.BCEWithLogitsLoss()
logits_binary = torch.randn(4, 1)  # 4 samples
labels_binary = torch.tensor([[1.], [0.], [1.], [0.]])
loss2 = bce_loss(logits_binary, labels_binary)
print(f"   Loss: {loss2.item():.4f}")

# 3. Multi-label classification (multiple labels per sample)
print("\n3. BCEWithLogitsLoss (Multi-label)")
logits_multilabel = torch.randn(4, 5)  # 4 samples, 5 possible labels
labels_multilabel = torch.tensor([
    [1, 0, 1, 0, 0],  # Sample has labels 0 and 2
    [0, 1, 0, 1, 1],  # Sample has labels 1, 3, 4
    [1, 1, 0, 0, 0],  # Sample has labels 0 and 1
    [0, 0, 0, 1, 0],  # Sample has label 3 only
], dtype=torch.float)
loss3 = bce_loss(logits_multilabel, labels_multilabel)
print(f"   Loss: {loss3.item():.4f}")

# 4. With class weights (for imbalanced datasets)
print("\n4. CrossEntropyLoss with class weights")
# If class 0 is rare, give it higher weight
class_weights = torch.tensor([3.0, 1.0, 1.0, 1.0, 2.0])
weighted_ce = nn.CrossEntropyLoss(weight=class_weights)
loss4 = weighted_ce(logits_multi, labels_multi)
print(f"   Unweighted loss: {loss1.item():.4f}")
print(f"   Weighted loss: {loss4.item():.4f}")

# 5. Label smoothing (regularization technique)
print("\n5. CrossEntropyLoss with label smoothing")
smooth_ce = nn.CrossEntropyLoss(label_smoothing=0.1)
loss5 = smooth_ce(logits_multi, labels_multi)
print(f"   Without smoothing: {loss1.item():.4f}")
print(f"   With smoothing (0.1): {loss5.item():.4f}")
print("   (Label smoothing prevents overconfident predictions)")
</code></pre>

                        <h3 id="optimization">Optimization & Regularization</h3>

                        <p>Optimizers update model parameters based on computed gradients. <strong>Adam</strong> (Adaptive Moment Estimation) is the most popular choice for NLP, combining momentum with adaptive learning rates per parameter. It typically works well out-of-the-box with learning rates around 1e-3 to 1e-4. For fine-tuning pretrained models, <strong>AdamW</strong> (Adam with weight decay) is preferred as it decouples weight decay from the gradient update.</p>

                        <p>Regularization prevents overfitting by constraining model capacity or adding noise during training. <strong>Dropout</strong> randomly zeroes neurons during training, forcing the network to learn redundant representations. <strong>Weight decay</strong> (L2 regularization) penalizes large weights, encouraging simpler models. <strong>Early stopping</strong> monitors validation loss and stops training when it starts increasing, preventing the model from memorizing training data.</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim

# Create a simple model for demonstration
model = nn.Sequential(
    nn.Linear(100, 64),
    nn.ReLU(),
    nn.Dropout(0.5),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 3)
)

# Different optimizer options
print("Optimizer Comparison")
print("=" * 50)

# 1. SGD with momentum
sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
print("\n1. SGD with Momentum:")
print(f"   lr=0.01, momentum=0.9")
print("   Good for: Simple problems, when you need reproducibility")

# 2. Adam - most popular for NLP
adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
print("\n2. Adam:")
print(f"   lr=0.001, betas=(0.9, 0.999)")
print("   Good for: Most NLP tasks, works well out-of-box")

# 3. AdamW - preferred for transformers/fine-tuning
adamw = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
print("\n3. AdamW:")
print(f"   lr=0.001, weight_decay=0.01")
print("   Good for: Fine-tuning BERT/GPT, prevents overfitting")

# 4. Learning rate scheduler
optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=3, verbose=True
)
print("\n4. Learning Rate Scheduler (ReduceLROnPlateau):")
print("   Reduces LR by 0.5 if val_loss doesn't improve for 3 epochs")

# 5. Warmup + Linear Decay (common for transformers)
from torch.optim.lr_scheduler import LambdaLR

def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        return max(0.0, float(num_training_steps - current_step) / 
                   float(max(1, num_training_steps - num_warmup_steps)))
    return LambdaLR(optimizer, lr_lambda)

scheduler_warmup = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=100, num_training_steps=1000
)
print("\n5. Linear Schedule with Warmup:")
print("   Warmup: 100 steps, then linear decay to 0")
print("   Standard for BERT/transformer fine-tuning")
</code></pre>

                        <div class="experiment-card">
                            <h4><i class="fas fa-shield-alt me-2"></i>Regularization Techniques Summary</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Best Practices</span>
                                <span class="badge bg-crimson">Prevent Overfitting</span>
                            </div>
                            <div class="content">
                                <table style="width: 100%; border-collapse: collapse;">
                                    <tr style="background: var(--color-teal); color: white;">
                                        <th style="padding: 8px; text-align: left;">Technique</th>
                                        <th style="padding: 8px; text-align: left;">When to Use</th>
                                        <th style="padding: 8px; text-align: left;">Typical Values</th>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;"><strong>Dropout</strong></td>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;">Always, especially in FC layers</td>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;">0.3-0.5 (text), 0.1-0.3 (transformers)</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;"><strong>Weight Decay</strong></td>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;">Fine-tuning, large models</td>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;">0.01-0.1</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;"><strong>Early Stopping</strong></td>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;">Always monitor validation loss</td>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;">Patience: 3-10 epochs</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;"><strong>Batch Norm</strong></td>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;">Deep MLPs, CNNs</td>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;">After linear/conv, before activation</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;"><strong>Label Smoothing</strong></td>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;">Classification with many classes</td>
                                        <td style="padding: 8px; border-bottom: 1px solid #ddd;">0.1</td>
                                    </tr>
                                    <tr>
                                        <td style="padding: 8px;"><strong>Gradient Clipping</strong></td>
                                        <td style="padding: 8px;">RNNs, transformers</td>
                                        <td style="padding: 8px;">max_norm=1.0</td>
                                    </tr>
                                </table>
                            </div>
                        </div>

                        <h2 id="embeddings-layer"><i class="fas fa-layer-group me-2"></i>Embedding Layers</h2>

                        <p>Embedding layers transform discrete token indices into continuous dense vectors. Instead of using sparse one-hot encodings or pre-computed embeddings, neural networks can learn task-specific embeddings during training. The embedding layer is essentially a lookup table: each vocabulary index maps to a trainable vector of dimension <code>embed_dim</code>. This allows the network to learn semantic relationships from the classification signal.</p>

                        <p>In practice, you can either train embeddings from scratch (good for domain-specific vocabulary) or initialize with pretrained embeddings like Word2Vec or GloVe and fine-tune them. Freezing pretrained embeddings prevents overfitting on small datasets, while fine-tuning adapts them to your specific task. A common strategy is to freeze embeddings initially and unfreeze them after a few epochs.</p>

<pre><code class="language-python">import torch
import torch.nn as nn
import numpy as np

# Understanding PyTorch Embedding Layer
print("Embedding Layer Mechanics")
print("=" * 50)

# Create vocabulary
vocab = {'&lt;PAD&gt;': 0, '&lt;UNK&gt;': 1, 'the': 2, 'cat': 3, 'sat': 4, 'on': 5, 'mat': 6}
vocab_size = len(vocab)
embed_dim = 4

# Create embedding layer
embedding = nn.Embedding(
    num_embeddings=vocab_size,
    embedding_dim=embed_dim,
    padding_idx=0  # Embedding for PAD stays zero
)

print(f"Vocabulary size: {vocab_size}")
print(f"Embedding dimension: {embed_dim}")
print(f"\nEmbedding weight matrix shape: {embedding.weight.shape}")
print(f"Embedding weight matrix:\n{embedding.weight.data.numpy().round(3)}")

# Convert sentence to indices
sentence = ['the', 'cat', 'sat', 'on', 'the', 'mat']
indices = torch.tensor([[vocab[word] for word in sentence]])
print(f"\nSentence: {sentence}")
print(f"Token indices: {indices.numpy()}")

# Get embeddings
embedded = embedding(indices)
print(f"\nEmbedded shape: {embedded.shape}")
print(f"(batch_size=1, seq_len=6, embed_dim=4)")
print(f"\nEmbedded vectors:\n{embedded.detach().numpy().round(3)}")

# Note: Same word 'the' has same embedding (indices 0 and 4)
print(f"\n'the' appears at positions 0 and 4:")
print(f"Position 0 embedding: {embedded[0, 0].detach().numpy().round(3)}")
print(f"Position 4 embedding: {embedded[0, 4].detach().numpy().round(3)}")
print(f"Same? {torch.allclose(embedded[0, 0], embedded[0, 4])}")
</code></pre>

<pre><code class="language-python">import torch
import torch.nn as nn
import numpy as np

# Loading and using pretrained embeddings
print("Using Pretrained Embeddings")
print("=" * 50)

# Simulated pretrained GloVe embeddings (normally loaded from file)
# Format: word -> vector
pretrained = {
    'the': [0.1, 0.2, 0.3, 0.4],
    'cat': [0.5, 0.1, 0.8, 0.2],
    'dog': [0.4, 0.2, 0.7, 0.3],
    'sat': [0.2, 0.9, 0.1, 0.5],
    'ran': [0.3, 0.8, 0.2, 0.6],
}

# Build vocabulary (add special tokens)
vocab = {'&lt;PAD&gt;': 0, '&lt;UNK&gt;': 1}
for word in pretrained:
    vocab[word] = len(vocab)
    
vocab_size = len(vocab)
embed_dim = 4

# Create embedding matrix
embedding_matrix = np.zeros((vocab_size, embed_dim))
found = 0
for word, idx in vocab.items():
    if word in pretrained:
        embedding_matrix[idx] = pretrained[word]
        found += 1
    elif word == '&lt;PAD&gt;':
        embedding_matrix[idx] = np.zeros(embed_dim)  # PAD is zeros
    else:
        embedding_matrix[idx] = np.random.normal(0, 0.1, embed_dim)  # Random for UNK

print(f"Vocabulary size: {vocab_size}")
print(f"Found {found}/{vocab_size-2} pretrained embeddings")

# Create embedding layer with pretrained weights
embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
embedding.weight.data.copy_(torch.from_numpy(embedding_matrix).float())

# Option 1: Freeze embeddings (no fine-tuning)
embedding.weight.requires_grad = False
print("\nOption 1: Frozen embeddings")
print("  - Faster training, less memory")
print("  - Good for small datasets")

# Option 2: Fine-tune embeddings
embedding.weight.requires_grad = True
print("\nOption 2: Fine-tunable embeddings")
print("  - Adapts to your task")
print("  - Needs more data to prevent overfitting")

# Test
test_indices = torch.tensor([[vocab['the'], vocab['cat'], vocab['sat']]])
embedded = embedding(test_indices)
print(f"\nTest embedding shape: {embedded.shape}")
print(f"'the' embedding: {embedded[0, 0].detach().numpy()}")
</code></pre>

                        <h2 id="practical"><i class="fas fa-code me-2"></i>Practical Implementation</h2>

                        <p>Now let's put everything together with a complete, production-ready text classification pipeline. This includes data preprocessing, vocabulary building, dataset creation, model training with early stopping, and evaluation. The code is modular and can be adapted for different text classification tasks.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-project-diagram me-2"></i>Complete Training Pipeline</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">End-to-End</span>
                                <span class="badge bg-crimson">Production Ready</span>
                            </div>
                            <div class="content">
                                <p>This comprehensive example demonstrates sentiment analysis using a CNN, including data loading, preprocessing, training loop with validation, and model evaluation.</p>
                            </div>
                        </div>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import numpy as np
import re

# Step 1: Text Preprocessing
class TextPreprocessor:
    def __init__(self, max_vocab_size=10000, max_seq_len=100):
        self.max_vocab_size = max_vocab_size
        self.max_seq_len = max_seq_len
        self.word2idx = {'&lt;PAD&gt;': 0, '&lt;UNK&gt;': 1}
        self.idx2word = {0: '&lt;PAD&gt;', 1: '&lt;UNK&gt;'}
        
    def tokenize(self, text):
        """Simple tokenization: lowercase and split on non-alphanumeric."""
        text = text.lower()
        text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text)
        return text.split()
    
    def build_vocab(self, texts):
        """Build vocabulary from training texts."""
        word_counts = Counter()
        for text in texts:
            word_counts.update(self.tokenize(text))
        
        # Keep most common words
        most_common = word_counts.most_common(self.max_vocab_size - 2)
        for word, count in most_common:
            idx = len(self.word2idx)
            self.word2idx[word] = idx
            self.idx2word[idx] = word
            
        print(f"Vocabulary built: {len(self.word2idx)} words")
        return self
    
    def encode(self, text):
        """Convert text to padded sequence of indices."""
        tokens = self.tokenize(text)
        indices = [self.word2idx.get(w, 1) for w in tokens]  # 1 = UNK
        
        # Pad or truncate
        if len(indices) < self.max_seq_len:
            indices = indices + [0] * (self.max_seq_len - len(indices))
        else:
            indices = indices[:self.max_seq_len]
            
        return indices

# Step 2: Dataset Class
class TextDataset(Dataset):
    def __init__(self, texts, labels, preprocessor):
        self.texts = texts
        self.labels = labels
        self.preprocessor = preprocessor
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        indices = self.preprocessor.encode(text)
        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)

# Create sample data
texts = [
    "This movie was absolutely fantastic and amazing",
    "Terrible film, waste of time, really bad",
    "It was okay, nothing special, average movie",
    "Best movie I have ever seen, loved it",
    "Awful, boring, and poorly made disaster",
    "Pretty good film with nice acting",
    "Not great but not terrible either",
    "Wonderful performances and beautiful cinematography",
    "Disappointing and dull, expected more",
    "A masterpiece of modern cinema",
] * 50  # Multiply for more samples

labels = [2, 0, 1, 2, 0, 2, 1, 2, 0, 2] * 50  # 0=neg, 1=neutral, 2=pos

# Build preprocessor and datasets
preprocessor = TextPreprocessor(max_vocab_size=1000, max_seq_len=20)
preprocessor.build_vocab(texts)

# Train/val split
split_idx = int(len(texts) * 0.8)
train_dataset = TextDataset(texts[:split_idx], labels[:split_idx], preprocessor)
val_dataset = TextDataset(texts[split_idx:], labels[split_idx:], preprocessor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
</code></pre>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

# Step 3: Model Definition (CNN)
class SentimentCNN(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, num_classes=3, 
                 num_filters=64, filter_sizes=[2, 3, 4], dropout=0.5):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        self.convs = nn.ModuleList([
            nn.Conv1d(embed_dim, num_filters, fs) for fs in filter_sizes
        ])
        
        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # x: (batch, seq_len)
        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)
        embedded = embedded.transpose(1, 2)  # (batch, embed_dim, seq_len)
        
        conv_outputs = []
        for conv in self.convs:
            conv_out = F.relu(conv(embedded))
            pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)
            conv_outputs.append(pooled)
            
        cat = torch.cat(conv_outputs, dim=1)
        cat = self.dropout(cat)
        return self.fc(cat)

# Initialize model (use vocab_size from preprocessor)
vocab_size = len(preprocessor.word2idx) if 'preprocessor' in dir() else 1000
model = SentimentCNN(vocab_size=vocab_size, embed_dim=128, num_classes=3)
print(f"Model created with {sum(p.numel() for p in model.parameters()):,} parameters")
print(model)
</code></pre>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np

# Step 4: Training Loop with Early Stopping

def train_epoch(model, dataloader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch_x, batch_y in dataloader:
        batch_x, batch_y = batch_x.to(device), batch_y.to(device)
        
        optimizer.zero_grad()
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        correct += predicted.eq(batch_y).sum().item()
        total += batch_y.size(0)
        
    return total_loss / len(dataloader), correct / total

def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_x, batch_y in dataloader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            correct += predicted.eq(batch_y).sum().item()
            total += batch_y.size(0)
            
    return total_loss / len(dataloader), correct / total

# Training configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Recreate model and move to device
vocab_size = 1000  # Should match preprocessor
model = SentimentCNN(vocab_size=vocab_size, embed_dim=128, num_classes=3)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)

# Create dummy data for standalone execution
from torch.utils.data import TensorDataset
dummy_x = torch.randint(0, vocab_size, (400, 20))
dummy_y = torch.randint(0, 3, (400,))
train_dataset = TensorDataset(dummy_x[:320], dummy_y[:320])
val_dataset = TensorDataset(dummy_x[320:], dummy_y[320:])
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

# Training with early stopping
num_epochs = 20
patience = 5
best_val_loss = float('inf')
patience_counter = 0
history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

print("\nStarting training...")
print("-" * 60)

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)
    val_loss, val_acc = evaluate(model, val_loader, criterion, device)
    
    history['train_loss'].append(train_loss)
    history['train_acc'].append(train_acc)
    history['val_loss'].append(val_loss)
    history['val_acc'].append(val_acc)
    
    scheduler.step(val_loss)
    
    print(f"Epoch {epoch+1:2d}/{num_epochs} | "
          f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
          f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}")
    
    # Early stopping check
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        torch.save(model.state_dict(), 'best_model.pt')
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"\nEarly stopping at epoch {epoch+1}")
            break

print("-" * 60)
print(f"Best validation loss: {best_val_loss:.4f}")
</code></pre>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Step 5: Model Evaluation and Inference

def predict_sentiment(model, texts, preprocessor, device, label_names=['negative', 'neutral', 'positive']):
    """Predict sentiment for a list of texts."""
    model.eval()
    predictions = []
    confidences = []
    
    with torch.no_grad():
        for text in texts:
            indices = torch.tensor([preprocessor.encode(text)], dtype=torch.long).to(device)
            logits = model(indices)
            probs = F.softmax(logits, dim=1)
            pred_class = probs.argmax(dim=1).item()
            confidence = probs.max().item()
            
            predictions.append(label_names[pred_class])
            confidences.append(confidence)
            
    return predictions, confidences

# Create a simple preprocessor for demonstration
class SimplePreprocessor:
    def __init__(self):
        self.word2idx = {'<PAD>': 0, '<UNK>': 1}
        for i in range(998):
            self.word2idx[f'word{i}'] = i + 2
    
    def encode(self, text):
        tokens = text.lower().split()
        indices = [self.word2idx.get(w, 1) for w in tokens]
        if len(indices) < 20:
            indices = indices + [0] * (20 - len(indices))
        return indices[:20]

# Demo model and preprocessor
vocab_size = 1000
device = torch.device('cpu')

class DemoSentimentCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, 128, padding_idx=0)
        self.conv = nn.Conv1d(128, 64, 3)
        self.fc = nn.Linear(64, 3)
        
    def forward(self, x):
        embedded = self.embedding(x).transpose(1, 2)
        conv_out = F.relu(self.conv(embedded))
        pooled = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)
        return self.fc(pooled)

model = DemoSentimentCNN().to(device)
preprocessor = SimplePreprocessor()

# Test predictions
test_texts = [
    "This is absolutely wonderful and amazing",
    "Terrible experience, really disappointing",
    "It was okay, nothing special"
]

predictions, confidences = predict_sentiment(model, test_texts, preprocessor, device)

print("Sentiment Predictions")
print("=" * 60)
for text, pred, conf in zip(test_texts, predictions, confidences):
    print(f"Text: '{text}'")
    print(f"  Prediction: {pred} (confidence: {conf:.2%})")
    print()
</code></pre>

<pre><code class="language-python">import torch
import matplotlib.pyplot as plt
import numpy as np

# Step 6: Visualize Training History

# Sample training history (replace with actual history from training)
history = {
    'train_loss': [1.1, 0.8, 0.6, 0.5, 0.4, 0.35, 0.3, 0.28, 0.25, 0.22],
    'val_loss': [1.0, 0.75, 0.6, 0.55, 0.52, 0.51, 0.52, 0.54, 0.56, 0.58],
    'train_acc': [0.4, 0.55, 0.65, 0.72, 0.78, 0.82, 0.85, 0.87, 0.89, 0.91],
    'val_acc': [0.42, 0.58, 0.66, 0.70, 0.72, 0.73, 0.72, 0.71, 0.70, 0.69]
}

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Plot loss
epochs = range(1, len(history['train_loss']) + 1)
ax1.plot(epochs, history['train_loss'], 'b-o', label='Training Loss', linewidth=2, markersize=6)
ax1.plot(epochs, history['val_loss'], 'r-s', label='Validation Loss', linewidth=2, markersize=6)
ax1.axvline(x=6, color='green', linestyle='--', label='Best Model (Epoch 6)')
ax1.set_xlabel('Epoch', fontsize=12)
ax1.set_ylabel('Loss', fontsize=12)
ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.3)

# Plot accuracy
ax2.plot(epochs, history['train_acc'], 'b-o', label='Training Accuracy', linewidth=2, markersize=6)
ax2.plot(epochs, history['val_acc'], 'r-s', label='Validation Accuracy', linewidth=2, markersize=6)
ax2.axvline(x=6, color='green', linestyle='--', label='Best Model (Epoch 6)')
ax2.set_xlabel('Epoch', fontsize=12)
ax2.set_ylabel('Accuracy', fontsize=12)
ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
ax2.legend(fontsize=10)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history.png', dpi=150)
plt.show()

print("Training curves show overfitting starting around epoch 6")
print("(validation loss starts increasing while training loss continues to decrease)")
</code></pre>

                        <div class="highlight-box">
                            <h4><i class="fas fa-clipboard-check me-2"></i>Neural NLP Checklist</h4>
                            <p>Before training your neural text classifier, ensure you've addressed these critical points:</p>
                            <ul>
                                <li><strong>Data Preprocessing:</strong> Consistent tokenization, vocabulary size, sequence length</li>
                                <li><strong>Embedding Strategy:</strong> Random init vs. pretrained, frozen vs. fine-tuned</li>
                                <li><strong>Architecture:</strong> Hidden dimensions, number of layers, filter sizes (CNN)</li>
                                <li><strong>Regularization:</strong> Dropout rate, weight decay, gradient clipping</li>
                                <li><strong>Optimization:</strong> Learning rate, scheduler, batch size</li>
                                <li><strong>Monitoring:</strong> Train/val loss curves, early stopping criteria</li>
                                <li><strong>Evaluation:</strong> Appropriate metrics (accuracy, F1, confusion matrix)</li>
                            </ul>
                        </div>

                        <h2 id="conclusion"><i class="fas fa-flag-checkered me-2"></i>Conclusion & Next Steps</h2>

                        <p>Neural networks have revolutionized NLP by automatically learning hierarchical representations from text data. In this guide, we covered the fundamental building blocks: feedforward networks that transform text vectors through dense layers, CNNs that capture local n-gram patterns through convolution and pooling, and the training infrastructure including loss functions, optimizers, and regularization techniques that make learning possible.</p>

                        <p>The key insights to remember are: (1) activation functions like ReLU introduce crucial nonlinearity, (2) embedding layers convert discrete tokens to learnable continuous vectors, (3) CNNs excel at capturing local patterns regardless of position, (4) cross-entropy loss guides multi-class classification learning, and (5) regularization through dropout and early stopping prevents overfitting. These fundamentals form the foundation for more advanced architectures.</p>

                        <p>However, feedforward networks and CNNs have a significant limitation: they don't naturally model sequential dependencies. A CNN can detect "not good" as a pattern, but it struggles with long-range dependencies like "I thought this movie would be bad, but actually it turned out to be quite good." This motivates our next topic: Recurrent Neural Networks (RNNs), LSTMs, and GRUs, which process text sequentially and maintain memory across the entire sequence.</p>

                        <div class="experiment-card">
                            <h4><i class="fas fa-road me-2"></i>Learning Path: What's Next?</h4>
                            <div class="meta mb-2">
                                <span class="badge bg-teal me-2">Next Steps</span>
                                <span class="badge bg-crimson">Part 7</span>
                            </div>
                            <div class="content">
                                <p>In <strong>Part 7: RNNs, LSTMs & GRUs</strong>, you'll learn:</p>
                                <ul>
                                    <li><strong>Recurrent Neural Networks:</strong> Processing sequences step-by-step with hidden state memory</li>
                                    <li><strong>Vanishing Gradient Problem:</strong> Why vanilla RNNs fail on long sequences</li>
                                    <li><strong>LSTM Architecture:</strong> Gates that control information flow and enable long-term memory</li>
                                    <li><strong>GRU Simplification:</strong> A more efficient alternative with fewer parameters</li>
                                    <li><strong>Bidirectional RNNs:</strong> Capturing context from both past and future</li>
                                    <li><strong>Sequence Labeling:</strong> Part-of-speech tagging and named entity recognition with RNNs</li>
                                </ul>
                                <p style="margin-top: 1rem;"><a href="nlp-rnn-lstm-gru.html" class="btn btn-primary"><i class="fas fa-arrow-right me-2"></i>Continue to Part 7: RNNs, LSTMs & GRUs</a></p>
                            </div>
                        </div>

                        <div class="related-posts">
                            <h3><i class="fas fa-book-reader me-2"></i>Continue the NLP Series</h3>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 5: Statistical Language Models & N-grams</h5>
                                <p class="text-muted small mb-2">Understand probabilistic models of language and sequence prediction.</p>
                                <a href="nlp-statistical-language-models.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 7: RNNs, LSTMs & GRUs</h5>
                                <p class="text-muted small mb-2">Master sequential neural networks for language modeling.</p>
                                <a href="nlp-rnn-lstm-gru.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                            <div class="related-post-item">
                                <h5 class="mb-2">Part 8: Transformers & Attention Mechanism</h5>
                                <p class="text-muted small mb-2">Understand the architecture that revolutionized modern NLP.</p>
                                <a href="nlp-transformers-attention.html">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.</p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook"><i class="fab fa-facebook-f"></i></a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter"><i class="fab fa-twitter"></i></a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn"><i class="fab fa-linkedin-in"></i></a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube"><i class="fab fa-youtube"></i></a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram"><i class="fab fa-instagram"></i></a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest"><i class="fab fa-pinterest-p"></i></a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email"><i class="fas fa-envelope"></i></a>
                    </div>
                </div>
            </div>
            <hr class="bg-secondary">
            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small"><i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a></p>
                    <p class="small mt-3"><a href="/" class="text-light text-decoration-none">Home</a> | <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a></p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">Enjoying this content? ☕ <a href="https://buymeacoffee.com/itswzee" target="_blank" class="text-light" style="text-decoration: underline;">Keep me caffeinated</a> to keep the pixels flowing!</p>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top"><i class="fas fa-arrow-up"></i></button>
    <!-- Category Indicator -->
    <div id="categoryIndicator" class="category-indicator" title="Current Section">
        <i class="fas fa-tag"></i><span id="categoryText">Technology</span>
    </div>
    <script src="../../../js/cookie-consent.js"></script>
    <script src="../../../js/main.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <script>
        const themes = { 'prism-theme': 'Tomorrow Night', 'prism-default': 'Default', 'prism-dark': 'Dark', 'prism-twilight': 'Twilight', 'prism-okaidia': 'Okaidia', 'prism-solarizedlight': 'Solarized Light' };
        const savedTheme = localStorage.getItem('prism-theme') || 'prism-theme';
        function switchTheme(themeId) { Object.keys(themes).forEach(id => { const link = document.getElementById(id); if (link) link.disabled = true; }); const selectedLink = document.getElementById(themeId); if (selectedLink) { selectedLink.disabled = false; localStorage.setItem('prism-theme', themeId); } document.querySelectorAll('div.code-toolbar select').forEach(dropdown => { dropdown.value = themeId; }); setTimeout(() => Prism.highlightAll(), 10); }
        document.addEventListener('DOMContentLoaded', function() { switchTheme(savedTheme); });
        Prism.plugins.toolbar.registerButton('theme-switcher', function(env) { const select = document.createElement('select'); select.setAttribute('aria-label', 'Select code theme'); Object.keys(themes).forEach(themeId => { const option = document.createElement('option'); option.value = themeId; option.textContent = themes[themeId]; if (themeId === savedTheme) option.selected = true; select.appendChild(option); }); select.addEventListener('change', function(e) { switchTheme(e.target.value); }); return select; });
    </script>

    <script>
        document.addEventListener('DOMContentLoaded', function() { const scrollToTopBtn = document.getElementById('scrollToTop'); window.addEventListener('scroll', function() { if (window.scrollY > 300) { scrollToTopBtn.classList.add('show'); } else { scrollToTopBtn.classList.remove('show'); } }); scrollToTopBtn.addEventListener('click', function() { window.scrollTo({ top: 0, behavior: 'smooth' }); }); });
        function openNav() { document.getElementById('tocSidenav').classList.add('open'); document.getElementById('tocOverlay').classList.add('show'); document.body.style.overflow = 'hidden'; }
        function closeNav() { document.getElementById('tocSidenav').classList.remove('open'); document.getElementById('tocOverlay').classList.remove('show'); document.body.style.overflow = 'auto'; }
        document.addEventListener('keydown', function(e) { if (e.key === 'Escape') closeNav(); });
    </script>

            <!-- Scroll-to-Top and Category Indicator Script -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const scrollToTopBtn = document.getElementById('scrollToTop');
            const categoryIndicator = document.getElementById('categoryIndicator');
            const categoryText = document.getElementById('categoryText');
            
            // Auto-detect H2 sections in the article (works with or without id)
            const h2Elements = document.querySelectorAll('.blog-content h2');
            const sections = [];
            h2Elements.forEach(function(h2, index) {
                // Get text without icon
                let text = h2.textContent.trim().replace(/^\d+\.\s*/, '');
                // Truncate to 25 chars
                if (text.length > 25) text = text.substring(0, 22) + '...';
                sections.push({ element: h2, name: text });
            });
            
            // Fallback to article category if no sections found
            const articleCategory = categoryText ? categoryText.textContent : 'Article';
            
            // Show/hide button on scroll and update section
            window.addEventListener('scroll', function() {
                if (window.scrollY > 300) {
                    if (scrollToTopBtn) scrollToTopBtn.classList.add('show');
                    if (categoryIndicator) categoryIndicator.classList.add('show');
                } else {
                    if (scrollToTopBtn) scrollToTopBtn.classList.remove('show');
                    if (categoryIndicator) categoryIndicator.classList.remove('show');
                }
                
                // Update current section
                updateCurrentSection();
            });
            
            // Update section based on viewport position
            function updateCurrentSection() {
                if (!categoryText || sections.length === 0) return;
                
                let currentSection = articleCategory;
                
                for (let section of sections) {
                    const rect = section.element.getBoundingClientRect();
                    if (rect.top <= window.innerHeight / 2) {
                        currentSection = section.name;
                    }
                }
                
                categoryText.textContent = currentSection;
            }
            
            // Smooth scroll to top on click
            if (scrollToTopBtn) {
                scrollToTopBtn.addEventListener('click', function() {
                    window.scrollTo({ top: 0, behavior: 'smooth' });
                });
            }
        });
    </script>
</body>
</html>
