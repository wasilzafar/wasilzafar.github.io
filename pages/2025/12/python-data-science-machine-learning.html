<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="robots" content="index, archive" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Master machine learning with Scikit-learn. Learn classification, regression, clustering, pipelines, and model evaluation. Part 4 of Python Data Science Series." />
    <meta name="author" content="Wasil Zafar" />
    <meta name="keywords" content="Scikit-learn, Machine Learning, Python, Classification, Regression, Clustering, Model Evaluation, Pipelines, Data Science" />
    <meta property="og:title" content="Python Data Science Series Part 4: Machine Learning with Scikit-learn" />
    <meta property="og:description" content="Master machine learning with Scikit-learn. Build classification, regression, and clustering models with a consistent API." />
    <meta property="og:type" content="article" />
    <meta property="article:published_time" content="2025-12-27" />
    <meta property="article:author" content="Wasil Zafar" />
    <meta property="article:section" content="Technology" />
    
    <title>Python Data Science Series Part 4: Machine Learning with Scikit-learn - Wasil Zafar</title>

    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    
    <!-- Font Awesome Icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet" />

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@400;500;600;700&family=Poppins:wght@300;400;500;600;700&family=Playfair+Display:wght@600;700&display=swap" rel="stylesheet" />

    <!-- Custom Styles -->
    <link rel="stylesheet" href="../../../css/main.css" type="text/css" />

    <!-- Prism.js Syntax Highlighting -->
    <!-- Multiple themes for dynamic switching -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" id="prism-theme" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" id="prism-default" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-dark.min.css" id="prism-dark" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" id="prism-twilight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" id="prism-okaidia" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-solarizedlight.min.css" id="prism-solarizedlight" disabled />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.css" />

    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="../../../images/favicon_io/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../images/favicon_io/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../images/favicon_io/favicon-16x16.png">
    <link rel="manifest" href="../../../images/favicon_io/site.webmanifest">

    <!-- Google Consent Mode v2 -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        
        gtag('consent', 'default', {
            'ad_storage': 'denied',
            'ad_user_data': 'denied',
            'ad_personalization': 'denied',
            'analytics_storage': 'denied',
            'region': ['AT','BE','BG','HR','CY','CZ','DK','EE','FI','FR','DE','GR','HU','IE','IT','LV','LT','LU','MT','NL','PL','PT','RO','SK','SI','ES','SE']
        });
        
        gtag('consent', 'default', {
            'ad_storage': 'granted',
            'ad_user_data': 'granted',
            'ad_personalization': 'granted',
            'analytics_storage': 'granted'
        });
        
        gtag('set', 'url_passthrough', true);
    </script>

    <!-- Google Tag Manager -->
    <script>
        (function(w, d, s, l, i) {
            w[l] = w[l] || [];
            w[l].push({
                'gtm.start': new Date().getTime(),
                event: 'gtm.js'
            });
            var f = d.getElementsByTagName(s)[0],
                j = d.createElement(s),
                dl = l != 'dataLayer' ? '&l=' + l : '';
            j.async = true;
            j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
            f.parentNode.insertBefore(j, f);
        })(window, document, 'script', 'dataLayer', 'GTM-PBS8M2JR');
    </script>

    <style>
        /* Blog Post Specific Styles */
        .blog-hero {
            background: linear-gradient(135deg, var(--color-navy) 0%, var(--color-blue) 100%);
            color: white;
            padding: 80px 0;
        }

        .blog-header {
            margin-bottom: 2rem;
        }

        .blog-meta {
            font-size: 0.95rem;
            color: var(--color-teal);
            margin-bottom: 1rem;
        }

        .blog-meta span {
            margin-right: 1.5rem;
        }

        .blog-content {
            max-width: 900px;
            margin: 0 auto;
            font-size: 1.05rem;
            line-height: 1.8;
            color: #333;
        }

        .blog-content h2 {
            font-size: 1.8rem;
            font-weight: 700;
            margin-top: 2.5rem;
            margin-bottom: 1.5rem;
            color: var(--color-navy);
            border-bottom: 3px solid var(--color-teal);
            padding-bottom: 0.5rem;
        }

        .blog-content h3 {
            font-size: 1.3rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 1rem;
            color: var(--color-blue);
        }

        .blog-content p {
            margin-bottom: 1.2rem;
            text-align: justify;
        }

        .blog-content strong {
            color: var(--color-crimson);
        }

        .highlight-box {
            background: rgba(59, 151, 151, 0.1);
            border-left: 4px solid var(--color-teal);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 4px;
        }

        .experiment-card {
            background: #f8f9fa;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 1.5rem;
            margin-bottom: 1.5rem;
            transition: all 0.3s ease;
        }

        .experiment-card:hover {
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
            transform: translateY(-2px);
        }

        .experiment-card h4 {
            color: var(--color-crimson);
            font-weight: 700;
            margin-bottom: 0.5rem;
        }

        .experiment-card .card-meta {
            font-size: 0.9rem;
            color: var(--color-blue);
            margin-bottom: 1rem;
            font-style: italic;
        }

        .card-meta .badge {
            font-size: 0.85rem;
            font-weight: 600;
            padding: 0.5rem 1rem;
            margin-right: 0.5rem;
            letter-spacing: 0.3px;
        }

        .bg-teal {
            background-color: var(--color-teal) !important;
        }

        .bg-crimson {
            background-color: var(--color-crimson) !important;
        }

        .toc-box {
            background: #f8f9fa;
            border: 2px solid var(--color-teal);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 2rem 0;
        }

        .toc-box h3 {
            color: var(--color-navy);
            font-weight: 700;
            margin-bottom: 1rem;
            border: none;
            margin-top: 0;
        }

        .toc-box ol {
            margin-bottom: 0;
            padding-left: 1.5rem;
        }

        .toc-box li {
            margin-bottom: 0.5rem;
        }

        .toc-box a {
            color: var(--color-blue);
            text-decoration: none;
            transition: color 0.2s ease;
        }

        .toc-box a:hover {
            color: var(--color-crimson);
            text-decoration: underline;
        }

        .reading-time {
            display: inline-block;
            background: var(--color-crimson);
            color: white;
            padding: 0.3rem 0.8rem;
            border-radius: 4px;
            font-size: 0.9rem;
        }

        .back-link {
            display: inline-block;
            color: white;
            text-decoration: none;
            transition: all 0.3s ease;
            margin-bottom: 1rem;
            opacity: 0.9;
        }

        .back-link:hover {
            color: var(--color-teal);
            opacity: 1;
            transform: translateX(-5px);
        }

        .related-posts {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 2rem;
            margin-top: 3rem;
        }

        .related-posts h3 {
            color: var(--color-navy);
            margin-bottom: 1.5rem;
        }

        .related-post-item {
            padding: 1rem;
            border-left: 3px solid var(--color-teal);
            margin-bottom: 1rem;
            transition: all 0.3s ease;
        }

        .related-post-item:hover {
            background: white;
            border-left-color: var(--color-crimson);
        }

        .related-post-item a {
            color: var(--color-blue);
            text-decoration: none;
            font-weight: 600;
        }

        .related-post-item a:hover {
            color: var(--color-crimson);
        }

        /* Code Block Styles */
        pre[class*="language-"] {
            position: relative;
            margin: 1.5rem 0;
            padding-top: 3rem;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
        }

        code[class*="language-"] {
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.6;
        }

        /* Toolbar styling */
        div.code-toolbar > .toolbar {
            opacity: 1;
            display: flex;
            gap: 0.5rem;
        }

        div.code-toolbar > .toolbar > .toolbar-item > button {
            background: var(--color-teal);
            color: white;
            border: none;
            padding: 0.4rem 0.8rem;
            border-radius: 4px;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        div.code-toolbar > .toolbar > .toolbar-item > button:hover {
            background: var(--color-blue);
            transform: translateY(-1px);
        }

        div.code-toolbar > .toolbar > .toolbar-item > button:focus {
            outline: 2px solid var(--color-teal);
            outline-offset: 2px;
        }

        /* Theme switcher dropdown */
        div.code-toolbar > .toolbar > .toolbar-item > select {
            background: var(--color-navy);
            color: white;
            border: 1px solid var(--color-teal);
            padding: 0.4rem 0.8rem;
            border-radius: 4px;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.3s ease;
            outline: none;
        }

        div.code-toolbar > .toolbar > .toolbar-item > select:hover {
            background: var(--color-blue);
            border-color: var(--color-crimson);
        }

        div.code-toolbar > .toolbar > .toolbar-item > select:focus {
            outline: 2px solid var(--color-teal);
            outline-offset: 2px;
        }

        /* Style select options */
        div.code-toolbar > .toolbar > .toolbar-item > select option {
            background: var(--color-navy);
            color: white;
        }

        /* Scroll-to-Top Button */
        .scroll-to-top {
            position: fixed;
            bottom: 2rem;
            right: 2rem;
            width: 50px;
            height: 50px;
            background: var(--color-teal);
            color: white;
            border: none;
            border-radius: 50%;
            font-size: 1.2rem;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            opacity: 0;
            visibility: hidden;
            transition: all 0.3s ease;
            box-shadow: 0 4px 12px rgba(59, 151, 151, 0.3);
            z-index: 999;
        }

        .scroll-to-top.show {
            opacity: 1;
            visibility: visible;
        }

        .scroll-to-top:hover {
            background: var(--color-crimson);
            transform: translateY(-3px);
            box-shadow: 0 6px 16px rgba(191, 9, 47, 0.4);
        }

        .scroll-to-top:active {
            transform: translateY(-1px);
        }

        @media (max-width: 768px) {
            .scroll-to-top {
                bottom: 1rem;
                right: 1rem;
                width: 45px;
                height: 45px;
                font-size: 1rem;
            }
        }
    </style>
</head>
<body>
    <!-- Google Tag Manager (noscript) -->
    <noscript>
        <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PBS8M2JR" height="0" width="0" style="display:none;visibility:hidden"></iframe>
    </noscript>

    <!-- GDPR Cookie Consent Banner -->
    <div id="cookieBanner" class="light display-bottom" style="display: none;">
        <div id="closeIcon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512">
                <path fill="currentColor" d="M256 8C119 8 8 119 8 256s111 248 248 248 248-111 248-248S393 8 256 8zm121.6 313.1c4.7 4.7 4.7 12.3 0 17L338 377.6c-4.7 4.7-12.3 4.7-17 0L256 312l-65.1 65.6c-4.7 4.7-12.3 4.7-17 0L134.4 338c-4.7-4.7-4.7-12.3 0-17l65.6-65-65.6-65.1c-4.7-4.7-4.7-12.3 0-17l39.6-39.6c4.7-4.7 12.3-4.7 17 0l65 65.7 65.1-65.6c4.7-4.7 12.3-4.7 17 0l39.6 39.6c4.7 4.7 4.7 12.3 0 17L312 256l65.6 65.1z"></path>
            </svg>
        </div>
        
        <div class="content-wrap">
            <div class="msg-wrap">
                <div class="title-wrap">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="20" height="20">
                        <path fill="#3B9797" d="M510.52 255.82c-69.97-.85-126.47-57.69-126.47-127.86-70.17 0-127-56.49-127.86-126.45-27.26-4.14-55.13.3-79.72 12.82l-69.13 35.22a132.221 132.221 0 0 0-57.79 57.81l-35.1 68.88a132.645 132.645 0 0 0-12.82 80.95l12.08 76.27a132.521 132.521 0 0 0 37.16 70.37l54.64 54.64a132.036 132.036 0 0 0 70.37 37.16l76.27 12.15c27.51 4.36 55.7-.11 80.95-12.8l68.88-35.08a132.166 132.166 0 0 0 57.79-57.81l35.1-68.88c12.56-24.64 17.01-52.58 12.91-79.91zM176 368c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32zm32-160c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32zm160 128c-17.67 0-32-14.33-32-32s14.33-32 32-32 32 14.33 32 32-14.33 32-32 32z"></path>
                    </svg>
                    <h4 style="margin: 0; font-size: 18px; color: var(--color-navy); font-weight: 700;">Cookie Consent</h4>
                </div>
                <p style="font-size: 14px; line-height: 1.6; color: var(--color-navy); margin-bottom: 15px;">
                    We use cookies to enhance your browsing experience, serve personalized content, and analyze our traffic. 
                    By clicking "Accept All", you consent to our use of cookies. See our 
                    <a href="/privacy-policy.html" style="color: var(--color-teal); border-bottom: 1px dotted var(--color-teal);">Privacy Policy</a> 
                    for more information.
                </p>
                
                <div id="cookieSettings" style="display: none;">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" width="14" height="14">
                        <path fill="currentColor" d="M487.4 315.7l-42.6-24.6c4.3-23.2 4.3-47 0-70.2l42.6-24.6c4.9-2.8 7.1-8.6 5.5-14-11.1-35.6-30-67.8-54.7-94.6-3.8-4.1-10-5.1-14.8-2.3L380.8 110c-17.9-15.4-38.5-27.3-60.8-35.1V25.8c0-5.6-3.9-10.5-9.4-11.7-36.7-8.2-74.3-7.8-109.2 0-5.5 1.2-9.4 6.1-9.4 11.7V75c-22.2 7.9-42.8 19.8-60.8 35.1L88.7 85.5c-4.9-2.8-11-1.9-14.8 2.3-24.7 26.7-43.6 58.9-54.7 94.6-1.7 5.4.6 11.2 5.5 14L67.3 221c-4.3 23.2-4.3 47 0 70.2l-42.6 24.6c-4.9 2.8-7.1 8.6-5.5 14 11.1 35.6 30 67.8 54.7 94.6 3.8 4.1 10 5.1 14.8 2.3l42.6-24.6c17.9 15.4 38.5 27.3 60.8 35.1v49.2c0 5.6 3.9 10.5 9.4 11.7 36.7 8.2 74.3 7.8 109.2 0 5.5-1.2 9.4-6.1 9.4-11.7v-49.2c22.2-7.9 42.8-19.8 60.8-35.1l42.6 24.6c4.9 2.8 11 1.9 14.8-2.3 24.7-26.7 43.6-58.9 54.7-94.6 1.5-5.5-.7-11.3-5.6-14.1zM256 336c-44.1 0-80-35.9-80-80s35.9-80 80-80 80 35.9 80 80-35.9 80-80 80z"></path>
                    </svg>
                    <span style="margin-left: 5px; font-size: 12px; font-weight: 600; color: var(--color-navy);">Customize Settings</span>
                </div>
                
                <div id="cookieTypes" style="display: none; margin-top: 15px; padding-top: 15px; border-top: 1px solid rgba(59, 151, 151, 0.2);">
                    <h5 style="font-size: 12px; font-weight: 700; color: var(--color-navy); margin-bottom: 10px; text-transform: uppercase;">Cookie Preferences</h5>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" checked disabled style="margin-top: 2px; margin-right: 8px; cursor: not-allowed;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Essential Cookies (Required)</strong>
                                <span style="font-size: 12px; color: #666;">Necessary for the website to function properly.</span>
                            </div>
                        </label>
                    </div>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" id="analyticsCookies" checked style="margin-top: 2px; margin-right: 8px;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Analytics Cookies</strong>
                                <span style="font-size: 12px; color: #666;">Help us understand how you interact with the website.</span>
                            </div>
                        </label>
                    </div>
                    
                    <div style="margin-bottom: 12px;">
                        <label style="display: flex; align-items: start; cursor: pointer;">
                            <input type="checkbox" id="marketingCookies" style="margin-top: 2px; margin-right: 8px;">
                            <div>
                                <strong style="font-size: 13px; color: var(--color-navy); display: block; margin-bottom: 2px;">Marketing Cookies</strong>
                                <span style="font-size: 12px; color: #666;">Used to deliver relevant advertisements.</span>
                            </div>
                        </label>
                    </div>
                </div>
            </div>
            
            <div class="btn-wrap">
                <button id="cookieAccept" style="background: var(--color-teal); color: white; font-weight: 600;">Accept All</button>
                <button id="cookieReject" style="background: transparent; color: var(--color-navy); border: 2px solid var(--color-teal); font-weight: 600;">Reject All</button>
                <button id="cookieSave" style="background: var(--color-blue); color: white; font-weight: 600; display: none;">Save Preferences</button>
            </div>
        </div>
    </div>

    <!-- Navigation Bar -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark shadow-sm">
        <div class="container-fluid">
            <a class="navbar-brand fw-bold" href="/">
                <span class="gradient-text">Wasil Zafar</span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="/">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#about">About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#skills">Skills</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#certifications">Certifications</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/#interests">Interests</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Hero Section -->
    <section class="blog-hero">
        <div class="container py-5">
            <div class="blog-header">
                <a href="/pages/categories/technology.html" class="back-link">
                    <i class="fas fa-arrow-left me-2"></i>Back to Technology
                </a>
                <h1 class="display-4 fw-bold mb-3">Python Data Science Series Part 4: Machine Learning with Scikit-learn</h1>
                <div class="blog-meta">
                    <span><i class="fas fa-calendar me-2"></i>December 27, 2025</span>
                    <span><i class="fas fa-user me-2"></i>Wasil Zafar</span>
                    <span class="reading-time"><i class="fas fa-clock me-1"></i>35 min read</span>
                </div>
                <p class="lead">Complete your data science journey by mastering Scikit-learn—Python's premier machine learning library. Learn classification, regression, clustering, pipelines, and model evaluation with a consistent, intuitive API.</p>
            </div>
        </div>
    </section>

    <!-- Main Content -->
    <section class="py-5">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 mx-auto">
                    
                    <!-- Table of Contents -->
                    <div class="toc-box mb-5">
                        <h3><i class="fas fa-list me-2"></i>Table of Contents</h3>
                        <ol>
                            <li><a href="#introduction">Introduction to Scikit-learn</a></li>
                            <li><a href="#workflow">The ML Workflow</a></li>
                            <li><a href="#datasets">Built-in Datasets & Examples</a>
                                <ul style="list-style-type: lower-alpha; margin-top: 0.5rem;">
                                    <li><a href="#iris-dataset">Iris Dataset (Classification)</a></li>
                                    <li><a href="#wine-dataset">Wine Dataset (Classification)</a></li>
                                    <li><a href="#digits-dataset">Digits Dataset (Image Classification)</a></li>
                                    <li><a href="#breast-cancer-dataset">Breast Cancer Dataset (Binary Classification)</a></li>
                                    <li><a href="#diabetes-dataset">Diabetes Dataset (Regression)</a></li>
                                    <li><a href="#california-housing-dataset">California Housing Dataset (Regression)</a></li>
                                </ul>
                            </li>
                            <li><a href="#preprocessing">Data Preprocessing</a></li>
                            <li><a href="#classification">Classification Models</a></li>
                            <li><a href="#regression">Regression Models</a></li>
                            <li><a href="#clustering">Clustering</a></li>
                            <li><a href="#evaluation">Model Evaluation</a></li>
                            <li><a href="#pipelines">Pipelines & Automation</a></li>
                            <li><a href="#tuning">Hyperparameter Tuning</a></li>
                            <li><a href="#best-practices">Best Practices & Summary</a></li>
                        </ol>
                    </div>

                    <!-- Introduction -->
                    <div id="introduction" class="blog-content">
                        <h2><i class="fas fa-robot me-2 text-teal"></i>Introduction to Scikit-learn</h2>
                        
                        <div class="highlight-box" style="background: rgba(191, 9, 47, 0.1); border-left-color: var(--color-crimson);">
                            <i class="fas fa-tools me-2"></i>
                            <strong>Prerequisites:</strong> Before running the code examples in this tutorial, make sure you have Python and Jupyter notebooks properly set up. If you haven't configured your development environment yet, check out our <a href="python-setup-notebooks-guide.html" style="color: var(--color-crimson); font-weight: 600;">complete setup guide for VS Code, PyCharm, Jupyter, and Colab</a>.
                        </div>
                        
                        <p>You've learned NumPy (arrays), Pandas (data manipulation), and visualization. Now it's time for <strong>machine learning</strong>—using data to make predictions and discover patterns.</p>

                        <div class="highlight-box">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Why Scikit-learn:</strong> Scikit-learn provides a simple, consistent API for hundreds of ML algorithms. Whether you're doing classification, regression, or clustering, the workflow is always: <code>fit()</code> to train, <code>predict()</code> to infer, <code>score()</code> to evaluate.
                        </div>

                        <h3>Key Features</h3>
                        <ul>
                            <li><strong>Consistent API:</strong> All models follow the same estimator interface</li>
                            <li><strong>Comprehensive:</strong> Classification, regression, clustering, dimensionality reduction</li>
                            <li><strong>Preprocessing tools:</strong> Scaling, encoding, feature selection</li>
                            <li><strong>Model evaluation:</strong> Cross-validation, metrics, confusion matrices</li>
                            <li><strong>Pipelines:</strong> Chain preprocessing and modeling steps</li>
                            <li><strong>Well-documented:</strong> Excellent examples and user guide</li>
                        </ul>

                        <pre><code class="language-bash"># Installation
pip install scikit-learn

# Import convention
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np</code></pre>
                    </div>

                    <!-- Workflow -->
                    <div id="workflow" class="blog-content mt-5">
                        <h2><i class="fas fa-project-diagram me-2 text-teal"></i>The ML Workflow</h2>
                        
                        <p>Every machine learning project follows these steps:</p>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">Standard ML Workflow</span>
                            </div>
                            <div class="card-content">
                                <ol>
                                    <li><strong>Load data:</strong> Import from CSV, database, or API</li>
                                    <li><strong>Explore:</strong> Visualize distributions, check for missing values</li>
                                    <li><strong>Split:</strong> Separate into training and test sets</li>
                                    <li><strong>Preprocess:</strong> Scale features, encode categoricals</li>
                                    <li><strong>Choose model:</strong> Select algorithm based on problem type</li>
                                    <li><strong>Train:</strong> Fit model on training data</li>
                                    <li><strong>Evaluate:</strong> Test on held-out data</li>
                                    <li><strong>Tune:</strong> Optimize hyperparameters</li>
                                    <li><strong>Deploy:</strong> Save model for production use</li>
                                </ol>
                            </div>
                        </div>

                        <div class="highlight-box">
                            <i class="fas fa-database"></i>
                            <strong>Built-in Datasets:</strong> Scikit-learn includes several classic datasets perfect for learning and experimentation. In the next section, we'll explore each dataset with complete workflow examples demonstrating every step from loading to evaluation.
                        </div>
                    </div>

                    <!-- Built-in Datasets -->
                    <div id="datasets" class="blog-content mt-5">
                        <h2><i class="fas fa-database me-2 text-teal"></i>Built-in Datasets & Complete Workflow Examples</h2>
                        
                        <p>Scikit-learn provides several datasets for learning and benchmarking. Let's explore each with comprehensive examples following the complete ML workflow.</p>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">Available Datasets</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>Classification:</strong> Iris, Wine, Digits, Breast Cancer</li>
                                    <li><strong>Regression:</strong> Diabetes, Boston Housing, California Housing</li>
                                    <li><strong>Toy Datasets:</strong> Perfect for quick experiments and learning</li>
                                    <li><strong>Real-world Data:</strong> Based on actual research and applications</li>
                                </ul>
                            </div>
                        </div>

                        <!-- Iris Dataset -->
                        <h3 id="iris-dataset"><i class="fas fa-flower me-2 text-crimson"></i>1. Iris Dataset (Multi-class Classification)</h3>
                        
                        <div class="highlight-box">
                            <i class="fas fa-info-circle"></i>
                            <strong>About Iris:</strong> Classic dataset with 150 samples of iris flowers. Features include sepal length, sepal width, petal length, and petal width. Target: 3 species (setosa, versicolor, virginica). Perfect for learning classification.
                        </div>

                        <pre><code class="language-python"># Import necessary libraries
import numpy as np  # For numerical operations
import pandas as pd  # For data manipulation
import matplotlib.pyplot as plt  # For plotting
from sklearn.datasets import load_iris  # Load built-in Iris dataset
from sklearn.model_selection import train_test_split, cross_val_score  # For data splitting and validation
from sklearn.preprocessing import StandardScaler  # For feature scaling
from sklearn.linear_model import LogisticRegression  # Linear classifier
from sklearn.ensemble import RandomForestClassifier  # Tree-based ensemble classifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # Evaluation metrics
import seaborn as sns  # For advanced visualization

# 1. LOAD DATA
# load_iris() returns a Bunch object (dict-like) containing:
#   - data: feature matrix (150 samples x 4 features)
#   - target: class labels (0, 1, 2 for setosa, versicolor, virginica)
#   - feature_names: names of the 4 features
#   - target_names: names of the 3 species
iris = load_iris()
X, y = iris.data, iris.target  # X = features (150x4), y = labels (150,)

# Display dataset information
print(f"Dataset shape: {X.shape}")  # Output: (150, 4) - 150 samples, 4 features
print(f"Feature names: {iris.feature_names}")  # sepal length/width, petal length/width
print(f"Target names: {iris.target_names}")  # setosa, versicolor, virginica
print(f"Sample distribution: {np.bincount(y)}")  # Count samples per class - Output: [50 50 50] (balanced)</code></pre>

                        <pre><code class="language-python"># Import libraries for data exploration
import pandas as pd  # For DataFrame operations
import matplotlib.pyplot as plt  # For visualization
import seaborn as sns  # For enhanced plots
from sklearn.datasets import load_iris  # Load dataset

# 2. EXPLORE DATA
iris = load_iris()
X, y = iris.data, iris.target

# Create DataFrame for easy exploration and analysis
# pd.DataFrame() converts NumPy array to tabular format with column names
df = pd.DataFrame(X, columns=iris.feature_names)
# Add species names by mapping numeric labels (0,1,2) to text labels
df['species'] = iris.target_names[y]  # e.g., 0 -> 'setosa'

# Display first 5 rows to see data structure
print(df.head())  # Shows sample data with feature values and species
# Statistical summary: count, mean, std, min, 25%, 50%, 75%, max
print(df.describe())  # Helps identify feature ranges and distributions
# Count samples per species - should be 50 each (balanced dataset)
print(df['species'].value_counts())

# Visualize feature distributions to understand data patterns
plt.figure(figsize=(12, 4))  # Create figure 12 inches wide, 4 tall
for i in range(4):  # Loop through 4 features
    plt.subplot(1, 4, i+1)  # Create 1 row, 4 columns of subplots
    # Create overlapping histograms for each species
    # X[y==0, i] gets feature i values for species 0, etc.
    plt.hist([X[y==0, i], X[y==1, i], X[y==2, i]], 
             label=iris.target_names, alpha=0.7)  # alpha=0.7 for transparency
    plt.xlabel(iris.feature_names[i])  # Label x-axis with feature name
    plt.ylabel('Frequency')  # Count of samples in each bin
    plt.legend()  # Show which color represents which species
plt.tight_layout()  # Adjust spacing to prevent overlap
plt.show()  # Display the plot</code></pre>

                        <pre><code class="language-python"># Import required libraries
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split  # For splitting data
from sklearn.preprocessing import StandardScaler  # For feature normalization
from sklearn.linear_model import LogisticRegression  # Linear classification model
from sklearn.ensemble import RandomForestClassifier  # Ensemble tree model
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 3. SPLIT DATA into training and testing sets
iris = load_iris()
X, y = iris.data, iris.target

# train_test_split() randomly divides data into train/test sets
# test_size=0.2: Use 20% for testing, 80% for training
# random_state=42: Set seed for reproducibility (same split every time)
# stratify=y: Maintain class proportions in both sets (33% of each species)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training set: {X_train.shape}")  # (120, 4) - 80% of 150 samples
print(f"Test set: {X_test.shape}")  # (30, 4) - 20% of 150 samples

# 4. PREPROCESS: Scale features to mean=0, std=1
# Scaling is crucial for distance-based algorithms (e.g., Logistic Regression, SVM)
scaler = StandardScaler()  # Create scaler object
# fit_transform(): Learn mean/std from training data AND transform it
X_train_scaled = scaler.fit_transform(X_train)
# transform(): Apply same scaling (using training mean/std) to test data
# NEVER fit on test data - this would cause data leakage!
X_test_scaled = scaler.transform(X_test)

# 5. TRAIN MODELS on the training data
# Logistic Regression (linear decision boundaries)
# max_iter=200: Maximum optimization iterations
# random_state=42: For reproducibility in stochastic processes
log_reg = LogisticRegression(max_iter=200, random_state=42)
log_reg.fit(X_train_scaled, y_train)  # Learn weights from scaled training data

# Random Forest (ensemble of decision trees)
# n_estimators=100: Build 100 decision trees and average their predictions
# Tree-based models are scale-invariant (don't need scaled features)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)  # Train on original (unscaled) data

# 6. EVALUATE models on test data (unseen data)
# predict(): Generate predictions for test samples
y_pred_lr = log_reg.predict(X_test_scaled)  # Use scaled test data
y_pred_rf = rf.predict(X_test)  # Use original test data

# accuracy_score(): Fraction of correct predictions
print(f"\nLogistic Regression Accuracy: {accuracy_score(y_test, y_pred_lr):.3f}")
print(f"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.3f}")

# classification_report(): Precision, recall, F1-score for each class
# Provides detailed per-class performance metrics
print("\nLogistic Regression Classification Report:")
print(classification_report(y_test, y_pred_lr, target_names=iris.target_names))</code></pre>

                        <pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns  # Advanced visualization library built on matplotlib
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix  # For error analysis

# 7. VISUALIZE RESULTS with a confusion matrix
iris = load_iris()
X, y = iris.data, iris.target
# Split data with same parameters to ensure reproducibility
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)  # Learn from training data
y_pred_rf = rf.predict(X_test)  # Make predictions on test data

# confusion_matrix(): Create matrix showing actual vs predicted classes
# Rows = actual classes, Columns = predicted classes
# Diagonal elements = correct predictions, off-diagonal = errors
cm = confusion_matrix(y_test, y_pred_rf)

# Visualize confusion matrix as a heatmap
plt.figure(figsize=(8, 6))  # Set figure size
# sns.heatmap(): Display matrix with color-coded cells
# annot=True: Show numbers in each cell
# fmt='d': Format numbers as integers (not decimals)
# cmap='Blues': Use blue color scheme (darker = higher values)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=iris.target_names,  # Label columns with species names
            yticklabels=iris.target_names)  # Label rows with species names
plt.xlabel('Predicted')  # What the model predicted
plt.ylabel('Actual')  # What the true class was
plt.title('Iris Classification Confusion Matrix')  # Descriptive title
plt.show()  # Display the plot

# How to read: If cell (setosa, versicolor) = 2, means 2 setosa samples
# were incorrectly classified as versicolor</code></pre>

                        <pre><code class="language-python">import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# 8. FEATURE IMPORTANCE - Which features are most useful for predictions?
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest (tree-based models provide feature importance)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# feature_importances_: Array of importance scores (sum to 1.0)
# Higher score = feature contributes more to accurate predictions
# Based on how much each feature decreases impurity (Gini) across trees
importances = rf.feature_importances_

# np.argsort(): Get indices that would sort array in ascending order
# [::-1] reverses to get descending order (most important first)
indices = np.argsort(importances)[::-1]

# Create bar plot of feature importance
plt.figure(figsize=(10, 6))
# Plot bars in order of importance
plt.bar(range(X.shape[1]), importances[indices])
# Label x-axis with feature names in sorted order, rotated 45° for readability
plt.xticks(range(X.shape[1]), [iris.feature_names[i] for i in indices], rotation=45)
plt.xlabel('Feature')  # X-axis label
plt.ylabel('Importance')  # Y-axis label (0 to ~0.5 for Iris dataset)
plt.title('Feature Importance for Iris Classification')
plt.tight_layout()  # Prevent label cutoff
plt.show()

# Print ranked list of features with importance scores
print("Feature ranking:")
for i in range(X.shape[1]):
    print(f"{i+1}. {iris.feature_names[indices[i]]}: {importances[indices[i]]:.3f}")
# Typically petal width and petal length are most important for Iris</code></pre>

                        <!-- Wine Dataset -->
                        <h3 id="wine-dataset" class="mt-5"><i class="fas fa-wine-glass me-2 text-crimson"></i>2. Wine Dataset (Multi-class Classification)</h3>
                        
                        <div class="highlight-box">
                            <i class="fas fa-info-circle"></i>
                            <strong>About Wine:</strong> Chemical analysis of 178 wine samples from Italy. 13 features (alcohol, acidity, phenols, etc.). Target: 3 wine types. Great for classification with multiple continuous features.
                        </div>

                        <pre><code class="language-python"># Import necessary libraries
from sklearn.datasets import load_wine  # Wine quality dataset
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler  # For feature scaling
from sklearn.svm import SVC  # Support Vector Machine classifier
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd  # For data manipulation
import numpy as np  # For numerical operations

# 1. LOAD & EXPLORE
# load_wine() returns chemical analysis of 178 wine samples
# Features include alcohol content, acidity, phenols, color intensity, etc.
wine = load_wine()
X, y = wine.data, wine.target  # X = 13 chemical features, y = wine class (0, 1, 2)

print(f"Dataset shape: {X.shape}")  # (178, 13) - 178 samples, 13 features
print(f"Features: {len(wine.feature_names)}")  # 13 chemical properties
print(f"Classes: {wine.target_names}")  # class_0, class_1, class_2 (wine cultivars)
print(f"Class distribution: {np.bincount(y)}")  # Samples per class - may be imbalanced

# Create DataFrame for easier exploration
df_wine = pd.DataFrame(X, columns=wine.feature_names)
df_wine['wine_type'] = y  # Add target column

print("\nFirst few rows:")  # Preview data structure
print(df_wine.head())
print("\nStatistics:")  # Mean, std, min, max for each feature
print(df_wine.describe())  # Note: features have different scales (0.74 to 1680)</code></pre>

                        <pre><code class="language-python"># Import required libraries
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler  # For feature normalization
from sklearn.svm import SVC  # Support Vector Machine
from sklearn.ensemble import GradientBoostingClassifier  # Boosting ensemble
from sklearn.metrics import accuracy_score, classification_report
import numpy as np

# 2. SPLIT & PREPROCESS data
wine = load_wine()
X, y = wine.data, wine.target

# train_test_split(): Randomly divide data
# test_size=0.25: Use 25% for testing (higher than standard 20% due to small dataset)
# random_state=42: Reproducible split
# stratify=y: Maintain class proportions in train/test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=42, stratify=y
)

# Scale features to mean=0, std=1 (critical for SVM performance)
# Wine features have vastly different scales (alcohol ~10-15, proline ~200-1600)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Learn scaling from training data
X_test_scaled = scaler.transform(X_test)  # Apply same scaling to test data

# 3. TRAIN MULTIPLE MODELS for comparison
# SVM with RBF (Radial Basis Function) kernel
# kernel='rbf': Non-linear decision boundaries
# C=10: High regularization penalty (tighter fit to training data)
# gamma='scale': Kernel coefficient = 1 / (n_features * X.var())
#   See: https://scikit-learn.org/stable/modules/svm.html
svm = SVC(kernel='rbf', C=10, gamma='scale', random_state=42)
svm.fit(X_train_scaled, y_train)  # Train on scaled features

# Gradient Boosting: Sequential ensemble of decision trees
# n_estimators=100: Build 100 trees (each learns from previous tree's errors)
#   See: https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting
# Tree models don't need scaled features (they only use rank order)
gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
gb.fit(X_train, y_train)  # Train on original unscaled data

# 4. EVALUATE both models on test set
y_pred_svm = svm.predict(X_test_scaled)  # SVM needs scaled features
y_pred_gb = gb.predict(X_test)  # GB uses original features

# Compare accuracy scores
print(f"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.3f}")
print(f"Gradient Boosting Accuracy: {accuracy_score(y_test, y_pred_gb):.3f}")

# Detailed per-class metrics (precision, recall, F1-score)
print("\nGradient Boosting Report:")
print(classification_report(y_test, y_pred_gb, target_names=wine.target_names))</code></pre>

                        <pre><code class="language-python"># Import libraries for cross-validation workflow
from sklearn.datasets import load_wine
from sklearn.model_selection import cross_val_score  # For k-fold cross-validation
from sklearn.preprocessing import StandardScaler  # Feature scaling
from sklearn.svm import SVC  # Classifier
from sklearn.pipeline import Pipeline  # Chain preprocessing + model together
import numpy as np

# 5. CROSS-VALIDATION with Pipeline
# Pipeline ensures scaling is done correctly within each CV fold
# This prevents data leakage (test data influencing training)
wine = load_wine()
X, y = wine.data, wine.target

# Create Pipeline: preprocessing step + model step
# Pipeline chains operations: data flows scaler → SVM
# See: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # Step 1: Scale features
    ('svm', SVC(kernel='rbf', C=10, random_state=42))  # Step 2: Train SVM
])

# cross_val_score(): Perform k-fold cross-validation
# cv=5: Split data into 5 folds
#   - Train on 4 folds, test on 1 fold
#   - Repeat 5 times (each fold used as test once)
#   - Returns 5 accuracy scores
# scoring='accuracy': Metric to evaluate (could be 'f1', 'precision', etc.)
# Pipeline ensures each fold is scaled independently (no data leakage)
# See: https://scikit-learn.org/stable/modules/cross_validation.html
scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')

# Display results
print(f"Cross-validation scores: {scores}")  # 5 individual fold scores
# Mean ± 2*std gives 95% confidence interval estimate
print(f"Mean accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
# Example output: "Mean accuracy: 0.978 (+/- 0.034)"</code></pre>

                        <!-- Digits Dataset -->
                        <h3 id="digits-dataset" class="mt-5"><i class="fas fa-hashtag me-2 text-crimson"></i>3. Digits Dataset (Image Classification)</h3>
                        
                        <div class="highlight-box">
                            <i class="fas fa-info-circle"></i>
                            <strong>About Digits:</strong> 1,797 images of handwritten digits (0-9), each 8x8 pixels (64 features). Perfect for learning image classification and dimensionality reduction techniques.
                        </div>

                        <pre><code class="language-python"># Import libraries for visualization and data loading
import matplotlib.pyplot as plt  # For plotting
from sklearn.datasets import load_digits  # Handwritten digits dataset
import numpy as np  # For numerical operations

# 1. LOAD & VISUALIZE handwritten digits
# load_digits() returns 1,797 images of digits 0-9
# Each image is 8x8 pixels, flattened to 64-dimensional vector
digits = load_digits()
X, y = digits.data, digits.target  # X = 64 features (pixel intensities), y = digit label (0-9)

print(f"Dataset shape: {X.shape}")  # (1797, 64) - 1797 images, 64 pixels each
print(f"Image shape: {digits.images.shape}")  # (1797, 8, 8) - original 2D format
print(f"Classes: 0-9 (10 classes)")  # 10 possible digit labels
print(f"Samples per class: {np.bincount(y)}")  # Distribution (~180 samples per digit)

# Visualize sample digits to understand the data
# Create 2 rows × 5 columns = 10 subplots
fig, axes = plt.subplots(2, 5, figsize=(12, 5))
for i, ax in enumerate(axes.flat):  # axes.flat iterates over all subplots
    # imshow(): Display 2D array as image
    # cmap='gray': Use grayscale colormap (0=black, 16=white)
    ax.imshow(digits.images[i], cmap='gray')
    ax.set_title(f"Label: {digits.target[i]}")  # Show true digit label
    ax.axis('off')  # Hide axis ticks and labels
plt.tight_layout()  # Adjust spacing between subplots
plt.show()  # Display the plot</code></pre>

                        <pre><code class="language-python"># Import libraries for neural network training
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler  # For feature normalization
from sklearn.neural_network import MLPClassifier  # Multi-Layer Perceptron (neural network)
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 2. SPLIT & TRAIN with neural network
digits = load_digits()
X, y = digits.data, digits.target  # X = 64 pixel intensities, y = digit label (0-9)

# Split data: 80% training, 20% testing
# stratify=y: Ensure balanced digit distribution in both sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features to improve neural network convergence
# Neural networks learn faster when features are standardized
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training data
X_test_scaled = scaler.transform(X_test)  # Transform test data (using training stats)

# MLPClassifier: Multi-Layer Perceptron (feedforward neural network)
# hidden_layer_sizes=(100, 50): Architecture with 2 hidden layers
#   - Layer 1: 100 neurons (fully connected to 64 input pixels)
#   - Layer 2: 50 neurons (fully connected to Layer 1)
#   - Output: 10 neurons (one per digit class)
# max_iter=500: Maximum training epochs (iterations through dataset)
# See: https://scikit-learn.org/stable/modules/neural_networks_supervised.html
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
mlp.fit(X_train_scaled, y_train)  # Train network using backpropagation

# Predict digit labels for test images
y_pred = mlp.predict(X_test_scaled)

# Evaluate performance
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")  # Overall correctness
print("\nClassification Report:")  # Per-digit precision, recall, F1-score
print(classification_report(y_test, y_pred))  # Shows performance for each digit 0-9</code></pre>

                        <pre><code class="language-python"># Import visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns  # For advanced heatmaps
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix  # For error pattern analysis

# 3. CONFUSION MATRIX VISUALIZATION - See where model makes mistakes
digits = load_digits()
X, y = digits.data, digits.target

# Reproduce same train/test split as previous example
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train neural network
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
mlp.fit(X_train_scaled, y_train)
y_pred = mlp.predict(X_test_scaled)  # Get predictions

# confusion_matrix(): 10x10 matrix showing actual vs predicted digits
# Rows = true digit, Columns = predicted digit
# Diagonal = correct predictions, off-diagonal = errors
cm = confusion_matrix(y_test, y_pred)

# Visualize as heatmap
plt.figure(figsize=(10, 8))  # Large figure for 10x10 matrix
# annot=True: Show count in each cell
# fmt='d': Display as integers
# cmap='Blues': Blue color scheme (darker = more samples)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Digit')  # What model predicted
plt.ylabel('True Digit')  # Actual digit in test set
plt.title('Digit Classification Confusion Matrix')
plt.show()

# How to read: If cell (8, 3) = 5, means 5 images of digit 8 were
# incorrectly classified as digit 3 (common mistake due to similar shapes)</code></pre>

                        <pre><code class="language-python"># Import visualization libraries
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
import numpy as np

# 4. VISUALIZE PREDICTIONS - See model's predictions on actual images
digits = load_digits()
X, y = digits.data, digits.target

# Reproduce same train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train neural network
mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
mlp.fit(X_train_scaled, y_train)
y_pred = mlp.predict(X_test_scaled)  # Predict all test samples

# Show first 18 test images (3 rows × 6 columns) with predictions
fig, axes = plt.subplots(3, 6, figsize=(15, 8))
for i, ax in enumerate(axes.flat):  # Loop through 18 subplots
    # X_test[i] is 64-element array; reshape to 8x8 for display
    ax.imshow(X_test[i].reshape(8, 8), cmap='gray')
    # Show true label vs predicted label
    ax.set_title(f"True: {y_test[i]}\nPred: {y_pred[i]}")
    ax.axis('off')  # Hide axis ticks
    
    # Highlight incorrect predictions in red for easy spotting
    if y_test[i] != y_pred[i]:
        # Make incorrect predictions stand out visually
        ax.set_title(f"True: {y_test[i]}\nPred: {y_pred[i]}", 
                    color='red', fontweight='bold')
plt.tight_layout()  # Prevent title overlap
plt.show()

# This visualization helps identify which digits the model confuses
# e.g., 8 vs 3, 5 vs 3, 1 vs 7 are common errors</code></pre>

                        <!-- Breast Cancer Dataset -->
                        <h3 id="breast-cancer-dataset" class="mt-5"><i class="fas fa-heartbeat me-2 text-crimson"></i>4. Breast Cancer Dataset (Binary Classification)</h3>
                        
                        <div class="highlight-box">
                            <i class="fas fa-info-circle"></i>
                            <strong>About Breast Cancer:</strong> 569 samples with 30 features computed from breast mass images. Binary classification: malignant (0) or benign (1). Real medical data—demonstrates importance of precision/recall.
                        </div>

                        <pre><code class="language-python"># Import libraries for medical dataset analysis
from sklearn.datasets import load_breast_cancer  # Real medical diagnostic data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import pandas as pd  # For data manipulation
import numpy as np  # For numerical operations

# 1. LOAD & EXPLORE breast cancer diagnostic data
# This dataset contains features computed from digitized images of breast mass
# Binary classification: malignant (cancerous) vs benign (non-cancerous)
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target  # X = 30 features, y = diagnosis (0=malignant, 1=benign)

print(f"Dataset shape: {X.shape}")  # (569, 30) - 569 samples, 30 features
print(f"Features: {cancer.feature_names[:5]}... (30 total)")  # radius, texture, perimeter, area, smoothness, etc.
print(f"Classes: {cancer.target_names}")  # ['malignant' 'benign']
print(f"Class distribution: {np.bincount(y)}")  # Count of each class
print(f"Malignant (0): {(y==0).sum()}, Benign (1): {(y==1).sum()}")  # Show imbalance if any

# Create DataFrame for statistical analysis
# Features include mean, std error, and worst values for 10 measurements
df_cancer = pd.DataFrame(X, columns=cancer.feature_names)
print("\nFeature statistics:")  # Mean, std, min, max for all features
print(df_cancer.describe())  # Note: Features have very different scales (0.1 to 3000)</code></pre>

                        <pre><code class="language-python"># Import libraries for medical classification
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression  # Linear classifier
from sklearn.ensemble import RandomForestClassifier  # Tree ensemble
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
import numpy as np

# 2. SPLIT, SCALE & TRAIN multiple models
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# Split: 80% train, 20% test, maintaining class balance
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale features (critical for logistic regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression
# max_iter=10000: High iteration limit (default 100 may not converge)
# See: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
lr = LogisticRegression(max_iter=10000, random_state=42)
lr.fit(X_train_scaled, y_train)  # Train on scaled data

# Train Random Forest (comparison model)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)  # Trees don't need scaling

# 3. EVALUATE with MULTIPLE METRICS (critical for medical applications)
# Medical data requires careful evaluation beyond just accuracy
y_pred_lr = lr.predict(X_test_scaled)  # Binary predictions (0 or 1)
y_pred_rf = rf.predict(X_test)
y_proba_lr = lr.predict_proba(X_test_scaled)[:, 1]  # Probability of benign (class 1)

print("Logistic Regression:")
print(f"  Accuracy: {accuracy_score(y_test, y_pred_lr):.3f}")  # Overall correctness
print(f"  Precision: {precision_score(y_test, y_pred_lr):.3f}")  # Of predicted benign, how many are actually benign?
print(f"  Recall: {recall_score(y_test, y_pred_lr):.3f}")  # Of actual benign, how many did we catch?
print(f"  F1 Score: {f1_score(y_test, y_pred_lr):.3f}")  # Harmonic mean of precision & recall
print(f"  ROC-AUC: {roc_auc_score(y_test, y_proba_lr):.3f}")  # Area under ROC curve (0.5-1.0)

print("\nRandom Forest:")
print(f"  Accuracy: {accuracy_score(y_test, y_pred_rf):.3f}")
print(f"  Precision: {precision_score(y_test, y_pred_rf):.3f}")
print(f"  Recall: {recall_score(y_test, y_pred_rf):.3f}")  # High recall = fewer missed cancers
print(f"  F1 Score: {f1_score(y_test, y_pred_rf):.3f}")

# For medical diagnosis: High recall is often prioritized (don't miss cancers)
# High precision avoids false alarms (unnecessary biopsies)</code></pre>

                        <pre><code class="language-python"># Import libraries for ROC curve analysis
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score  # For ROC analysis

# 4. ROC CURVE - Visualize classifier performance across thresholds
# ROC = Receiver Operating Characteristic
# Shows trade-off between True Positive Rate and False Positive Rate
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# Reproduce same train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train logistic regression
lr = LogisticRegression(max_iter=10000, random_state=42)
lr.fit(X_train_scaled, y_train)
# Get probability predictions (not binary 0/1)
y_proba = lr.predict_proba(X_test_scaled)[:, 1]  # Probability of benign (class 1)

# roc_curve(): Calculate TPR and FPR at different classification thresholds
# fpr: False Positive Rate (X-axis) - How many benign predicted as malignant?
# tpr: True Positive Rate (Y-axis) - How many benign correctly identified?
# thresholds: Classification thresholds (0.0 to 1.0)
# See: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html
fpr, tpr, thresholds = roc_curve(y_test, y_proba)

# roc_auc_score(): Area Under ROC Curve
# 0.5 = random classifier, 1.0 = perfect classifier
auc = roc_auc_score(y_test, y_proba)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})', linewidth=2)
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')  # Diagonal line (AUC=0.5)
plt.xlabel('False Positive Rate')  # More FP = more false alarms
plt.ylabel('True Positive Rate')  # More TP = fewer missed diagnoses
plt.title('ROC Curve - Breast Cancer Classification')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Ideal curve: Hugs top-left corner (high TPR, low FPR)
# Higher AUC = better overall classifier performance</code></pre>

                        <!-- Diabetes Dataset -->
                        <h3 id="diabetes-dataset" class="mt-5"><i class="fas fa-medkit me-2 text-crimson"></i>5. Diabetes Dataset (Regression)</h3>
                        
                        <div class="highlight-box">
                            <i class="fas fa-info-circle"></i>
                            <strong>About Diabetes:</strong> 442 samples with 10 baseline features (age, BMI, blood pressure, etc.). Target: quantitative measure of disease progression one year after baseline. Great for learning regression.
                        </div>

                        <pre><code class="language-python"># Import libraries for regression analysis
from sklearn.datasets import load_diabetes  # Medical progression prediction dataset
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso  # Linear models with regularization
from sklearn.ensemble import RandomForestRegressor  # Tree ensemble for regression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import pandas as pd  # For data analysis
import numpy as np

# 1. LOAD & EXPLORE diabetes progression dataset
# This dataset contains baseline patient data and disease progression after 1 year
# Target is quantitative measure of disease progression (continuous value, not classification)
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target  # X = 10 features (age, BMI, BP, etc.), y = progression score

print(f"Dataset shape: {X.shape}")  # (442, 10) - 442 patients, 10 baseline measurements
print(f"Features: {diabetes.feature_names}")  # age, sex, bmi, bp, s1-s6 (blood serum measurements)
print(f"Target statistics: min={y.min():.1f}, max={y.max():.1f}, mean={y.mean():.1f}")
# Target values range from 25 to 346 (higher = worse progression)

# Create DataFrame for correlation analysis
df_diabetes = pd.DataFrame(X, columns=diabetes.feature_names)
df_diabetes['progression'] = y  # Add target column

# Identify which features correlate most with disease progression
print("\nCorrelation with target:")  # Positive = feature increases with disease progression
print(df_diabetes.corr()['progression'].sort_values(ascending=False))
# Typically: bmi (body mass index), s5 (serum triglycerides) are top predictors</code></pre>

                        <pre><code class="language-python"># Import regression models and metrics
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso  # Linear models
from sklearn.ensemble import RandomForestRegressor  # Non-linear ensemble
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import numpy as np

# 2. SPLIT & TRAIN MULTIPLE REGRESSION MODELS
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# Split data (no stratification needed for regression)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Linear Regression - no regularization (baseline model)
# See: https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares
lr = LinearRegression()
lr.fit(X_train, y_train)  # Learns weights for each feature

# Ridge Regression - L2 regularization (penalizes large coefficients)
# alpha=1.0: Regularization strength (higher = more penalty = simpler model)
# Good when features are correlated (reduces overfitting)
# See: https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso Regression - L1 regularization (can zero out features)
# alpha=0.5: Regularization strength
# Performs automatic feature selection (sets some coefficients to exactly 0)
# See: https://scikit-learn.org/stable/modules/linear_model.html#lasso
lasso = Lasso(alpha=0.5)
lasso.fit(X_train, y_train)

# Random Forest Regressor - ensemble of decision trees
# n_estimators=100: Build 100 trees and average predictions
# Captures non-linear relationships between features and target
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)

# 3. EVALUATE with regression metrics
models = {
    'Linear Regression': lr,
    'Ridge': ridge,
    'Lasso': lasso,
    'Random Forest': rf_reg
}

for name, model in models.items():
    y_pred = model.predict(X_test)
    # RMSE: Root Mean Squared Error (same units as target, penalizes large errors)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    # R² Score: Coefficient of determination (0-1, higher = better fit)
    #   1.0 = perfect predictions, 0 = model as good as mean baseline
    r2 = r2_score(y_test, y_pred)
    # MAE: Mean Absolute Error (average absolute difference, robust to outliers)
    mae = mean_absolute_error(y_test, y_pred)
    
    print(f"\n{name}:")
    print(f"  RMSE: {rmse:.2f}")  # Lower is better
    print(f"  R² Score: {r2:.3f}")  # Higher is better (max 1.0)
    print(f"  MAE: {mae:.2f}")  # Lower is better</code></pre>

                        <pre><code class="language-python"># Import visualization library
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# 4. VISUALIZE PREDICTIONS - Scatter plot of actual vs predicted values
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# Reproduce same train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest (typically best performer)
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)
y_pred = rf_reg.predict(X_test)  # Predict disease progression for test patients

# Create scatter plot
plt.figure(figsize=(10, 6))
# Each point = one test patient
# X-axis = actual progression, Y-axis = model's prediction
plt.scatter(y_test, y_pred, alpha=0.6)  # alpha=0.6 for transparency (see overlapping points)

# Plot ideal prediction line (y=x)
# Perfect predictions would fall exactly on this red dashed line
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)

plt.xlabel('Actual Disease Progression')  # True progression after 1 year
plt.ylabel('Predicted Disease Progression')  # Model's prediction
plt.title('Diabetes Progression: Actual vs Predicted')  # Title
plt.grid(alpha=0.3)  # Light grid for easier reading
plt.show()

# Points close to red line = good predictions
# Points far from line = model errors (over/under-estimation)</code></pre>

                        <pre><code class="language-python"># Import libraries for feature importance analysis
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# 5. FEATURE IMPORTANCE - Which features best predict disease progression?
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# Reproduce same split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)

# Extract feature importances (how much each feature contributes to predictions)
importances = rf_reg.feature_importances_  # Sum to 1.0
# Sort features by importance (descending order)
indices = np.argsort(importances)[::-1]

# Create bar chart
plt.figure(figsize=(10, 6))
plt.bar(range(X.shape[1]), importances[indices])  # Bars sorted by importance
# Label x-axis with feature names in sorted order
plt.xticks(range(X.shape[1]), [diabetes.feature_names[i] for i in indices], rotation=45)
plt.xlabel('Feature')  # Baseline patient measurements
plt.ylabel('Importance')  # 0 to ~0.3 for Diabetes dataset
plt.title('Feature Importance for Diabetes Progression Prediction')
plt.tight_layout()  # Prevent label cutoff
plt.show()

# Print ranked list with importance scores
print("Feature ranking:")
for i in range(X.shape[1]):
    print(f"{i+1}. {diabetes.feature_names[indices[i]]}: {importances[indices[i]]:.3f}")

# Typically: bmi (body mass index) and s5 (serum triglycerides) are most important
# This tells us which patient measurements to prioritize in clinical settings</code></pre>

                        <!-- California Housing Dataset -->
                        <h3 id="california-housing-dataset" class="mt-5"><i class="fas fa-home me-2 text-crimson"></i>6. California Housing Dataset (Regression)</h3>
                        
                        <div class="highlight-box">
                            <i class="fas fa-info-circle"></i>
                            <strong>About California Housing:</strong> 20,640 samples from California census data. 8 features (median income, house age, rooms, location, etc.). Target: median house value. Larger dataset ideal for testing model scalability.
                        </div>

                        <pre><code class="language-python"># Import libraries for large-scale regression
from sklearn.datasets import fetch_california_housing  # NOTE: fetch_, not load_
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd  # For data analysis
import numpy as np

# 1. LOAD & EXPLORE California Housing dataset
# fetch_california_housing() downloads dataset from internet (first time only)
# This is a larger dataset (20,640 samples) - good for testing model scalability
# Based on 1990 California census data
housing = fetch_california_housing()
X, y = housing.data, housing.target  # X = 8 features, y = median house value

print(f"Dataset shape: {X.shape}")  # (20640, 8) - 20,640 California districts
print(f"Features: {housing.feature_names}")
# MedInc: median income, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude

print(f"Target (median house value in $100k): min={y.min():.2f}, max={y.max():.2f}, mean={y.mean():.2f}")
# Values in $100,000s - e.g., 2.5 = $250,000 median house value

# Create DataFrame for correlation analysis
df_housing = pd.DataFrame(X, columns=housing.feature_names)
df_housing['MedHouseVal'] = y  # Add target column

print("\nFirst few rows:")  # Preview data structure
print(df_housing.head())

print("\nCorrelation with target:")  # Which features correlate with house prices?
print(df_housing.corr()['MedHouseVal'].sort_values(ascending=False))
# Typically: MedInc (median income) is strongest predictor of house value</code></pre>

                        <pre><code class="language-python"># Import regression models and evaluation metrics
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression  # Simple linear model
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor  # Powerful ensembles
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# 2. SPLIT, SCALE & TRAIN multiple models
housing = fetch_california_housing()
X, y = housing.data, housing.target

# Split: 80% train (16,512 samples), 20% test (4,128 samples)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"Training samples: {X_train.shape[0]}")  # 16,512 districts
print(f"Test samples: {X_test.shape[0]}")  # 4,128 districts

# Scale features (important for linear models, not trees)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Linear Regression - fast baseline model
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)  # Train on scaled data

# Gradient Boosting - powerful for tabular data
# n_estimators=100: Build 100 sequential trees
# learning_rate=0.1: Step size for gradient descent (smaller = more conservative)
# max_depth=5: Maximum tree depth (prevents overfitting)
# See: https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting
gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
gb.fit(X_train, y_train)  # Trees don't need scaling

# Random Forest - ensemble of independent trees
# max_depth=20: Allow deeper trees than Gradient Boosting
rf = RandomForestRegressor(n_estimators=100, max_depth=20, random_state=42)
rf.fit(X_train, y_train)

# 3. EVALUATE all models on test set
models = {
    'Linear Regression': (lr, X_test_scaled),  # Needs scaled data
    'Gradient Boosting': (gb, X_test),  # Original data
    'Random Forest': (rf, X_test)  # Original data
}

for name, (model, X_test_data) in models.items():
    y_pred = model.predict(X_test_data)
    # RMSE in $100k units (multiply by 100,000 for dollars)
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    # R²: proportion of variance explained (0-1, higher = better)
    r2 = r2_score(y_test, y_pred)
    
    print(f"\n{name}:")
    print(f"  RMSE: {rmse:.3f} ($100k)")  # e.g., 0.5 = ±$50,000 error
    print(f"  R² Score: {r2:.3f}")  # e.g., 0.8 = explains 80% of variance</code></pre>

                        <pre><code class="language-python"># Import visualization library
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor

# 4. VISUALIZE PREDICTIONS with dual plots
housing = fetch_california_housing()
X, y = housing.data, housing.target

# Reproduce same split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Gradient Boosting (typically best model for this dataset)
gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
gb.fit(X_train, y_train)
y_pred = gb.predict(X_test)  # Predict house values for 4,128 test districts

# Create figure with 2 side-by-side subplots
fig, axes = plt.subplots(1, 2, figsize=(15, 5))  # 1 row, 2 columns

# LEFT PLOT: Scatter plot of Actual vs Predicted
axes[0].scatter(y_test, y_pred, alpha=0.3)  # alpha=0.3 for transparency (many points)
# Plot y=x line (perfect predictions)
axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
axes[0].set_xlabel('Actual House Value ($100k)')  # True median house value
axes[0].set_ylabel('Predicted House Value ($100k)')  # Model's prediction
axes[0].set_title('California Housing: Actual vs Predicted')
axes[0].grid(alpha=0.3)
# Points near red line = accurate predictions
# Points above line = overestimation, below = underestimation

# RIGHT PLOT: Residual plot (errors vs predictions)
# residuals = actual - predicted (positive = underestimated, negative = overestimated)
residuals = y_test - y_pred
axes[1].scatter(y_pred, residuals, alpha=0.3)
axes[1].axhline(y=0, color='r', linestyle='--', lw=2)  # Zero error line
axes[1].set_xlabel('Predicted House Value ($100k)')
axes[1].set_ylabel('Residuals')  # Error in predictions
axes[1].set_title('Residual Plot')  # Check for patterns in errors
axes[1].grid(alpha=0.3)
# Random scatter around y=0 = good (no systematic bias)
# Pattern (e.g., curve) = model missing relationships

plt.tight_layout()  # Prevent subplot overlap
plt.show()</code></pre>

                        <pre><code class="language-python"># Import libraries for feature importance analysis
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
import numpy as np

# 5. FEATURE IMPORTANCE - Which factors most influence house prices?
housing = fetch_california_housing()
X, y = housing.data, housing.target

# Reproduce same split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Gradient Boosting
gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
gb.fit(X_train, y_train)

# Extract feature importances from trained model
importances = gb.feature_importances_  # Sum to 1.0
# Sort features by importance (descending)
indices = np.argsort(importances)[::-1]

# Create bar chart of feature importance
plt.figure(figsize=(10, 6))
plt.bar(range(X.shape[1]), importances[indices])  # 8 features
# Label x-axis with feature names in sorted order
plt.xticks(range(X.shape[1]), [housing.feature_names[i] for i in indices], rotation=45)
plt.xlabel('Feature')  # Census and geographic features
plt.ylabel('Importance')  # 0 to ~0.5 for California Housing
plt.title('Feature Importance for California Housing Price Prediction')
plt.tight_layout()  # Prevent x-label cutoff
plt.show()

# Print ranked list with importance scores
print("Feature ranking:")
for i in range(X.shape[1]):
    print(f"{i+1}. {housing.feature_names[indices[i]]}: {importances[indices[i]]:.3f}")

# Typically: MedInc (median income) is by far the most important predictor
# Latitude and Longitude also matter (location, location, location!)
# This tells us income and location are key drivers of California house prices</code></pre>

                        <div class="highlight-box mt-4">
                            <i class="fas fa-graduation-cap"></i>
                            <strong>Datasets Summary:</strong> You've now seen complete workflows for all major Scikit-learn datasets—from loading and exploring to training, evaluation, and visualization. These patterns apply to any ML project. Use these datasets to experiment with new algorithms and techniques!
                        </div>
                    </div>

                    <!-- Preprocessing -->
                    <div id="preprocessing" class="blog-content mt-5">
                        <h2><i class="fas fa-wrench me-2 text-teal"></i>Data Preprocessing</h2>
                        
                        <p>Preprocessing transforms raw data into a format suitable for ML algorithms. This is critical—garbage in, garbage out.</p>

                        <h3>Feature Scaling</h3>
                        <p>Many algorithms (SVM, neural networks, k-NN) require features on similar scales:</p>

                        <pre><code class="language-python">from sklearn.preprocessing import StandardScaler, MinMaxScaler

# StandardScaler: mean=0, std=1
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Use training stats!

# MinMaxScaler: scale to [0, 1]
minmax = MinMaxScaler()
X_minmax = minmax.fit_transform(X_train)</code></pre>

                        <h3>Encoding Categorical Variables</h3>
                        <pre><code class="language-python">from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# LabelEncoder: convert strings to integers
le = LabelEncoder()
y_encoded = le.fit_transform(['cat', 'dog', 'cat', 'bird'])  # [0, 1, 0, 2]

# OneHotEncoder: create binary columns
ohe = OneHotEncoder(sparse=False)
categories = [['red'], ['blue'], ['green']]
encoded = ohe.fit_transform(categories)
# [[1, 0, 0],
#  [0, 1, 0],
#  [0, 0, 1]]</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-exclamation-triangle"></i>
                            <strong>Critical:</strong> Always <code>fit()</code> on training data, then <code>transform()</code> on both train and test. Never <code>fit()</code> on test data—this causes data leakage!
                        </div>
                    </div>

                    <!-- Classification -->
                    <div id="classification" class="blog-content mt-5">
                        <h2><i class="fas fa-tags me-2 text-teal"></i>Classification Models</h2>
                        
                        <p>Classification predicts categorical outcomes (spam/not spam, disease/healthy, customer churn).</p>

                        <h3>Common Classifiers</h3>
                        <pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Logistic Regression (linear boundary)
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)
print("Logistic:", log_reg.score(X_test, y_test))

# Random Forest (ensemble of decision trees)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
print("Random Forest:", rf.score(X_test, y_test))

# Support Vector Machine (complex boundaries)
svm = SVC(kernel='rbf')
svm.fit(X_train, y_train)
print("SVM:", svm.score(X_test, y_test))

# k-Nearest Neighbors (instance-based)
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
print("KNN:", knn.score(X_test, y_test))</code></pre>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-teal text-white">Choosing an Algorithm</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>Logistic Regression:</strong> Fast, interpretable, works well for linearly separable data</li>
                                    <li><strong>Random Forest:</strong> Handles non-linear relationships, robust to outliers, good default choice</li>
                                    <li><strong>SVM:</strong> Powerful for complex boundaries, sensitive to feature scaling</li>
                                    <li><strong>k-NN:</strong> Simple, no training phase, good for small datasets</li>
                                </ul>
                                <p><strong>Rule of thumb:</strong> Start with Logistic Regression (fast baseline), then try Random Forest if you need more complexity.</p>
                            </div>
                        </div>
                    </div>

                    <!-- Regression -->
                    <div id="regression" class="blog-content mt-5">
                        <h2><i class="fas fa-chart-line me-2 text-teal"></i>Regression Models</h2>
                        
                        <p>Regression predicts continuous values (house prices, temperatures, sales).</p>

                        <pre><code class="language-python">from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

# Generate synthetic data
from sklearn.datasets import make_regression
X_reg, y_reg = make_regression(n_samples=200, n_features=3, noise=10, random_state=42)
X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)

# Linear Regression
lin_reg = LinearRegression()
lin_reg.fit(X_train_r, y_train_r)
y_pred = lin_reg.predict(X_test_r)
print(f"Linear - RMSE: {mean_squared_error(y_test_r, y_pred, squared=False):.2f}")
print(f"Linear - R²: {r2_score(y_test_r, y_pred):.3f}")

# Ridge Regression (with L2 regularization)
ridge = Ridge(alpha=1.0)
ridge.fit(X_train_r, y_train_r)
y_pred_ridge = ridge.predict(X_test_r)
print(f"Ridge - R²: {r2_score(y_test_r, y_pred_ridge):.3f}")

# Random Forest Regressor
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train_r, y_train_r)
print(f"RF - R²: {r2_score(y_test_r, rf_reg.predict(X_test_r)):.3f}")</code></pre>
                    </div>

                    <!-- Clustering -->
                    <div id="clustering" class="blog-content mt-5">
                        <h2><i class="fas fa-circle-notch me-2 text-teal"></i>Clustering</h2>
                        
                        <p>Clustering finds groups in unlabeled data (customer segmentation, anomaly detection).</p>

                        <pre><code class="language-python">from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Generate blob data
from sklearn.datasets import make_blobs
X_blob, y_true = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)

# K-Means clustering
kmeans = KMeans(n_clusters=4, random_state=42)
labels = kmeans.fit_predict(X_blob)

print(f"Cluster centers: {kmeans.cluster_centers_.shape}")
print(f"Silhouette score: {silhouette_score(X_blob, labels):.3f}")

# Visualize (assuming 2D data)
import matplotlib.pyplot as plt
plt.scatter(X_blob[:, 0], X_blob[:, 1], c=labels, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
            marker='X', s=200, c='red', label='Centroids')
plt.legend()
plt.title('K-Means Clustering')
plt.show()</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-info-circle"></i>
                            <strong>Silhouette Score:</strong> Ranges from -1 to 1. Values near 1 indicate well-separated clusters, near 0 means overlapping clusters, negative values suggest misclassification.
                        </div>
                    </div>

                    <!-- Evaluation -->
                    <div id="evaluation" class="blog-content mt-5">
                        <h2><i class="fas fa-check-square me-2 text-teal"></i>Model Evaluation</h2>
                        
                        <h3>Classification Metrics</h3>
                        <pre><code class="language-python">from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

# Predictions
y_pred = model.predict(X_test)

# Metrics
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")
print(f"Precision: {precision_score(y_test, y_pred, average='weighted'):.3f}")
print(f"Recall: {recall_score(y_test, y_pred, average='weighted'):.3f}")
print(f"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.3f}")

# Confusion Matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Classification Report (comprehensive)
print(classification_report(y_test, y_pred))</code></pre>

                        <h3>Cross-Validation</h3>
                        <p>Single train/test split can be misleading. Cross-validation provides robust estimates:</p>

                        <pre><code class="language-python">from sklearn.model_selection import cross_val_score, KFold

# 5-fold cross-validation
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"CV scores: {scores}")
print(f"Mean: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")

# Custom K-Fold
kf = KFold(n_splits=10, shuffle=True, random_state=42)
for train_idx, test_idx in kf.split(X):
    X_train_fold, X_test_fold = X[train_idx], X[test_idx]
    # Train and evaluate...</code></pre>

                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-crimson text-white">Metric Selection</span>
                            </div>
                            <div class="card-content">
                                <ul>
                                    <li><strong>Accuracy:</strong> Good for balanced datasets</li>
                                    <li><strong>Precision:</strong> Important when false positives are costly (spam detection)</li>
                                    <li><strong>Recall:</strong> Critical when false negatives are costly (disease detection)</li>
                                    <li><strong>F1 Score:</strong> Harmonic mean of precision and recall—good for imbalanced data</li>
                                    <li><strong>ROC-AUC:</strong> Measures model's ability to distinguish classes (0.5 = random, 1.0 = perfect)</li>
                                </ul>
                            </div>
                        </div>
                    </div>

                    <!-- Pipelines -->
                    <div id="pipelines" class="blog-content mt-5">
                        <h2><i class="fas fa-stream me-2 text-teal"></i>Pipelines & Automation</h2>
                        
                        <p>Pipelines chain preprocessing and modeling steps, preventing data leakage and simplifying code.</p>

                        <pre><code class="language-python">from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

# Create pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# Fit entire pipeline
pipeline.fit(X_train, y_train)

# Predict
y_pred = pipeline.predict(X_test)
print(f"Pipeline accuracy: {pipeline.score(X_test, y_test):.3f}")

# Cross-validate entire pipeline
scores = cross_val_score(pipeline, X, y, cv=5)
print(f"CV mean: {scores.mean():.3f}")</code></pre>

                        <h3>ColumnTransformer for Mixed Data</h3>
                        <pre><code class="language-python">from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

# Define transformers for different column types
numeric_features = ['age', 'income']
categorical_features = ['city', 'gender']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Complete pipeline
full_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000))
])

full_pipeline.fit(X_train, y_train)</code></pre>
                    </div>

                    <!-- Tuning -->
                    <div id="tuning" class="blog-content mt-5">
                        <h2><i class="fas fa-sliders-h me-2 text-teal"></i>Hyperparameter Tuning</h2>
                        
                        <p>Hyperparameters control model behavior (learning rate, tree depth, etc.). Tuning finds optimal values.</p>

                        <h3>Grid Search</h3>
                        <pre><code class="language-python">from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

# Grid search with cross-validation
grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1  # Use all CPU cores
)

grid.fit(X_train, y_train)

print(f"Best params: {grid.best_params_}")
print(f"Best CV score: {grid.best_score_:.3f}")
print(f"Test score: {grid.score(X_test, y_test):.3f}")</code></pre>

                        <h3>Random Search (Faster)</h3>
                        <pre><code class="language-python">from sklearn.model_selection import RandomizedSearchCV

# Random search samples random combinations
param_dist = {
    'n_estimators': [50, 100, 150, 200, 250, 300],
    'max_depth': [None, 5, 10, 15, 20, 25, 30],
    'min_samples_split': [2, 5, 10, 15]
}

random = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_dist,
    n_iter=20,  # Try 20 random combinations
    cv=5,
    random_state=42,
    n_jobs=-1
)

random.fit(X_train, y_train)
print(f"Best params: {random.best_params_}")</code></pre>

                        <div class="highlight-box">
                            <i class="fas fa-bolt"></i>
                            <strong>Grid vs Random:</strong> Grid Search is exhaustive but slow. Random Search samples randomly—often finds good parameters 10x faster. For large grids, use Random Search first, then Grid Search to fine-tune.
                        </div>
                    </div>

                    <!-- Best Practices -->
                    <div id="best-practices" class="blog-content mt-5">
                        <h2><i class="fas fa-trophy me-2 text-teal"></i>Best Practices & Summary</h2>
                        
                        <h3>Key Takeaways</h3>
                        <ul>
                            <li>✅ <strong>Always split data:</strong> Training set to train, test set to evaluate (never train on test!)</li>
                            <li>✅ <strong>Scale features:</strong> Especially for distance-based models (SVM, k-NN, neural nets)</li>
                            <li>✅ <strong>Use pipelines:</strong> Prevent data leakage and simplify workflows</li>
                            <li>✅ <strong>Cross-validate:</strong> Single train/test split can be misleading</li>
                            <li>✅ <strong>Choose metrics wisely:</strong> Accuracy isn't always appropriate (use F1 for imbalanced data)</li>
                            <li>✅ <strong>Start simple:</strong> Baseline with Logistic Regression before complex models</li>
                            <li>✅ <strong>Set random_state:</strong> For reproducibility in experiments</li>
                            <li>✅ <strong>Save models:</strong> Use <code>joblib</code> to persist trained models</li>
                        </ul>

                        <h3>Common Pitfalls</h3>
                        <div class="experiment-card">
                            <div class="card-meta mb-2">
                                <span class="badge bg-crimson text-white">Avoid These Mistakes</span>
                            </div>
                            <div class="card-content">
                                <ol>
                                    <li><strong>Data leakage:</strong> Fitting preprocessors on test data</li>
                                    <li><strong>Not scaling:</strong> SVM and neural nets need scaled features</li>
                                    <li><strong>Using accuracy for imbalanced data:</strong> 99% accuracy means nothing if 99% of data is one class</li>
                                    <li><strong>Overfitting:</strong> Model performs great on training data, poorly on test data</li>
                                    <li><strong>Not setting random_state:</strong> Results change every run</li>
                                </ol>
                            </div>
                        </div>

                        <h3>Model Persistence</h3>
                        <pre><code class="language-python">import joblib

# Save model
joblib.dump(model, 'model.joblib')

# Load model
loaded_model = joblib.load('model.joblib')
predictions = loaded_model.predict(X_new)</code></pre>

                        <h3>Series Completion</h3>
                        <div class="highlight-box">
                            <i class="fas fa-graduation-cap"></i>
                            <strong>Congratulations!</strong> You've completed the Python Data Science Series. You now have the full toolkit:
                            <ul class="mt-2">
                                <li><strong>NumPy:</strong> Efficient numerical computation</li>
                                <li><strong>Pandas:</strong> Data manipulation and analysis</li>
                                <li><strong>Matplotlib/Seaborn:</strong> Compelling visualizations</li>
                                <li><strong>Scikit-learn:</strong> Machine learning models and pipelines</li>
                            </ul>
                            <p class="mt-3">You're ready to tackle real-world data science projects—from exploratory analysis to predictive modeling!</p>
                        </div>
                    </div>

                    <!-- Scikit-learn API Cheat Sheet -->
                    <div class="blog-content mt-5" id="cheat-sheet">
                        <h2><i class="fas fa-bookmark me-2 text-teal"></i>Scikit-learn API Cheat Sheet</h2>
                        
                        <p class="lead">Quick reference for machine learning workflows with Scikit-learn.</p>

                        <div class="row mt-4">
                            <!-- Data Preparation -->
                            <div class="col-md-6 mb-4">
                                <div class="card border-0 shadow-sm h-100">
                                    <div class="card-header bg-teal text-dark">
                                        <h5 class="mb-0" style="color: #ffffff !important;"><i class="fas fa-database me-2"></i>Data Preparation</h5>
                                    </div>
                                    <div class="card-body">
                                        <table class="table table-sm table-hover text-dark">
                                            <tbody>
                                                <tr><td><code>train_test_split(X, y)</code></td><td>Split data</td></tr>
                                                <tr><td><code>test_size=0.2</code></td><td>20% test set</td></tr>
                                                <tr><td><code>random_state=42</code></td><td>Reproducibility</td></tr>
                                                <tr><td><code>StandardScaler()</code></td><td>Standardize</td></tr>
                                                <tr><td><code>MinMaxScaler()</code></td><td>Scale 0-1</td></tr>
                                                <tr><td><code>LabelEncoder()</code></td><td>Encode labels</td></tr>
                                                <tr><td><code>OneHotEncoder()</code></td><td>One-hot encode</td></tr>
                                            </tbody>
                                        </table>
                                    </div>
                                </div>
                            </div>

                            <!-- Model Training -->
                            <div class="col-md-6 mb-4">
                                <div class="card border-0 shadow-sm h-100">
                                    <div class="card-header bg-navy text-dark">
                                        <h5 class="mb-0"><i class="fas fa-brain me-2"></i>Model Training</h5>
                                    </div>
                                    <div class="card-body">
                                        <table class="table table-sm table-hover text-dark">
                                            <tbody>
                                                <tr><td><code>model.fit(X_train, y_train)</code></td><td>Train model</td></tr>
                                                <tr><td><code>model.predict(X_test)</code></td><td>Make predictions</td></tr>
                                                <tr><td><code>model.score(X, y)</code></td><td>Model accuracy</td></tr>
                                                <tr><td><code>model.predict_proba(X)</code></td><td>Probabilities</td></tr>
                                                <tr><td><code>LinearRegression()</code></td><td>Linear model</td></tr>
                                                <tr><td><code>LogisticRegression()</code></td><td>Classification</td></tr>
                                                <tr><td><code>RandomForestClassifier()</code></td><td>Random forest</td></tr>
                                            </tbody>
                                        </table>
                                    </div>
                                </div>
                            </div>

                            <!-- Model Evaluation -->
                            <div class="col-md-6 mb-4">
                                <div class="card border-0 shadow-sm h-100">
                                    <div class="card-header bg-blue text-dark">
                                        <h5 class="mb-0"><i class="fas fa-chart-bar me-2"></i>Model Evaluation</h5>
                                    </div>
                                    <div class="card-body">
                                        <table class="table table-sm table-hover text-dark">
                                            <tbody>
                                                <tr><td><code>accuracy_score(y, pred)</code></td><td>Accuracy</td></tr>
                                                <tr><td><code>precision_score(y, pred)</code></td><td>Precision</td></tr>
                                                <tr><td><code>recall_score(y, pred)</code></td><td>Recall</td></tr>
                                                <tr><td><code>f1_score(y, pred)</code></td><td>F1 score</td></tr>
                                                <tr><td><code>confusion_matrix(y, pred)</code></td><td>Confusion matrix</td></tr>
                                                <tr><td><code>mean_squared_error(y, pred)</code></td><td>MSE</td></tr>
                                                <tr><td><code>r2_score(y, pred)</code></td><td>R² score</td></tr>
                                            </tbody>
                                        </table>
                                    </div>
                                </div>
                            </div>

                            <!-- Preprocessing -->
                            <div class="col-md-6 mb-4">
                                <div class="card border-0 shadow-sm h-100">
                                    <div class="card-header bg-crimson text-dark">
                                        <h5 class="mb-0" style="color: #ffffff !important;"><i class="fas fa-cogs me-2"></i>Preprocessing</h5>
                                    </div>
                                    <div class="card-body">
                                        <table class="table table-sm table-hover text-dark">
                                            <tbody>
                                                <tr><td><code>scaler.fit(X_train)</code></td><td>Fit scaler</td></tr>
                                                <tr><td><code>scaler.transform(X)</code></td><td>Transform data</td></tr>
                                                <tr><td><code>scaler.fit_transform(X)</code></td><td>Fit & transform</td></tr>
                                                <tr><td><code>SimpleImputer()</code></td><td>Fill missing</td></tr>
                                                <tr><td><code>PolynomialFeatures()</code></td><td>Poly features</td></tr>
                                                <tr><td><code>normalize(X)</code></td><td>Normalize</td></tr>
                                                <tr><td><code>binarize(X)</code></td><td>Binarize</td></tr>
                                            </tbody>
                                        </table>
                                    </div>
                                </div>
                            </div>

                            <!-- Model Selection -->
                            <div class="col-md-6 mb-4">
                                <div class="card border-0 shadow-sm h-100">
                                    <div class="card-header bg-teal text-dark">
                                        <h5 class="mb-0" style="color: #ffffff !important;"><i class="fas fa-sliders-h me-2"></i>Model Selection</h5>
                                    </div>
                                    <div class="card-body">
                                        <table class="table table-sm table-hover text-dark">
                                            <tbody>
                                                <tr><td><code>cross_val_score(model, X, y)</code></td><td>Cross-validation</td></tr>
                                                <tr><td><code>cv=5</code></td><td>5-fold CV</td></tr>
                                                <tr><td><code>GridSearchCV(model, params)</code></td><td>Grid search</td></tr>
                                                <tr><td><code>RandomizedSearchCV()</code></td><td>Random search</td></tr>
                                                <tr><td><code>learning_curve()</code></td><td>Learning curve</td></tr>
                                                <tr><td><code>validation_curve()</code></td><td>Validation curve</td></tr>
                                            </tbody>
                                        </table>
                                    </div>
                                </div>
                            </div>

                            <!-- Pipelines -->
                            <div class="col-md-6 mb-4">
                                <div class="card border-0 shadow-sm h-100">
                                    <div class="card-header bg-navy text-dark">
                                        <h5 class="mb-0"><i class="fas fa-project-diagram me-2"></i>Pipelines</h5>
                                    </div>
                                    <div class="card-body">
                                        <table class="table table-sm table-hover text-dark">
                                            <tbody>
                                                <tr><td><code>Pipeline(steps)</code></td><td>Create pipeline</td></tr>
                                                <tr><td><code>make_pipeline()</code></td><td>Quick pipeline</td></tr>
                                                <tr><td><code>pipe.fit(X, y)</code></td><td>Fit pipeline</td></tr>
                                                <tr><td><code>pipe.predict(X)</code></td><td>Predict</td></tr>
                                                <tr><td><code>ColumnTransformer()</code></td><td>Column-wise ops</td></tr>
                                                <tr><td><code>FeatureUnion()</code></td><td>Combine features</td></tr>
                                            </tbody>
                                        </table>
                                    </div>
                                </div>
                            </div>

                            <!-- Ensemble Methods -->
                            <div class="col-md-6 mb-4">
                                <div class="card border-0 shadow-sm h-100">
                                    <div class="card-header bg-blue text-dark">
                                        <h5 class="mb-0"><i class="fas fa-layer-group me-2"></i>Ensemble Methods</h5>
                                    </div>
                                    <div class="card-body">
                                        <table class="table table-sm table-hover text-dark">
                                            <tbody>
                                                <tr><td><code>RandomForestClassifier()</code></td><td>Random forest</td></tr>
                                                <tr><td><code>GradientBoostingClassifier()</code></td><td>Gradient boost</td></tr>
                                                <tr><td><code>AdaBoostClassifier()</code></td><td>AdaBoost</td></tr>
                                                <tr><td><code>VotingClassifier()</code></td><td>Voting</td></tr>
                                                <tr><td><code>BaggingClassifier()</code></td><td>Bagging</td></tr>
                                                <tr><td><code>StackingClassifier()</code></td><td>Stacking</td></tr>
                                            </tbody>
                                        </table>
                                    </div>
                                </div>
                            </div>

                            <!-- Dimensionality Reduction -->
                            <div class="col-md-6 mb-4">
                                <div class="card border-0 shadow-sm h-100">
                                    <div class="card-header bg-crimson text-dark">
                                        <h5 class="mb-0" style="color: #ffffff !important;"><i class="fas fa-compress me-2"></i>Dimensionality Reduction</h5>
                                    </div>
                                    <div class="card-body">
                                        <table class="table table-sm table-hover text-dark">
                                            <tbody>
                                                <tr><td><code>PCA(n_components=2)</code></td><td>PCA</td></tr>
                                                <tr><td><code>pca.fit_transform(X)</code></td><td>Transform to PC</td></tr>
                                                <tr><td><code>pca.explained_variance_</code></td><td>Variance explained</td></tr>
                                                <tr><td><code>TruncatedSVD()</code></td><td>SVD</td></tr>
                                                <tr><td><code>TSNE()</code></td><td>t-SNE</td></tr>
                                                <tr><td><code>SelectKBest()</code></td><td>Feature selection</td></tr>
                                            </tbody>
                                        </table>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <div class="highlight-box mt-4">
                            <i class="fas fa-lightbulb"></i>
                            <strong>Pro Tips:</strong>
                            <ul class="mb-0">
                                <li><strong>Train-test split:</strong> Always split before preprocessing to avoid data leakage</li>
                                <li><strong>Cross-validation:</strong> Use CV for robust model evaluation (5-10 folds typical)</li>
                                <li><strong>Pipelines:</strong> Chain preprocessing + model to prevent leakage and simplify workflow</li>
                                <li><strong>Scaling:</strong> Required for distance-based algorithms (KNN, SVM, neural networks)</li>
                                <li><strong>Class imbalance:</strong> Use <code>class_weight='balanced'</code> or SMOTE for imbalanced data</li>
                            </ul>
                        </div>
                    </div>

                    <!-- Related Posts -->
                    <div class="related-posts">
                        <h3><i class="fas fa-book me-2"></i>Related Articles in This Series</h3>
                        <div class="related-post-item">
                            <h5 class="mb-2">Part 1: NumPy Foundations for Data Science</h5>
                            <p class="text-muted small mb-2">Master NumPy arrays, vectorization, broadcasting, and linear algebra operations—the foundation of Python data science.</p>
                            <a href="python-data-science-numpy-foundations.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                        </div>
                        <div class="related-post-item">
                            <h5 class="mb-2">Part 2: Pandas for Data Analysis</h5>
                            <p class="text-muted small mb-2">Master Pandas DataFrames, Series, data cleaning, transformation, groupby operations, and merge techniques for real-world data analysis.</p>
                            <a href="python-data-science-pandas-analysis.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                        </div>
                        <div class="related-post-item">
                            <h5 class="mb-2">Part 3: Data Visualization with Matplotlib & Seaborn</h5>
                            <p class="text-muted small mb-2">Create compelling visualizations with Python's most powerful plotting libraries. Learn line plots, bar charts, scatter plots, and statistical graphics.</p>
                            <a href="python-data-science-visualization.html" class="text-decoration-none">Read Article <i class="fas fa-arrow-right ms-1"></i></a>
                        </div>
                    </div>

            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer id="social-media" class="bg-dark text-light py-5">
        <div class="container py-5">
            <div class="row mb-4">
                <div class="col-lg-6 mb-4 mb-lg-0">
                    <h5 class="fw-bold mb-3">Let's Connect</h5>
                    <p class="text-light">
                        I'm always interested in sharing content about my interests on different topics. Read disclaimer and feel free to share further.
                    </p>
                </div>
                <div class="col-lg-6">
                    <h5 class="fw-bold mb-3">Follow Me</h5>
                    <div class="social-links d-flex gap-2 flex-wrap">
                        <a href="https://www.facebook.com/wasil.zafar/" target="_blank" class="social-icon" title="Facebook">
                            <i class="fab fa-facebook-f"></i>
                        </a>
                        <a href="https://twitter.com/wasilzafar" target="_blank" class="social-icon" title="Twitter">
                            <i class="fab fa-twitter"></i>
                        </a>
                        <a href="https://www.linkedin.com/in/wasilzafar" target="_blank" class="social-icon" title="LinkedIn">
                            <i class="fab fa-linkedin-in"></i>
                        </a>
                        <a href="https://www.youtube.com/@wasilzafar" target="_blank" class="social-icon" title="YouTube">
                            <i class="fab fa-youtube"></i>
                        </a>
                        <a href="https://www.instagram.com/itswzee/" target="_blank" class="social-icon" title="Instagram">
                            <i class="fab fa-instagram"></i>
                        </a>
                        <a href="https://in.pinterest.com/wasilz/" target="_blank" class="social-icon" title="Pinterest">
                            <i class="fab fa-pinterest-p"></i>
                        </a>
                        <a href="mailto:wasil.zafar@gmail.com" class="social-icon" title="Email">
                            <i class="fas fa-envelope"></i>
                        </a>
                    </div>
                </div>
            </div>

            <hr class="bg-secondary">

            <div class="row mt-4">
                <div class="col-md-6">
                    <p class="small mb-2">
                        <i class="fas fa-camera me-2"></i>Background photo by Max Andrey from <a href="https://www.pexels.com/" target="_blank" class="text-light">Pexels</a>
                    </p>
                    <p class="small">
                        <i class="fas fa-icons me-2"></i>Icons from <a href="https://www.flaticon.com/" target="_blank" class="text-light">Flaticon</a> &amp; <a href="https://fontawesome.com/" target="_blank" class="text-light">Font Awesome</a>
                    </p>
                    <p class="small mt-3">
                        <a href="/" class="text-light text-decoration-none">Home</a> | 
                        <a href="/disclaimer.html" class="text-light text-decoration-none">Disclaimer</a> | 
                        <a href="/privacy-policy.html" class="text-light text-decoration-none">Privacy Policy</a>
                    </p>
                </div>
                <div class="col-md-6 text-md-end">
                    <p class="small">
                        Updated by <strong>Wasil Zafar</strong> | <time>October 26, 2025</time>
                    </p>
                </div>
            </div>
        </div>
    </footer>

    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    
    <!-- Scroll-to-Top Button -->
    <button id="scrollToTop" class="scroll-to-top" title="Back to Top">
        <i class="fas fa-arrow-up"></i>
    </button>
    
    <!-- Cookie Consent JS -->
    <script src="../../../js/cookie-consent.js"></script>
    
    <!-- Main JS -->
    <script src="../../../js/main.js"></script>

    <!-- Prism.js Syntax Highlighting -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/toolbar/prism-toolbar.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>

    <!-- Scroll-to-Top Script -->
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const scrollToTopBtn = document.getElementById('scrollToTop');
            
            // Show/hide button on scroll
            window.addEventListener('scroll', function() {
                if (window.scrollY > 300) {
                    scrollToTopBtn.classList.add('show');
                } else {
                    scrollToTopBtn.classList.remove('show');
                }
            });
            
            // Smooth scroll to top on click
            scrollToTopBtn.addEventListener('click', function() {
                window.scrollTo({ top: 0, behavior: 'smooth' });
            });
        });
    </script>

    <!-- Prism Theme Switcher -->
    <script>
        // Available themes with display names
        const themes = {
            'prism-theme': 'Tomorrow Night',
            'prism-default': 'Default',
            'prism-dark': 'Dark',
            'prism-twilight': 'Twilight',
            'prism-okaidia': 'Okaidia',
            'prism-solarizedlight': 'Solarized Light'
        };

        // Load saved theme from localStorage or use default
        const savedTheme = localStorage.getItem('prism-theme') || 'prism-theme';

        // Function to switch theme
        function switchTheme(themeId) {
            // Disable all themes
            Object.keys(themes).forEach(id => {
                const link = document.getElementById(id);
                if (link) {
                    link.disabled = true;
                }
            });
            
            // Enable selected theme
            const selectedLink = document.getElementById(themeId);
            if (selectedLink) {
                selectedLink.disabled = false;
                localStorage.setItem('prism-theme', themeId);
            }

            // Update all dropdowns on the page to match selected theme
            document.querySelectorAll('div.code-toolbar select').forEach(dropdown => {
                dropdown.value = themeId;
            });

            // Re-apply syntax highlighting with new theme
            setTimeout(() => {
                Prism.highlightAll();
            }, 10);
        }

        // Apply saved theme on page load
        document.addEventListener('DOMContentLoaded', function() {
            switchTheme(savedTheme);
        });

        // Add theme switcher to Prism toolbar
        Prism.plugins.toolbar.registerButton('theme-switcher', function(env) {
            const select = document.createElement('select');
            select.setAttribute('aria-label', 'Select code theme');
            select.className = 'prism-theme-selector';
            
            // Populate dropdown with themes
            Object.keys(themes).forEach(themeId => {
                const option = document.createElement('option');
                option.value = themeId;
                option.textContent = themes[themeId];
                if (themeId === savedTheme) {
                    option.selected = true;
                }
                select.appendChild(option);
            });
            
            // Handle theme change
            select.addEventListener('change', function(e) {
                switchTheme(e.target.value);
            });
            
            return select;
        });
    </script>
</body>
</html>
